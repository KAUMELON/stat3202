[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"\n\n\nbook collection lecture notes covering syllabus statistics course STATISTICAL METHODS & APPLICATIONS (STAT 3202) B.Sc.(Hons.) Agriculture Kerala Agricultural University\n","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\n\nNote: book published MeLoN (Module e-Learning & Online Notes) . online version book free read .\n\nfeedback, please feel free contact Dr.Pratheesh P. Gopinath. E-mail: pratheesh.pg@kau.Thank !\n","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"lecture introduction, includes, definition statistics, collection classification data, formation frequency distribution.(Goon Dasgupta 1983) (Gupta Kapoor 1997)","code":""},{"path":"introduction.html","id":"origin-of-the-word-statistics","chapter":"1 Introduction","heading":"1.1 Origin of the word ‚ÄúStatistics‚Äù","text":"term statistics derived Neo-Latin word statisticum collegium meaning ‚Äúcouncil state‚Äù Italian word statista meaning ‚Äústatesman‚Äù ‚Äúpolitician‚Äù.\nFigure 1.1: Statistical Account Scotland Sir John Sinclair (1791)\n","code":""},{"path":"introduction.html","id":"statistics-and-mathematics","chapter":"1 Introduction","heading":"1.2 Statistics and Mathematics","text":"Mathematics follows rigid theorem proof. Mathematical theories involve well-defined proven facts minimal scope change. However, Statistics discipline real-life data handled. factor makes field study abstract, individuals develop newer solutions problems new observed . Statistics applied science; mathematics goal prove theorems. statistics, main goal develop good methods understanding data making decisions. Statisticians often use mathematical theorems justify methods, theorems main focus. Statistics now considered independent field uses mathematics solve real life problems.","code":""},{"path":"introduction.html","id":"definition-of-statistics","chapter":"1 Introduction","heading":"1.3 Definition of Statistics","text":"Statistics science deals theCollection dataCollection dataOrganization data Classification dataOrganization data Classification dataPresentation dataPresentation dataAnalysis dataAnalysis dataInterpretation dataInterpretation dataTwo main branches statistics :Descriptive statistics, deals summarizing data sample using indexes mean standard deviation etc.Inferential statistics, use random sample data taken population describe make inferences population parameters.","code":""},{"path":"introduction.html","id":"data","chapter":"1 Introduction","heading":"1.4 Data","text":"Data can defined individual pieces factual information recorded used purpose analysis. raw information inferences drawn using science ‚ÄúSTATISTICS‚Äù.Example dataNo.¬†farmers block..¬†farmers block.rainfall period time.rainfall period time.Area paddy crop state.Area paddy crop state.","code":""},{"path":"introduction.html","id":"use-and-limitations-of-statistics","chapter":"1 Introduction","heading":"1.5 Use and limitations of statistics","text":"Functions statistics: Statistics simplifies complexity, presents facts definite form, helps formulation suitable policies, facilitates comparison helps forecasting. Valid results conclusion obtained research experiments using proper statistical tools.Uses statistics: Statistics pervaded almost spheres human activities. Statistics useful administration, Industry, business, economics, research workers, banking,insurance companies etc.Limitations StatisticsStatistical theories can applied variability experimental material.Statistical theories can applied variability experimental material.Statistics deals aggregates groups individual objects.Statistics deals aggregates groups individual objects.Statistical results exact.Statistical results exact.Statistics often misused.Statistics often misused.","code":""},{"path":"introduction.html","id":"population-and-sample","chapter":"1 Introduction","heading":"1.6 Population and Sample","text":"Consider following example.Suppose wish study body masses students College Agriculture, Vellayani. take us long time measure body masses students college may select 20 students measure body masses (kg). Suppose obtain measurements like this49 56 48 61 59 43 58 52 64 71 57 52 63 58 51 47 57 46 53 59In study, interested body masses students College Agriculture, Vellayani. set body masses students College Agriculture, Vellayani called population study. set 20 body masses, W = {49, 56,48, ‚Ä¶, 53, 59}, sample population.","code":""},{"path":"introduction.html","id":"population","chapter":"1 Introduction","heading":"1.6.1 Population","text":"population set objects wish study","code":""},{"path":"introduction.html","id":"sample","chapter":"1 Introduction","heading":"1.6.2 Sample","text":"sample part population study learn population.","code":""},{"path":"introduction.html","id":"variables-and-constants","chapter":"1 Introduction","heading":"1.7 Variables and constants","text":"","code":""},{"path":"introduction.html","id":"variables","chapter":"1 Introduction","heading":"1.7.1 Variables","text":"type observation can take different values different people, different values different times, places, called variable. following examples variables:family size, number hospital beds, number schools country, etc.family size, number hospital beds, number schools country, etc.height, mass, blood pressure, temperature, blood glucose level, etc.height, mass, blood pressure, temperature, blood glucose level, etc.Broadly speaking, two types variables ‚Äì quantitative qualitative (categorical) variables","code":""},{"path":"introduction.html","id":"constants","chapter":"1 Introduction","heading":"1.7.2 Constants","text":"Constants characteristics values change. Examples constants : pi (ùùÖ) = ratio circumference circle diameter (ùùÖ = 3.14159...) e, base natural (Napierian) logarithms (e=2.71828).","code":""},{"path":"introduction.html","id":"types-of-variables","chapter":"1 Introduction","heading":"1.8 Types of variables","text":"","code":""},{"path":"introduction.html","id":"quantitative-variables","chapter":"1 Introduction","heading":"1.8.1 Quantitative variables","text":"quantitative variable one can take numerical values. variables like family size, number hospital beds, number schools country, height, mass, blood pressure, temperature, blood glucose level, etc. examples quantitative variables. Quantitative variables may characterized whether \ndiscrete continuous","code":""},{"path":"introduction.html","id":"discrete-variables","chapter":"1 Introduction","heading":"1.8.2 Discrete variables","text":"variables like family size, number hospital beds, number schools country, etc. can counted. examples discrete variables. Variables can take finite number values called \"discrete variables.\" variable phrased ‚Äúnumber ‚Ä¶‚Äù, discrete, possible list possible\nvalues {0,1, ‚Ä¶}. variable finite number possible values discrete. following example illustrates point. number daily admissions hospital discrete variable since can represented whole number, 0, 1, 2 3. number daily\nadmissions given day number 1.8, 3.96 5.33.","code":""},{"path":"introduction.html","id":"continuous-variables","chapter":"1 Introduction","heading":"1.8.3 Continuous variables","text":"variables like height, mass, blood pressure, temperature, blood glucose level, etc. can measured. examples continuous variables. continuous variable possess gaps interruptions characteristic discrete variable. continuous\nvariable can assume value within specific relevant interval values assumed variable. Notice age continuous since individual age discrete jumps. Weight can measured 35.5, 35.8 kg etc , continuous variable.","code":""},{"path":"introduction.html","id":"categorical-variables","chapter":"1 Introduction","heading":"1.8.4 Categorical variables","text":"variable called categorical measurement scale set categories. example, marital status, categories (single,married, widowed), categorical. Whether employed (yes, ), religious affiliation (Protestant, Catholic, Jewish, Muslim, others, none),\ncolours etc. Categorical variables often called qualitative. can seen categorical variables can neither measured counted.","code":""},{"path":"introduction.html","id":"measurement-scales","chapter":"1 Introduction","heading":"1.9 Measurement scales","text":"Variables can classified according following four levels measurement: nominal, ordinal, interval ratio.","code":""},{"path":"introduction.html","id":"nominal-scale","chapter":"1 Introduction","heading":"1.9.1 Nominal scale","text":"scale measure applies qualitative variables . nominal scale, order required. example,gender nominal, blood group nominal, marital status also nominal. perform arithmetic operations data measured nominal scale.","code":""},{"path":"introduction.html","id":"ordinal-scale","chapter":"1 Introduction","heading":"1.9.2 Ordinal scale","text":"scale also applies qualitative data. \nordinal scale, order necessary. means one category lower next one vice versa. example, Grades ordinal, excellent higher good, turn higher good, . noted , ordinal scale, differences category values meaning.","code":""},{"path":"introduction.html","id":"interval-scale","chapter":"1 Introduction","heading":"1.9.3 Interval scale","text":"scale measurement applies quantitative\ndata . scale, zero point indicate total absence quantity measured. example scale temperature Celsius Fahrenheit scale. Suppose minimum temperatures 3 cities, , B C, particular day 00C, 200C 100C, respectively. clear can find differences temperatures. example, city B 200C hotter city . However, say city temperature. Moreover, say city B twice hot city C, just city B 200C city C 100C. reason , interval scale, ratio two numbers meaningful.","code":""},{"path":"introduction.html","id":"ratio-scale","chapter":"1 Introduction","heading":"1.9.4 Ratio scale","text":"scale measurement also applies quantitative\ndata properties interval scale. addition properties, ratio scale meaningful zero starting point meaningful ratio 2 numbers. example variables measured ratio scale, weight. weighing scale reads 0 kg\ngives indication absolutely weight . zero starting point meaningful. Ram weighs 40 kg Laxman weighs 20 kg, Ram weighs twice Laxman. Another example variable measured ratio scale temperature measured Kelvin scale. true zero point.\nFigure 1.2: Classification variables\n","code":""},{"path":"introduction.html","id":"collection-of-data","chapter":"1 Introduction","heading":"1.10 Collection of Data","text":"first step enquiry (investigation) collection data. data may collected whole population sample . mostly collected sample basis. Collecting data difficult job. enumerator investigator well trained individual collects statistical data. respondents persons information collected.","code":""},{"path":"introduction.html","id":"types-of-data","chapter":"1 Introduction","heading":"1.10.1 Types of Data","text":"two types (sources) collection data:\n* Primary Data\n* Secondary Data","code":""},{"path":"introduction.html","id":"primary-data","chapter":"1 Introduction","heading":"1.10.1.1 Primary Data","text":"Primary data first hand information collected, compiled published organizations purpose. original data character undergone sort statistical treatment.Example: Population census reports primary data collected, complied published population census organization.","code":""},{"path":"introduction.html","id":"secondary-data","chapter":"1 Introduction","heading":"1.10.1.2 Secondary Data","text":"secondary data second hand information already collected organization purpose available present study. Secondary data pure character undergone treatment least .Example: economic survey England secondary data data collected one organization like Bureau Statistics, Board Revenue, banks, etc.","code":""},{"path":"introduction.html","id":"methods-of-collecting-primary-data","chapter":"1 Introduction","heading":"1.11 Methods of Collecting Primary Data","text":"Primary data collected using following methods:","code":""},{"path":"introduction.html","id":"personal-investigation","chapter":"1 Introduction","heading":"1.11.1 Personal Investigation","text":"researcher conducts survey /collects data . data collected way usually accurate reliable. method collecting data \napplicable case small research projects.","code":""},{"path":"introduction.html","id":"through-investigation","chapter":"1 Introduction","heading":"1.11.2 Through Investigation","text":"Trained investigators employed collect data. investigators contact individuals fill \nquestionnaires asking required information. organizations utilize method.","code":""},{"path":"introduction.html","id":"collection-through-questionnaire","chapter":"1 Introduction","heading":"1.11.3 Collection through Questionnaire","text":"Researchers get data local representations agents based upon experience. method quick gives rough estimate.","code":""},{"path":"introduction.html","id":"through-the-telephone","chapter":"1 Introduction","heading":"1.11.4 Through the Telephone","text":"Researchers get information individuals telephone. method quick gives accurate information.","code":""},{"path":"introduction.html","id":"methods-of-collecting-secondary-data","chapter":"1 Introduction","heading":"1.12 Methods of Collecting Secondary Data","text":"Secondary data collected following methods:","code":""},{"path":"introduction.html","id":"official","chapter":"1 Introduction","heading":"1.12.1 Official","text":"Publications Statistical Division, Ministry Finance, Federal Bureaus Statistics, Ministries Food, Agriculture, Industry, Labor, etc.","code":""},{"path":"introduction.html","id":"semi-official","chapter":"1 Introduction","heading":"1.12.2 Semi-Official","text":"Publications State Bank, Railway Board, Central Cotton Committee, Boards Economic Enquiry etc.Publication Trade Associations, Chambers Commerce, etc.Technical Trade Journals Newspapers.Research Organizations universities institutions.","code":""},{"path":"introduction.html","id":"difference-between-primary-and-secondary-data","chapter":"1 Introduction","heading":"1.13 Difference Between Primary and Secondary Data","text":"difference primary secondary data change hand. Primary data first hand information directly collected form one source. original character undergone sort statistical treatment, secondary data obtained sources agencies. pure character undergone treatment least .","code":""},{"path":"introduction.html","id":"frequency-distribution","chapter":"1 Introduction","heading":"1.14 Frequency distribution","text":"\nFigure 1.3: raw data set .¬†children 54 families\n\nFigure 1.4: Frequency distribution table\nNow certain features data become apparent. instance, can easily seen , 54 families selected two children number houses 2 children 18. information easily obtained raw data. table called frequency table frequency distribution. called gives frequency number times \nobservation occurs. Thus, finding frequency observation, intelligible picture obtained.","code":""},{"path":"introduction.html","id":"construction-of-frequency-distribution","chapter":"1 Introduction","heading":"1.14.1 Construction of frequency distribution","text":"List values variable ascending order magnitude.List values variable ascending order magnitude.Form tally column, , value data, record stroke tally column next value. tally, fifth stroke made across first four. makes easy count entries enter frequency observation.Form tally column, , value data, record stroke tally column next value. tally, fifth stroke made across first four. makes easy count entries enter frequency observation.Check frequencies sum total number observationsCheck frequencies sum total number observations","code":""},{"path":"introduction.html","id":"grouped-frequency-distribution","chapter":"1 Introduction","heading":"1.15 Grouped frequency distribution","text":"Data gives body masses 22 patients, measured nearest kilogram.\nFigure 1.5: Body masses 22 patients\ncan seen minimum maximum body masses 42 kg 83 kg, respectively. frequency distribution giving every body mass 42 kg 83 kg long informative. problem overcome grouping data classes.\nchoose classes\n41 ‚Äì 49\n50 ‚Äì 58\n59 ‚Äì 67\n68 ‚Äì 76\n77 ‚Äì 85, obtain frequency distribution given :\nFigure 1.6: Grouped Frequency distribution table\ntable gives frequency group class; therefore called grouped frequency table grouped frequency distribution. Using grouped frequency distribution, easier obtain information data using raw data. instance, can seen 17 22 patients body masses 50 kg 76 kg (inclusive). information easily obtained raw data.\nnoted , even though table concise, information lost. example, grouped frequency distribution give us exact body masses patients. Thus individual body masses patients lost effort obtain overall picture.","code":""},{"path":"introduction.html","id":"terms-used-in-grouped-frequency-tables.","chapter":"1 Introduction","heading":"1.16 Terms used in grouped frequency tables.","text":"Class limitsThe intervals observations put called class intervals. end points class intervals called class limits. example, class interval 41 ‚Äì 49, lower class limit 41 upper class limit 49.Class boundariesThe raw data example recorded nearest kilogram. Thus, body mass 49.5kg recorded 50 kg, body mass 58.4 kg recorded 58 kg, body mass 58.5 kg recorded 59 kg. can therefore seen\n, class interval 50 ‚Äì 58, consists measurements greater equal 49.5 kg less 58.5 kg. numbers 49.5 58.5 called lower upper boundaries class interval 50 ‚Äì 58. class boundaries class intervals given :\nFigure 1.7: Class boundary class limits\nNote:\nNotice lower class boundary ith class interval mean lower class limit class interval upper class limit (-1)th class interval (= 2, 3, 4, ‚Ä¶). example, table lower class boundaries second fourth class intervals (50 + 49) /2 = 49.5 (68 + 67)/2 = 67.5 respectively.\ncan also seen upper class boundary ith class interval mean upper class limit class interval lower class limit (+1)th class interval (= 1, 2, 3, ‚Ä¶). Thus, table upper class boundary fourth class\ninterval (76 + 77)/2 = 76.5.Class mark\nmid-point class interval called class mark class mid-point class interval. average upper lower class limits class interval. also average upper lower class boundaries class interval. example, table, class mark third class interval found \nfollows: class mark =(59+67) /2 = (58.5 + 67.5)/2= 63.Class width\ndifference upper lower class boundaries class interval called class width class interval. Class widths class intervals can also found subtracting two consecutive lower class limits, subtracting two consecutive upper class limits.Note:width ith class interval numerical difference upper class limits ith ( -1)th class intervals (= 2, 3, ‚Ä¶). also numerical difference lower class limits ith (+1) th class intervals (= 1, 2, ‚Ä¶).grouped frequency table width first class interval |41-50| = 9. numerical difference lower class limits first second class intervals. width second class interval |50-59|= 9. numerical difference lower class limits second third\nclass intervals. also equal |58-49| numerical, difference upper class limits first second class intervals.","code":""},{"path":"introduction.html","id":"construction-of-frequency-distribution-table","chapter":"1 Introduction","heading":"1.17 Construction of frequency distribution table","text":"Step 1. Decide many classes wish use.Step 2. Determine class widthStep 3. Set individual class limits\nStep 4. Tally items classesStep 5. Count number items classConsider example\nagricultural student measured lengths leaves oak tree (nearest cm). Measurements 38 leaves follows\n9,16,13,7,8,4,18,10,17,18,9,12,5,9,9,16,1,8,17,1,10,5,9,11,15,6,14,9,1,12,5,16,4,16,8,15,14,17Step 1. Decide many classes wish use.H.. Sturges provides formula determining approximation number classes. \\(\\mathbf{k = 1 + 3.322}\\mathbf{\\log}\\mathbf{N}\\). Number classes greater calculated k\nexample N=38, k=1+3.322√ólog(38) = 1+3.322√ó1.5797 = 6.24 = approx 7So approximated number classes less 6.24 .e.\\(\\ k^{'}\\) =7Step 2. Determine class widthGenerally, class width size classes. C= | max ‚àí min|/ k. Class width \\(C^{'}\\)greater calculated C. example, C = | 18‚àí 1|/6.24 = 2.72, approximately class width\\(C^{'} =\\) 3 (Note k used calculated value using Struges formula approximated).Step 3. set individual class limits, need find lower limit \\[L = min - \\frac{C^{'} \\times k^{'} - (max - min)}{2}\\]C k final approximated class width number classes respectively example \\(L = 1 - \\frac{3 \\times 7 - (18 - 1)}{2}\\)=1-2=-1; since negative values data = 0.Even though student measured whole numbers, data continuous, \"4 cm\" means actual value anywhere 3.5 cm 4.5 cm.","code":""},{"path":"introduction.html","id":"cumulative-frequency","chapter":"1 Introduction","heading":"1.18 Cumulative frequency","text":"many situations, interested number observations given class interval, number observations less (greater ) specified value. example, table, can seen 3 leaves length less 3.5 cm 9\nleaves (.e.¬†3 + 6) length less 6.5 cm. frequencies called cumulative frequencies. table cumulative frequencies called cumulative frequency table cumulative frequency distribution.Cumulative frequency defined running total frequencies. Cumulative frequency can also defined sum previous frequencies current point. Notice last cumulative frequency equal sum frequencies. Two types cumulative frequencies Less cumulative frequency Greater cumulative frequency. Less cumulative frequency\n(LCF) number values less specified value. Greater cumulative frequency (GCF) number observations greater specified value.specified value LCF case grouped frequency\ndistribution upper limits GCF lower limits classes. LCF‚Äôs obtained adding frequencies successive classes GCF obtained subtracting successive class frequencies total frequency.","code":""},{"path":"introduction.html","id":"relative-frequency","chapter":"1 Introduction","heading":"1.19 Relative frequency","text":"sometimes useful know proportion, rather number, values falling within particular class interval. obtain information dividing frequency particular class interval total number observations. Relative frequency class\nfrequency class / total observation. Relative frequencies add 1.[1] ‚ÄúNote: = Less cumulative frequency; B= Greater cumulative frequency, C = Relative frequency‚Äù¬†\n¬†\n¬†Data sword 21st century, wield well, Samurai.‚Äù - Jonathan Rosenberg, former Google SVP!","code":""},{"path":"graphical-representation-of-data.html","id":"graphical-representation-of-data","chapter":"2 Graphical representation of data","heading":"2 Graphical representation of data","text":"found information given frequency distribution easier interpret raw data. Information given frequency distribution tabular form easier grasp presented graphically. Many types diagrams used statistics, depending nature data purpose diagram intended.","code":""},{"path":"graphical-representation-of-data.html","id":"histogram","chapter":"2 Graphical representation of data","heading":"2.1 Histogram","text":"histogram consists rectangles :Bases horizontal axis, centres class marks, lengths equal class widths.Bases horizontal axis, centres class marks, lengths equal class widths.Areas proportional class frequencies.Areas proportional class frequencies.Note:\nclass intervals equal size, heights rectangles proportional class frequencies customary take heights rectangles numerically equal class frequencies. class intervals different widths, \nheights rectangles proportional \\(\\frac{\\text{Class Frequency}}{\\text{Class Width}}\\). ratio called frequency density.Table shows frequency distribution body masses 50 AIDS patients. Draw Histogram.\nFigure 2.1: Histogram\n","code":""},{"path":"graphical-representation-of-data.html","id":"cumulative-frequency-curve-ogive","chapter":"2 Graphical representation of data","heading":"2.2 Cumulative frequency curve (Ogive)","text":"graph obtained plotting cumulative frequency class boundary joining points smooth curve, called cumulative frequency curve. also called Ogive. Two types ogive , Less Type Cumulative Frequency Curve (Less Ogive) Greater Type Cumulative Frequency Curve (Greater Ogive).","code":""},{"path":"graphical-representation-of-data.html","id":"less-than-ogive","chapter":"2 Graphical representation of data","heading":"2.2.1 Less than Ogive","text":"Also known less type cumulative frequency curve. use upper limit classes less cumulative frequency plot curve. Let us see example body masses 50 AIDS patients.\nFigure 2.2: Less ogive\n","code":""},{"path":"graphical-representation-of-data.html","id":"greater-than-ogive","chapter":"2 Graphical representation of data","heading":"2.2.2 Greater than Ogive","text":"Also known greater type cumulative frequency curve use lower limit classes greater cumulative frequency plot curve.\nFigure 2.3: greater ogive\nNote:\nIntersection ogives gives median","code":""},{"path":"graphical-representation-of-data.html","id":"frequency-polygon","chapter":"2 Graphical representation of data","heading":"2.2.3 Frequency polygon","text":"grouped frequency table can also represented frequency\npolygon, special kind line graph. construct frequency\npolygon, plot graph class frequencies corresponding\nclass mid-points join successive points straight lines. Frequency polygon also obtained joining midpoints histogram shown Fig 2.5.\nFigure 2.4: Frequency polygon\n\nFigure 2.5: Frequency polygon histogram\n","code":""},{"path":"graphical-representation-of-data.html","id":"stem-and-leaf-plot","chapter":"2 Graphical representation of data","heading":"2.3 Stem-and-leaf plot","text":"stem--leaf plot graphical device useful representing relatively small set data takes numerical values. construct stem--leaf plot, partition measurement two parts. first part called stem, second part called leaf. numerical value divided two parts:\nleading digits become stem trailing digits become leaf. One advantage stem--leaf display frequency distribution retain value observation. Another distribution data within groups clear. stem--leaf plot conveys similar information histogram. Turned side, shape histogram. fact, since \nstem--leaf plot shows observation,displays information lost histogram. properly\nconstructed stem--leaf plot, like histogram, provides information regarding range data set, shows location highest concentration measurements, reveals presence absence symmetry.Consider example10,15,22,25,28,23,29,31,36,45,48stem leaf plot can drawn shown .\nFigure 2.6: Stem Leaf plot\n","code":""},{"path":"graphical-representation-of-data.html","id":"bar-chart","chapter":"2 Graphical representation of data","heading":"2.4 Bar chart","text":"bar chart bar graph diagram consisting series horizontal vertical bars equal width. bars represent various categories data. three types bar charts, simple bar charts, component bar charts grouped bar charts.","code":""},{"path":"graphical-representation-of-data.html","id":"simple-bar-chart","chapter":"2 Graphical representation of data","heading":"2.4.1 Simple bar chart","text":"simple bar chart, height (length) bar equal value category y-axis represents. example data shows production timber five districts Kerala certain year.\nFigure 2.7: Barchart\n","code":""},{"path":"graphical-representation-of-data.html","id":"component-bar-chart","chapter":"2 Graphical representation of data","heading":"2.4.2 Component bar chart","text":"component bar chart, bar category subdivided component parts; hence name. Component bar charts therefore used show division items components. illustrated following example.Example shows distribution sales agricultural produce Farm 1995, 1996 1997.\nFigure 2.8: Sales data agricultural produce\n\nFigure 2.9: Component bar chart\ncomponent bar chart shows changes component years well comparison total sales different years.","code":""},{"path":"graphical-representation-of-data.html","id":"grouped-bar-chart","chapter":"2 Graphical representation of data","heading":"2.4.3 Grouped bar chart","text":"grouped bar chart, components grouped together drawn side side. illustrate example.\nFigure 2.10: Grouped bar chart\n","code":""},{"path":"graphical-representation-of-data.html","id":"histogram-and-bar-chart","chapter":"2 Graphical representation of data","heading":"2.5 Histogram and Bar chart","text":"","code":""},{"path":"graphical-representation-of-data.html","id":"pie-charts","chapter":"2 Graphical representation of data","heading":"2.6 Pie Charts","text":"pie chart circular graph divided sectors, sector representing different value category. angle sector pie chart proportional value part data represents. bar chart precise pie chart visual comparison categories similar relative frequencies.","code":""},{"path":"graphical-representation-of-data.html","id":"steps-for-constructing-a-pie-chart","chapter":"2 Graphical representation of data","heading":"2.6.1 Steps for constructing a pie chart","text":"Find sum category values.Calculate angle sector category, using following formula.Angle sector category = \\(\\frac{\\text{value category }}{\\text{sum category values}} \\times 360\\)Construct circle mark centre.Use protractor divide circle sectors, using angles obtained step 2.Label sector clearly.See example:\nlady spent following sums money buying ingredients family Christmas cake.\nFigure 2.11: Pie chart\n¬†\n¬†\n¬†Statistics grammar science.‚Äù - Karl Pearson!","code":""},{"path":"measures-of-central-tendency---i.html","id":"measures-of-central-tendency---i","chapter":"3 Measures of central tendency - I","heading":"3 Measures of central tendency - I","text":"previous lecture, learnt data can summarised \nform tables presented form graphs important\nfeatures can illustrated easily effectively. \nLecture, consider statistical measures can used describe\ncharacteristics set data.interested single value serves representative\nvalue overall data. ¬†measure central tendency¬†\nsummary statistic represents centre point typical value \ndataset.five averages. Among mean, median mode called\nsimple averages two averages geometric mean harmonic\nmean called special averages. measures reflect numerical\nvalues centre set data therefore called measures\ncentral tendency.Requisites Good Measure Central Tendency:rigidly defined.rigidly defined.simple understand & easy calculateIt simple understand & easy calculateIt based upon values given dataIt based upon values given dataIt capable mathematical treatment.capable mathematical treatment.sampling stability.sampling stability.unduly affected extreme valuesIt unduly affected extreme valuesThe main objectives Measure Central TendencyTo condense data single value.condense data single value.facilitate comparisons data.facilitate comparisons data.","code":""},{"path":"measures-of-central-tendency---i.html","id":"arithmetic-mean","chapter":"3 Measures of central tendency - I","heading":"3.1 Arithmetic Mean","text":"people usually intend say \"average\". Arithmetic\nmean simply mean variable defined sum \nobservations divided number observations. Mean set \nnumbers \\(x_{1\\ },x_{2},\\ldots,x_{n}\\) denoted \\(\\overline{x}\\). \ngiven formula\\[\\overline{x} = \\frac{x_{1} + x_{2} + \\ldots + x_{n}}{n}\\]\\(= \\frac{1}{n}\\sum_{= 1}^{n}x_{}\\)Example 3.1 Find mean numbers 2, 4, 7, 8, 11, 12\\[\\overline{x} = \\frac{2 + 4 + 7 + 8 + 11 + 12}{6} = \\frac{44}{6} = 7.33\\]","code":""},{"path":"measures-of-central-tendency---i.html","id":"the-mean-of-a-frequency-distribution","chapter":"3 Measures of central tendency - I","heading":"3.1.1 The mean of a frequency distribution","text":"","code":""},{"path":"measures-of-central-tendency---i.html","id":"direct-method","chapter":"3 Measures of central tendency - I","heading":"3.1.1.1 Direct method","text":"numbers \\(x_{1\\ },x_{2},\\ldots,x_{n}\\) occur frequencies\n\\(f_{1\\ },f_{2},\\ldots,f_{n}\\) respectively \\[\\overline{x} = \\frac{x_{1}f_{1} + x_{2}f_{2\\ \\ } + \\ldots + x_{n}f_{n}}{f_{1} + f_{2} + \\ldots f_{n}}\\]\\[= \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}}\\]Example 3.2 Table shows body masses 50 men. Find \nmean body mass.Table 3.1:  Body masses 50 men.Solution 3.2The calculation can arranged shown\\(\\overline{x} = \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}} = \\frac{3054}{50}\\)=\n61.08 kg","code":""},{"path":"measures-of-central-tendency---i.html","id":"assumed-mean-method-indirect-method","chapter":"3 Measures of central tendency - I","heading":"3.1.1.2 Assumed mean method (Indirect method)","text":"amount computation involved can reduced using \nfollowing formula:\\[\\overline{x} = + \\frac{\\sum_{= 1}^{n}{f_{}d_{}}}{\\sum_{= 1}^{n}f_{}}\\]\\(\\) assumed mean, can value x.\n\\(d_{} = x_{} - \\), \\(f_{}\\) frequency \\(x_{}\\)Consider Example 2.2 see Table: 3.1let \\(\\) = 61; can number x\\(\\overline{x} = 61 + \\frac{4}{50}\\) = 61.08 kgThe mean mass 61.08 kg","code":""},{"path":"measures-of-central-tendency---i.html","id":"mean-of-grouped-data","chapter":"3 Measures of central tendency - I","heading":"3.1.2 Mean of Grouped Data","text":"","code":""},{"path":"measures-of-central-tendency---i.html","id":"direct-method-1","chapter":"3 Measures of central tendency - I","heading":"3.1.2.1 Direct method","text":"mean grouped data obtained following formula:\\[\\overline{x} = \\frac{\\sum_{= 1}^{k}{f_{}x_{}}}{n}\\]\\(x_{}\\) = mid-point ith class (ith class mark);\n\\(f_{}\\)= frequency ith class; \\(n\\) = sum \nfrequencies total frequencies sample. Note =1,2...,\nk, .e. k classes.Example 3.3 Shows distribution marks scored 60 students Physics examination. Find mean mark.Table 3.2:  Distribution marks scored 60 studentsSolution 3.3The solution can arranged shown\\(\\overline{x} = \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}} = \\frac{4365}{60}\\)=\n72.75The mean mark 72.75%","code":""},{"path":"measures-of-central-tendency---i.html","id":"coding-method-indirect-method","chapter":"3 Measures of central tendency - I","heading":"3.1.2.2 Coding method (Indirect method)","text":"class intervals grouped frequency distribution equal size \\(C\\) (class width); following formula can used instead direct method . formula makes calculations easier.\\[\\overline{x} = + C\\frac{\\sum_{= 1}^{n}{f_{}u_{}}}{\\sum_{= 1}^{n}f_{}}\\]\\(\\) class mark highest frequency,\n\\(u_{} = \\frac{x_{} - }{C}\\), \\(f_{}\\) frequency \\(x_{}\\), C\nclass widthThis called ‚Äúcoding‚Äù method computing mean. short method always used finding mean grouped frequency distribution equal class widths.Consider Example 3.3 see Table:3.2\\(\\)=72.5, class mark highest frequency; \\(C\\) =5\\(\\overline{x} = 72.5 + 5 \\times \\left( \\frac{3}{60} \\right)\\)= 72.75The mean mark 72.75%","code":""},{"path":"measures-of-central-tendency---i.html","id":"merits-and-demerits-of-arithmetic-mean-merits","chapter":"3 Measures of central tendency - I","heading":"3.2 Merits and demerits of Arithmetic mean Merits","text":"Merits rigidly defined.rigidly defined.easy understand easy calculate.easy understand easy calculate.number items sufficiently large, accurate\nreliable.number items sufficiently large, accurate\nreliable.calculated value based position \nseries.calculated value based position \nseries.possible calculate even details data\nlacking.possible calculate even details data\nlacking.averages, affected least fluctuations sampling.averages, affected least fluctuations sampling.provides good basis comparison.provides good basis comparison.DemeritsIt obtained inspection located frequency\ngraph.obtained inspection located frequency\ngraph.study qualitative phenomena capable \nnumerical measurement .e. Intelligence, beauty, honesty etc.study qualitative phenomena capable \nnumerical measurement .e. Intelligence, beauty, honesty etc.can ignore single item risk losing \naccuracy.can ignore single item risk losing \naccuracy.affected much extreme values.affected much extreme values.calculated open-end classes.calculated open-end classes.may lead fallacious conclusions, details data\ncomputed given.may lead fallacious conclusions, details data\ncomputed given.","code":""},{"path":"measures-of-central-tendency---i.html","id":"the-median","chapter":"3 Measures of central tendency - I","heading":"3.3 The median","text":"median set data defined middle value \ndata arranged order magnitude. ties, half \nobservations smaller median, half \nobservations larger median. median can \nmiddle item divides group two equal parts, one part\ncomprising values greater, , values less \nitem. positional measure.","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-ungrouped-or-raw-data","chapter":"3 Measures of central tendency - I","heading":"3.3.1 Median of ungrouped or raw data","text":"Arrange given n observations \\(x_{1\\ },x_{2},\\ldots,x_{n}\\) \nascending order. number values odd, median middle\nvalue. number values even, median mean middle two\nvalues.Arrange data ascending use following formulaWhen n odd, Median = Md\n=\\(\\left( \\frac{n + 1}{2} \\right)^{\\text{th}}\\)valueWhen n even, Median = Md\n=\\({\\text{Average\\ \\ }\\left( \\frac{n}{2} \\right)^{th}\\text{\\ }\\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}}\\)valueExample 3.4 Find median following sets \nnumbers.\\(\\) 12, 15, 22, 17, 20, 26, 22, 26, 12\\(b\\) 4, 7, 9, 10, 5, 1, 3, 4, 12, 10Solution 3.4\\(\\) Arranging data increasing order magnitude, \nobtain 12, 12, 15, 17, 20, 22, 22, 26, 26. , N (= 9) odd, ,\nmedian =\\(\\left( \\frac{9 + 1}{2} \\right)^{\\text{th}}\\)= 5th ordered\nobservation = 20.Note: number repeated, still count number times \nappears calculate median.\\(b\\) Arranging data increasing order magnitude, obtain\n1, 3, 4, 4, 5, 7, 9, 10, 10, 12. , N(=10) even number \nmedian = \\(\\frac{1}{2}\\){5th ordered observation + 6th ordered\nobservation} = \\(\\frac{1}{2}\\left( 5 + 7 \\right) = 6\\).Note: can see case, median divides distribution \ntwo equal parts, 50% observations greater \n50% less .","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-ungrouped-frequency-distribution","chapter":"3 Measures of central tendency - I","heading":"3.3.2 Median of ungrouped frequency distribution","text":"median middle number ordered set data. \nfrequency table, observations already arranged ascending\norder. can obtain median looking value middle\nposition.","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-a-frequency-table-when-the-number-of-observations-is-odd","chapter":"3 Measures of central tendency - I","heading":"3.3.2.1 Median of a frequency table when the number of observations is odd","text":"number observations (n) odd, median value\n¬†\\(\\left( \\frac{n + 1}{2} \\right)^{\\text{th}}\\) positional value.\nuse less cumulative frequency.Example 3.5: following frequency table score obtained mathematics quiz. Find median score.Table 3.3:  Score obtained mathematics quiz.Solution 3.5:Total frequency = 3 + 4 + 7 + 6 + 3 = 23 (odd number). Since number\nscores odd, median \n\\(\\left( \\frac{23 + 1}{2} \\right)^{\\text{th}} =\\) 12th¬†position. find\n12th¬†position, use less cumulative frequencies \nshown:12th¬†position 7th¬†position \n14th¬†position. , median 2.","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-a-frequency-table-when-the-number-of-observations-is-even","chapter":"3 Measures of central tendency - I","heading":"3.3.2.2 Median of a frequency table when the number of observations is even","text":"number observations even, median average\n\n\\({\\left( \\frac{n}{2} \\right)^{th}\\text{\\ }\\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}}\\)\nposition values.Example 3.6: table frequency table marks obtained competition. Find median score.Table 3.4:  Distribution marks obtained competition.Solution 3.6:Total frequency = 11 + 9 + 5 + 10 + 15 = 50 (even number). Since \nnumber scores even, median average ¬†values \n\\({\\left( \\frac{n}{2} \\right)^{th} = 25\\ \\ \\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}} = 26\\)\npositions. find 25th¬†position 26th¬†position, add frequencies shown:mark 25th¬†position 2 mark 26th¬†position\n3. median average scores 25th¬†\n26th¬†positions =¬†\\(\\frac{2 + 3}{2} = 2.5\\)","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-grouped-frequency-distribution","chapter":"3 Measures of central tendency - I","heading":"3.3.3 Median of grouped frequency distribution","text":"exact value median grouped data obtained\nactual values grouped data known. grouped\nfrequency distribution, median class interval \ncontains \\(\\left( \\frac{N}{2} \\right)^{\\text{th}}\\)ordered\nobservation, \\(N\\) total number observations. class\ninterval called median class. median grouped\nfrequency distribution can estimated either following two\nmethods:","code":""},{"path":"measures-of-central-tendency---i.html","id":"linear-interpolation-method-for-estimating-the-median","chapter":"3 Measures of central tendency - I","heading":"3.3.3.1 Linear interpolation method for estimating the median","text":"median grouped frequency distribution can estimated \nlinear interpolation. assume observations evenly spread\nmedian class. median can computed using \nfollowing formula:\\[median = L + \\left( \\frac{\\frac{1}{2}N - F}{f_{m}} \\right)C\\]\\(N\\) = total number observations, \\(L\\) = lower limit \nmedian class, \\(F\\) = sum frequencies L(cumulative\nfrequency), \\(f_{m}\\) = frequency median class, \\(C\\) = class width\nmedian class.","code":""},{"path":"measures-of-central-tendency---i.html","id":"estimation-of-the-median-from-a-cumulative-frequency-curve","chapter":"3 Measures of central tendency - I","heading":"3.3.3.2 Estimation of the median from a cumulative frequency curve","text":"\nFigure 3.1: median cumulative frequency curve\nExample 3.7 Table gives distribution heights 60\nstudents Senior High school. Find median height studentsTable 3.5: Distribution heights 60\nstudentsSolution 3.7() Linear interpolation method estimating median\\(N\\) = 60Median class= class interval contains \n\\(\\left( \\frac{N}{2} \\right)^{\\text{th}}\\)ordered observation; \n\\(\\left( \\frac{60}{2} \\right)^{\\text{th}} =\\) 30th observation. \nclass 160-165 3+9+16=28 observations 30th observation\nclass 160-165, therefore median class.\\(L\\) = lower limit median class =160\\(F\\) = sum frequencies 160(cumulative frequency) = 16+9+3=\n28\\(f_{m}\\) = frequency median class=18\\(C\\) = class width median class=5\\(median = 160 + \\left( \\frac{\\frac{1}{2}60 - 28}{18} \\right)5\\) = 160.56(ii) Estimation median cumulative frequency curve\nFigure 3.2: Median cumulative frequency curve Example 3.7\n","code":""},{"path":"measures-of-central-tendency---i.html","id":"merits-and-demerits-of-median","chapter":"3 Measures of central tendency - I","heading":"3.4 Merits and Demerits of Median","text":"MeritsMedian influenced extreme values \npositional average.Median influenced extreme values \npositional average.Median can calculated case distribution open-end\nintervalsMedian can calculated case distribution open-end\nintervalsMedian can located even data incomplete.Median can located even data incomplete.DemeritsA slight change series may bring drastic change median\nvalue.slight change series may bring drastic change median\nvalue.case even number items continuous series, median \nestimated value value series.case even number items continuous series, median \nestimated value value series.suitable mathematical treatment except use\ncalculating mean deviation.suitable mathematical treatment except use\ncalculating mean deviation.take account observationsIt take account observations","code":""},{"path":"measures-of-central-tendency---i.html","id":"the-mode","chapter":"3 Measures of central tendency - I","heading":"3.5 The mode","text":"mode set data value occurs greatest\nfrequency. mode therefore common value. mode \nimportant measure case qualitative data. mode can used \ndescribe quantitative qualitative data.","code":""},{"path":"measures-of-central-tendency---i.html","id":"mode-of-ungrouped-or-raw-data","chapter":"3 Measures of central tendency - I","heading":"3.5.1 Mode of ungrouped or raw data","text":"ungrouped data series individual observations, mode often\nfound mere inspection.Example 3.8\\(\\) mode 1, 2, 2, 2, 3 2.\\(b\\) modes 2, 3, 4, 4, 5, 5 4 5.\\(c\\) mode exist every observation frequency. example, following sets data modes: () 3,\n6, 8, 9; (ii) 4, 4, 4, 7, 7, 7, 9, 9, 9.Note: can seen mode distribution may exist, \neven exists, may unique. Distributions single\nmode referred unimodal. Distributions two modes \nreferred bimodal. Distributions may several modes, \ncase referred multimodal.Example 3.9 20 patients selected random blood groups\ndetermined. results given table belowTable 3.6:  Blood groups 20 patientsThe blood group highest frequency O. mode data \ntherefore blood group O. can say patients selected\nblood group O. Notice mean median \napplied data. variable ‚Äúblood group‚Äù \ntake numerical values. However, can seen mode can used\ndescribe quantitative qualitative data.","code":""},{"path":"measures-of-central-tendency---i.html","id":"mode-of-grouped-frequency-distribution","chapter":"3 Measures of central tendency - I","heading":"3.5.2 Mode of Grouped frequency distribution","text":"\\[mode = L + \\left( \\frac{f_{s}}{f_{p} + f_{s}} \\right)C\\]Locate highest frequency class corresponding frequency\ncalled modal class.\\(L\\) = lower limit model class; \\(f_{p}\\)= frequency \nclass preceding model class; \\(f_{s}\\)= frequency class\nsucceeding model class \\(C\\) = class intervalExample 3.10 frequency distribution weights sorghum\near-heads given table . Calculate mode.Table 3.7: requency distribution weights sorghum ear headsModal class 100-120\\(mode = 100 + \\left( \\frac{35}{38 + 35} \\right)20 =\\) 109.589","code":""},{"path":"measures-of-central-tendency---i.html","id":"mode-using-histogram","chapter":"3 Measures of central tendency - I","heading":"3.5.3 Mode using Histogram","text":"\nFigure 3.3: Median cumulative frequency curve Example 3.10\n","code":""},{"path":"measures-of-central-tendency---i.html","id":"merits-and-demerits-of-mode","chapter":"3 Measures of central tendency - I","heading":"3.6 Merits and Demerits of Mode","text":"MeritsIt readily comprehensible easy compute. case \ncan computed merely inspection.readily comprehensible easy compute. case \ncan computed merely inspection.affected extreme values. can obtained even \nextreme values known.affected extreme values. can obtained even \nextreme values known.Mode can determined distributions open classes.Mode can determined distributions open classes.Mode can located graph also.Mode can located graph also.Mode can used describe quantitative qualitative data.Mode can used describe quantitative qualitative data.DemeritsThe mode unique. , can one mode \ngiven set data.mode unique. , can one mode \ngiven set data.mode set data may existThe mode set data may existIt based upon observation.based upon observation.¬†\n¬†\n¬†statistics boring, ‚Äôve got wrong numbers!","code":""},{"path":"measures-of-central-tendency--ii.html","id":"measures-of-central-tendency--ii","chapter":"4 Measures of central tendency -II","heading":"4 Measures of central tendency -II","text":"","code":""},{"path":"measures-of-central-tendency--ii.html","id":"geometric-mean","chapter":"4 Measures of central tendency -II","heading":"4.1 Geometric mean","text":"geometric mean type average, usually used growth rates,\nlike population growth interest rates. arithmetic mean adds\nitems, geometric mean multiplies items.geometric mean series containing n observations \nnth root product values. \n\\(x_{1},\\ x_{2},\\ldots,\\ x_{n}\\ \\)observations \\[\\mathbf{\\text{Geometric mean}}\\mathbf{,\\ }\\mathbf{GM =}\\sqrt[\\mathbf{n}]{\\mathbf{x}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots}\\mathbf{x}_{\\mathbf{n}}}\\]\\[\\mathbf{=}\\left( \\mathbf{x}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots}\\mathbf{x}_{\\mathbf{n}} \\right)^{\\frac{\\mathbf{1}}{\\mathbf{n}}}\\]\\[\\mathbf{\\log}\\mathbf{\\text{GM}}\\mathbf{=}\\frac{\\mathbf{1}}{\\mathbf{n}}\\mathbf{\\log}\\left( \\mathbf{x}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots}\\mathbf{x}_{\\mathbf{n}} \\right)\\]\\[\\mathbf{=}\\frac{\\mathbf{1}}{\\mathbf{n}}\\left( \\mathbf{\\log}\\mathbf{x}_{\\mathbf{1}}\\mathbf{+}\\mathbf{\\log}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots +}\\mathbf{\\log}\\mathbf{x}_{\\mathbf{n}} \\right)\\]\\[\\mathbf{=}\\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{n}}{\\mathbf{\\log}\\mathbf{x}_{\\mathbf{}}}}{\\mathbf{n}}\\]\\[\\mathbf{\\ GM = Antilog}\\left( \\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{n}}{\\mathbf{\\log}\\mathbf{x}_{\\mathbf{}}}}{\\mathbf{n}} \\right)\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"geometric-mean-for-grouped-frequency-table-data","chapter":"4 Measures of central tendency -II","heading":"4.1.1 Geometric mean for grouped frequency table data","text":"\\[\\mathbf{GM = \\ Antilog}\\left( \\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{k}}{{\\mathbf{f}_{\\mathbf{}}\\mathbf{\\log}}\\mathbf{x}_{\\mathbf{}}}}{\\mathbf{n}} \\right)\\]\\(x_{}\\) mid-value, \\(f_{}\\) frequency , k \nnumber classesExample 4.1: weight sorghum ear heads 45, 60, 48,100,\n65 gms. Find Geometric mean?Solution 4.1:n =5Geometric mean=\\[\\text{Antilog}\\left( \\frac{\\sum_{= 1}^{n}{\\log x_{}}}{n} \\right) =\\]\\[Antilog\\left( \\frac{8.926}{5} \\right) =\\]\\[ Antilog(1.785) = 60.95\\]\n(note: \\(\\text{Antilog}\\left( x \\right) = 10^{x}\\) .e.\n\\[\\text{Antilog}\\left( 1.785 \\right) = \\ 10^{1.785} = 60.95\\]Example 4.2: Geometric mean Frequency DistributionSolution 4.2:\nn =32\\[GM = \\ Antilog\\left( \\frac{\\sum_{= 1}^{k}{{f_{}\\log}x_{}}}{n} \\right)\\]\\[{\\sum_{= 1}^{k}{{f_{}\\log}x_{}} = 57.782\n}\\]\\[{\\text{GM} = \\ Antilog\\left( \\frac{57.782}{32} \\right)  }\\]\\[{= Antilog\\left( 1.8056 \\right)= 10^{1.8056} = 63.92}\\]\nExample 4.3: Geometric mean Grouped Frequency DistributionSolution 4.4:\nn =32\\[GM = \\ Antilog\\left( \\frac{\\sum_{= 1}^{k}{{f_{}\\log}x_{}}}{n} \\right)\\]\\[{\\sum_{= 1}^{k}{{f_{}\\log}x_{}} = 65.787}\\]\n\\[{\\text{GM} = \\ Antilog\\left( \\frac{65.787}{32} \\right)}\\]\n\\[{= Antilog\\left( 2.0558 \\right) = 10^{2.0558} = 113.72}\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"merits-and-demerits-of-geometric-mean","chapter":"4 Measures of central tendency -II","heading":"4.2 Merits and Demerits of Geometric mean","text":"MeritsIt rigidly defined.rigidly defined.based observations series.based observations series.suitable measuring relative changes.suitable measuring relative changes.gives weights small values less weight \nlarge values.gives weights small values less weight \nlarge values.used averaging ratios, percentages determining\nrate gradual increase decrease.used averaging ratios, percentages determining\nrate gradual increase decrease.capable algebraic treatment.capable algebraic treatment.DemeritsIt easy understand.easy understand.difficult calculate.difficult calculate.calculated, number negative values odd.calculated, number negative values odd.calculated, value series zero.calculated, value series zero.times gives value may found series \nimpractical.times gives value may found series \nimpractical.","code":""},{"path":"measures-of-central-tendency--ii.html","id":"harmonic-mean","chapter":"4 Measures of central tendency -II","heading":"4.3 Harmonic mean","text":"Harmonic means often used averaging things like rates (e.g.¬†\naverage travel speed given duration several trips). Harmonic mean\n(HM) set observations defined reciprocal \narithmetic average reciprocal given value.\\(x_{1},\\ x_{2},\\ldots,\\ x_{n}\\ \\)n observations \\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{n}\\frac{1}{x_{}}}\\]case Frequency distribution\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{k}{f_{}\\frac{1}{x_{}}}}\\]\\(x_{}\\) mid-value, \\(f_{}\\) frequency , k \nnumber classes","code":""},{"path":"measures-of-central-tendency--ii.html","id":"steps-in-calculating-harmonic-mean-h.m","chapter":"4 Measures of central tendency -II","heading":"4.3.1 Steps in calculating Harmonic Mean (H.M)","text":"Calculate reciprocal (1/value) every value.Calculate reciprocal (1/value) every value.Find average reciprocals (just add divide \nmany )Find average reciprocals (just add divide \nmany )reciprocal average (=1/average)reciprocal average (=1/average)Example 4.4: given data 5, 10, 17, 24, 30 calculate H.MSolution 4.4:n = 5\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{n}\\frac{1}{x_{}}} = \\frac{5}{0.433824} = 11.525\\]Example 4.5: Number tomatoes per plant given . Calculate\nharmonic mean.Solution 4.5:n =18\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{n}{f_{}\\frac{1}{x_{}}}} = \\frac{18}{0.821898} = 21.90\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"merits-and-demerits-of-harmonic-mean","chapter":"4 Measures of central tendency -II","heading":"4.4 Merits and Demerits of Harmonic mean","text":"MeritsIt rigidly defined.rigidly defined.defined observations.defined observations.amenable algebraic treatment.amenable algebraic treatment.suitable average desired give greater\nweight smaller less weight larger ones.suitable average desired give greater\nweight smaller less weight larger ones.DemeritsIt easily understood.easily understood.difficult compute.difficult compute.summary figure may actual item \nseriesIt summary figure may actual item \nseriesIt gives greater importance small items therefore, useful\nsmall items given greater weightage.gives greater importance small items therefore, useful\nsmall items given greater weightage.rarely used grouped data.rarely used grouped data.","code":""},{"path":"measures-of-central-tendency--ii.html","id":"relation-between-am-gm-and-hm","chapter":"4 Measures of central tendency -II","heading":"4.5 Relation between AM, GM and HM","text":"stands Arithmetic Mean, GM stands Geometric Mean HM\nstands Harmonic Mean; \\[\\mathbf{\\text{}}\\mathbf{\\times}\\mathbf{\\text{HM}}\\mathbf{=}\\mathbf{\\text{GM}}^{\\mathbf{2}}\\]also\\[\\mathbf{\\geq GM \\geq HM}\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"when-to-use-am-gm-and-hm","chapter":"4 Measures of central tendency -II","heading":"4.6 When to use AM, GM and HM?","text":"practical answer depends numbers \nmeasuring.measuring units add linearly sequence; \nlengths, distances, weights, arithmetic mean give \nmeaningful average. example, arithmetic mean height \nweight students class represents average height weight \nstudents class.Harmonic mean give meaningful average, measuring\nunits add reciprocals sequence; speed \ndistance travelled per unit time, capacitance series, resistance \nparallel. example, harmonic mean capacitors series\nrepresents capacitance single capacitor \none capacitor used instead set capacitors series.‚Äôre measuring units multiply sequence; growth\nrates percentages, geometric mean give meaningful\naverage. example, geometric mean sequence different\nannual interest rates 10 years represents interest rate , \napplied constantly ten years, produce amount growth\nprincipal sequence different annual interest rates ten\nyears .","code":""},{"path":"measures-of-central-tendency--ii.html","id":"positional-averages","chapter":"4 Measures of central tendency -II","heading":"4.7 Positional Averages","text":"Positional average series values refers averages \ntaken series represents whole series\nmay positional properties.median, middle value series taken \nrepresentative value. Therefore, median positional average. Mode \nalso positional average modal values frequently\noccurring values directly taken series . \npositional averages include Percentiles, Quartiles \nDecilesNote Arithmetic mean, Harmonic mean Geometric mean termed\nmathematical averages","code":""},{"path":"measures-of-central-tendency--ii.html","id":"quartile","chapter":"4 Measures of central tendency -II","heading":"4.8 Quartiles","text":"median divides set data two equal parts. can also\ndivide set data two parts. ordered set \ndata divided four equal parts, division points called\nquartiles.first lower quartile (\\(\\mathbf{Q}_{\\mathbf{1}}\\)) \nvalue one fourth, 25% observations value.second quartile (\\(\\mathbf{Q}_{\\mathbf{2}}\\)), one-half,\n50% observations value. second quartile equal\nmedian.third upper quartile, (\\(\\mathbf{Q}_{\\mathbf{3}}\\)), \nvalue three-fourths, 75% observations .\\(\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)item\\(\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)itemCalculations quartiles explained using example . See \nexample procedure followed fraction appear \ncalculation.Example 4.6: Compute quartiles data 25, 18, 30, 8, 15, 5,\n10, 35, 40, 45Solution 4.6:First arrange data ascending order5, 8, 10, 15, 18, 25, 30, 35, 40, 45here n = 10\\(\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)itemi.e. \\(Q_{1} = \\left( \\frac{10 + 1}{4} \\right)^{th}\\) = 2.75th item;\nfraction appears use following procedure\\(Q_{1} = \\ \\)2.75th item = 2nd item + 0.75(3rd\nitem ‚Äì 2nd item)given data \\(Q_{1}\\)= 8+0.75(10‚Äì 8) = 9.5\\[\\mathbf{Q}_{\\mathbf{2}}\\mathbf{= median}\\]\\(Q_{2} = \\ \\)(18+25)/2 = 21.5\\(\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)itemi.e. \\(Q_{3} = \\left( 3 \\times \\frac{(10 + 1)}{4} \\right)^{th}\\) =\n8.25th item = 8th item + 0.25(9th item ‚Äì8th item) =\n35+0.25(40-35) =36.25","code":""},{"path":"measures-of-central-tendency--ii.html","id":"quartiles-of-a-discrete-frequency-data","chapter":"4 Measures of central tendency -II","heading":"4.8.1 Quartiles of a discrete frequency data","text":"Find cumulative frequencies.Find cumulative frequencies.Find \\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\)Find \\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\) , \ncorresponding value \\(x\\) \\(Q_{1}\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\) , \ncorresponding value \\(x\\) \\(Q_{1}\\)Find \\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\)Find \\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\) ,\ncorresponding value \\(x\\) \\(Q_{3}\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\) ,\ncorresponding value \\(x\\) \\(Q_{3}\\)Example 4.7: Compute quartiles data given bellowSolution 4.7:n =24\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\) =\n\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\mathbf{\\ }\\)=\n\\(\\left( \\frac{\\mathbf{25}}{\\mathbf{4}} \\right)\\)= 6.25The cumulative frequency value just greater 6.25 7, \\(\\mathbf{x}\\) value corresponding cumulative frequency 7 8. \n\\(\\mathbf{Q}_{\\mathbf{1}}\\)= 8.\\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\) =\n\\(\\left( \\frac{\\mathbf{3}\\mathbf{\\times}\\mathbf{25}}{\\mathbf{4}} \\right)\\)=\n18.75The cumulative frequency value just greater 18.75 20, \\(\\mathbf{x}\\) value corresponding cumulative frequency 20 24. \n\\(\\mathbf{Q}_{\\mathbf{3}}\\)= 24.","code":""},{"path":"measures-of-central-tendency--ii.html","id":"quartiles-of-a-continuous-frequency-data","chapter":"4 Measures of central tendency -II","heading":"4.8.2 Quartiles of a continuous frequency data","text":"Find cumulative frequenciesFind cumulative frequenciesFind \\(\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)Find \\(\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)See cumulative frequencies, value just greater\n\\(\\ \\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\), \ncorresponding class interval called first quartile class.See cumulative frequencies, value just greater\n\\(\\ \\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\), \ncorresponding class interval called first quartile class.Find \\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)Find \\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)See cumulative frequencies value just greater \n\\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{\\ }\\)\ncorresponding class interval called 3rd quartile class.\napply respective formulaeSee cumulative frequencies value just greater \n\\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{\\ }\\)\ncorresponding class interval called 3rd quartile class.\napply respective formulae\\[\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\mathbf{l}_{\\mathbf{1}}\\mathbf{+}\\frac{\\frac{\\mathbf{n}}{\\mathbf{4}}\\mathbf{-}\\mathbf{m}_{\\mathbf{1}}}{\\mathbf{f}_{\\mathbf{1}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{1}}\\]\\[\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\mathbf{l}_{\\mathbf{3}}\\mathbf{+}\\frac{\\mathbf{3}\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{-}\\mathbf{m}_{\\mathbf{3}}}{\\mathbf{f}_{\\mathbf{3}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{3}}\\]\\(l_{1}\\) = lower limit first quartile class\\(f_{1}\\) = frequency first quartile class\\(c_{1}\\) = width first quartile class\\(m_{1}\\) = cumulative frequency preceding first quartile class\\(l_{3}\\)= 1ower limit 3rd quartile class\\(f_{3}\\)= frequency 3rd quartile class\\(c_{3}\\)= width 3rd quartile class\\(m_{3}\\) = cumulative frequency preceding 3rd quartile classExample 4.8: Find quartiles grouped frequency data givenSolution 4.8:\\(\\left( \\frac{n}{4} \\right)\\) = \\(\\frac{204}{4}\\) = 51The cumulative frequency value just greater 51 54 class\n20-30 1st quartile class\\[\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\mathbf{l}_{\\mathbf{1}}\\mathbf{+}\\frac{\\frac{\\mathbf{n}}{\\mathbf{4}}\\mathbf{-}\\mathbf{m}_{\\mathbf{1}}}{\\mathbf{f}_{\\mathbf{1}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{1}}\\]\\[\\mathbf{= 20 +}\\frac{\\mathbf{51 - 29}}{\\mathbf{25}}\\mathbf{\\times 10\\  = 28.8}\\]\\(3\\left( \\frac{n}{4} \\right)\\) = \\(3 \\times \\frac{204}{4}\\) = 153The cumulative frequency value just greater 153 167 class\n60-70 3rd quartile class\\[\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\mathbf{l}_{\\mathbf{3}}\\mathbf{+}\\frac{\\mathbf{3}\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{-}\\mathbf{m}_{\\mathbf{3}}}{\\mathbf{f}_{\\mathbf{3}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{3}}\\]\\[\\mathbf{= 60 +}\\frac{\\mathbf{153 - 145}}{\\mathbf{22}}\\mathbf{\\times 10 = 63.63}\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"percentiles","chapter":"4 Measures of central tendency -II","heading":"4.9 Percentiles","text":"percentile values divide ordered set data 100 equal parts\ncontaining 1 percent observations. xth percentile,\ndenoted \\(P_{x}\\) value x percent values \ndistribution fall. may noted median 50th\npercentile, 25th percentile first quartile \\(Q_{1}\\) 75th\npercentile \\(\\text{\\ Q}_{3}\\ \\)raw data, first arrange n observations increasing order.\nxth percentile given \\(\\mathbf{P}_{\\mathbf{x}}\\mathbf{=}\\left( \\frac{\\mathbf{x}\\left( \\mathbf{n + 1} \\right)}{\\mathbf{100}} \\right)^{\\mathbf{\\text{th}}}\\)itemFor frequency distribution xth percentile given \nfollowing stepsFind cumulative frequenciesFind cumulative frequenciesFind \\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)Find \\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)\ncorresponding class interval called Percentile class.See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)\ncorresponding class interval called Percentile class.Use following formulaUse following formula\\[\\mathbf{P}_{\\mathbf{x}}\\mathbf{= l +}\\frac{\\left( \\frac{\\mathbf{x \\times n}}{\\mathbf{100}} \\right)\\mathbf{- cf}}{\\mathbf{f}}\\mathbf{\\times c}\\]\\(\\mathbf{l}\\) = lower limit percentile class\\(\\mathbf{\\text{cf}}\\) = cumulative frequency preceding percentile\nclass\\(\\mathbf{f}\\) = frequency percentile class\\(\\mathbf{c}\\) = class interval\\(\\mathbf{n}\\) = total number observationsExample 4.9: Compute \\(\\mathbf{P}_{\\mathbf{25}}\\)\n\\(\\mathbf{P}_{\\mathbf{75}}\\) data 25, 18, 30, 8, 15, 5, 10, 35,\n40, 45Solution 4.9:First arrange data ascending order5, 8, 10, 15, 18, 25, 30, 35, 40, 45Here n =10\\(\\mathbf{P}_{\\mathbf{25}}\\mathbf{=}\\left( \\frac{\\mathbf{25}\\left( \\mathbf{10 + 1} \\right)}{\\mathbf{100}} \\right)^{\\mathbf{\\text{th}}}\\)=\n2.75th item\\(P_{25} = \\ \\)2.75th item = 2nd item + 0.75(3rd\nitem ‚Äì 2nd item)given data \\(P_{25}\\)= 8+0.75(10‚Äì 8) = 9.5\\(\\mathbf{P}_{\\mathbf{75}}\\mathbf{=}\\left( \\frac{\\mathbf{75}\\left( \\mathbf{10 + 1} \\right)}{\\mathbf{100}} \\right)^{\\mathbf{\\text{th}}}\\)=\n8.25th itemi.e. \\(P_{75} = \\left( 75 \\times \\frac{10 + 1}{100} \\right)^{th}\\) =\n8.25th item = 8th item + 0.25(9th item ‚Äì8th item) =\n35+0.25(40-35) =36.25Note: Data example Example 3.6; can seen\n\\(P_{25} = Q_{1}\\) & \\(P_{75} = Q_{3}\\) always","code":""},{"path":"measures-of-central-tendency--ii.html","id":"deciles","chapter":"4 Measures of central tendency -II","heading":"4.10 Deciles","text":"Deciles similar quartiles. quartiles three points\ndivide ordered set data four quarters, deciles 9\npoints divide ordered set data ten equal parts. \nxth decile denoted \\(\\text{\\ d}_{x}\\). may noted \nmedian 5thdecile.\\(\\mathbf{d}_{\\mathbf{x}}\\mathbf{=}\\left( \\frac{\\mathbf{x}\\left( \\mathbf{n + 1} \\right)}{\\mathbf{10}} \\right)^{\\mathbf{\\text{th}}}\\)itemFor frequency distribution xth decile given following\nstepsFind cumulative frequenciesFind cumulative frequenciesFind \\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)Find \\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)corresponding\nclass interval called decile class.See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)corresponding\nclass interval called decile class.Use following formulaUse following formula\\[\\mathbf{d}_{\\mathbf{x}}\\mathbf{= l +}\\frac{\\left( \\frac{\\mathbf{x \\times n}}{\\mathbf{10}} \\right)\\mathbf{- cf}}{\\mathbf{f}}\\mathbf{\\times c}\\]\\(\\mathbf{l}\\) = lower limit decile class\\(\\mathbf{\\text{cf}}\\) = cumulative frequency preceding decile class\\(\\mathbf{f}\\) = frequency decile class\\(\\mathbf{c}\\) = class interval\\(\\mathbf{n}\\) = total number observationsThe best thing statistician get play everybody else‚Äôs backyard.\n‚Äì John Tukey","code":""},{"path":"measures-of-dispersion.html","id":"measures-of-dispersion","chapter":"5 Measures of Dispersion","heading":"5 Measures of Dispersion","text":"discussed previous lectures, set data can summarized\nsingle representative value describes central value \ndata. Consider two sets data & B belowYou can see mean, median mode sets & B 3See dot diagrams data sets B.\nFigure 5.1: Scatter diagram data sets & B\ncan seen , values data set grouped close \nmean, values data set B spread . \nsay values data set B dispersed (scattered) \ndata set AThis example shows mean, mode median, \nenough describing set data. addition using measures,\nneed numerical measure dispersion (variation) set \ndata.Statistical dispersion means extent numerical data \nlikely vary average value.","code":""},{"path":"measures-of-dispersion.html","id":"characteristics-of-a-good-measure-of-dispersion","chapter":"5 Measures of Dispersion","heading":"5.1 Characteristics of a good measure of dispersion","text":"ideal measure dispersion expected possess following\npropertiesIt rigidly definedIt rigidly definedIt based items.based items.unduly affected extreme items.unduly affected extreme items.lend algebraic manipulation.lend algebraic manipulation.simple understand easy calculateIt simple understand easy calculateThe important measures dispersion Range, \nInter-quartile range, Mean Absolute Deviation (MAD)standard deviation.","code":""},{"path":"measures-of-dispersion.html","id":"the-range","chapter":"5 Measures of Dispersion","heading":"5.2 The Range","text":"simplest possible measure dispersion. range set\ndata defined difference largest observation \nsmallest observation set data.Thus,Range = largest observation ‚Äì smallest observation.symbols, Range = L ‚Äì S.L = Largest value; S = Smallest value.individual observations discrete series, L S easily\nidentified. continuous series, following two methods \nfollowed.","code":""},{"path":"measures-of-dispersion.html","id":"method-1","chapter":"5 Measures of Dispersion","heading":"5.2.1 Method 1","text":"L = Upper boundary highest classS = Lower boundary lowest class.","code":""},{"path":"measures-of-dispersion.html","id":"method-2","chapter":"5 Measures of Dispersion","heading":"5.2.2 Method 2","text":"L = Mid-value highest class.S = Mid-value lowest class.Example 5.1: marks obtained 8 students Mathematics \nPhysics examinations follows:Mathematics: 35, 60, 70, 40, 85, 96, 55, 65.Physics: 50, 55, 70, 65, 89, 68, 72, 80.Find ranges two sets data. Physics marks \ndispersed Mathematics marks?SolutionFor Mathematics,Highest mark = 96, lowest mark = 35, range =96 ‚Äì 35 = 61For Physics,Highest mark = 89, lowest mark = 50, range =89 ‚Äì 50 = 39.mathematics marks wider range Physics marks. \nMathematics marks therefore dispersed Physics marks.Example 5.2: Calculate range following distributionSolutionL = Upper boundary highest class = 75S = Lower boundary lowest class = 60Range = L ‚Äì S = 75 ‚Äì 60 = 15","code":""},{"path":"measures-of-dispersion.html","id":"merits-and-demerits-of-range","chapter":"5 Measures of Dispersion","heading":"5.2.3 Merits and Demerits of Range","text":"MeritsIt simple understand.simple understand.easy calculate.easy calculate.certain types problems like quality control, weather\nforecasts, share price analysis, etc.certain types problems like quality control, weather\nforecasts, share price analysis, etc.DemeritsIt much affected extreme items.much affected extreme items.based two extreme observations.based two extreme observations.calculated open-end class intervals.calculated open-end class intervals.suitable mathematical treatment.suitable mathematical treatment.rarely used measure.rarely used measure.","code":""},{"path":"measures-of-dispersion.html","id":"the-inter-quartile-range-iqr-or-midspread","chapter":"5 Measures of Dispersion","heading":"5.3 The Inter-Quartile Range (IQR) or Midspread","text":"range advantage quick easy calculate.\nHowever, since depends maximum minimum values \nset data, show whole data distributed\ntwo values. range therefore good measure \ndispersion one two values differ greatly \nvalues data. overcome problem, sometimes use \ninter-quartile range. robust measure dispersion \ninter-quartile range. inter-quartile range set data \ndifference upper lower quartiles data. Thus,Inter-Quartile Range (IQR) = Q3 ‚Äì Q1.inter-quartile range set data therefore affected \nvalues data outside Q1 Q3. inter-quartile range\nsometimes used measure dispersion.Example 5.3: Consider two sets data , find IQRA: 3, 4, 5, 6, 8, 9, 10, 12, 15B: 3, 8, 8, 9, 9, 9, 10, 10, 15For data set , Q1 = 5, Q3 = 10; Inter-Quartile Range =10 ‚Äì\n5 = 5For data set B, Q1 = 8, Q3 = 10; Inter-Quartile Range =10 ‚Äì\n8 = 2Since inter-quartile range data set greater \ndata set B, results confirm data set dispersed \ndata set B. can also see Range sets.","code":""},{"path":"measures-of-dispersion.html","id":"mean-absolute-deviation-mad","chapter":"5 Measures of Dispersion","heading":"5.4 Mean Absolute Deviation (MAD)","text":"mean absolute deviation (MAD) measure variability \nindicates average distance observations mean. MAD\nuses original units data, simplifies interpretation.\nLarger values signify data points spread \naverage. Conversely, lower values correspond data points bunching\ncloser . mean absolute deviation also known mean\ndeviation average absolute deviation.calculate mean absolute deviation.Calculate mean.Calculate mean.Calculate difference observation mean take\nabsolute value .e. ignore sign. difference known \nabsolute deviationCalculate difference observation mean take\nabsolute value .e. ignore sign. difference known \nabsolute deviationAdd deviations together.Add deviations together.Divide sum number data points.Divide sum number data points.\\[MAD = \\frac{\\sum_{= 1}^{n}\\left| x_{} - \\overline{x} \\right|}{n}\\]Example 5.4: find mean absolute deviation following 10, 15,\n15, 17, 18, 21Here n = 6 \\(\\sum_{= 1}^{n}\\left| x_{} - \\overline{x} \\right|\\) =\n16 therefore MAD = \\(\\frac{16}{6} = 2.67\\)","code":""},{"path":"measures-of-dispersion.html","id":"merits-and-demerits-of-mad","chapter":"5 Measures of Dispersion","heading":"5.4.1 Merits and Demerits of MAD","text":"MeritsSimple EasySimple EasyDifferent items observations can easily compared mean deviationDifferent items observations can easily compared mean deviationMean deviation better quartile deviation range based observations series.Mean deviation better quartile deviation range based observations series.Mean deviation less affected extreme values series comparing standard deviation.Mean deviation less affected extreme values series comparing standard deviation.Mean deviation rigidly defined. , fixed value.Mean deviation rigidly defined. , fixed value.DemeritsIt becomes difficult compute mean deviation case fractions.becomes difficult compute mean deviation case fractions.Mean deviation applicable algebraic calculations.Mean deviation applicable algebraic calculations.calculated open-end class intervals.calculated open-end class intervals.Mean deviation good measure ignores negative signs deviations.Mean deviation good measure ignores negative signs deviations.","code":""},{"path":"measures-of-dispersion.html","id":"the-variance-and-standard-deviation","chapter":"5 Measures of Dispersion","heading":"5.5 The variance and standard deviation","text":"important measures variability sample variance \nsample standard deviation. x1, x2‚Ä¶ xn sample\nn observations, sample variance denoted s¬≤ \ndefined equation.\\[{\\mathbf{\\text{sample variance}},\\ \\mathbf{s}}^{\\mathbf{2}}\\mathbf{=}\\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)^{\\mathbf{2}}}{\\mathbf{n - 1}}\\]sample standard deviation, s, positive square root \nsample variance.\\[\\mathbf{variance =}\\left( \\mathbf{\\text{standard deviation}} \\right)^{\\mathbf{2}}\\]\\[\\mathbf{standard\\ deviation = \\ }\\sqrt{\\mathbf{\\text{variance}}}\\]Note: sA, standard deviation data set , greater\nsB, standard deviation data set B, data set \ndispersed data set B. noted standard\ndeviation set data non-negative number.Example 5.4: Consider two sets data & B ; find standard\ndeviation?Solution:Mean (\\(\\overline{x}\\)) =\\(\\ \\frac{18}{6} = 3\\)\\[{Sample\\ variane,\\ s}_{}^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1} = \\frac{10}{5} = 2\\]\\[\\text{sample standard deviation,}\\ s_{} = \\ \\sqrt{s_{}^{2}} = \\ \\sqrt{2} = 1.414\\]Mean (\\(\\overline{x}\\)) =\\(\\ \\frac{18}{6} = 3\\)\\[{Sample\\ variane,\\ s}_{B}^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1} = \\frac{54}{5} = 10.8\\]\\[sample\\ standard\\ deviation,\\ s_{B} = \\ \\sqrt{s_{B}^{2}} = \\ \\sqrt{10.8} = 3.29\\]can seen \\(s_{B} > s_{}\\ \\), confirming data set B \ndispersed data set (see dot diagrams)Note: unit measurement sample variance square\nunit measurement data. Sample standard deviation \nunit measurement data. Thus, x measured \ncentimetres (cm), unit measurement sample variance \ncm2 sample standard deviation cm. standard\ndeviation desirable property measuring variability \nunit data.alternative formula computing varianceThe computation s¬≤ requires calculations \\(\\overline{x}\\), n\nsubtractions n squaring adding operations. original\nobservations deviations \\(\\left( x_{} - \\overline{x} \\right)\\) \nintegers, deviations \\(\\left( x_{} - \\overline{x} \\right)\\) may\ndifficult work , several decimals may carried\nensure numerical accuracy. efficient computational formula \ns¬≤ given \\(s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{x_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}x_{} \\right)^{2} \\right\\}\\)Example 5.5: Consider data set ; find standard deviation?Solution:\\(s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{x_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}x_{} \\right)^{2} \\right\\}\\)\n; n = 9\\(s^{2} = \\frac{1}{8}\\left\\{ 700 - {\\frac{1}{9}\\left( 72 \\right)}^{2} \\right\\}\\)\n=15.5\\(s = \\ \\sqrt{15.5} = 3.94\\)","code":""},{"path":"measures-of-dispersion.html","id":"variance-and-standard-deviation-for-grouped-data","chapter":"5 Measures of Dispersion","heading":"5.5.1 Variance and standard deviation for grouped data","text":"","code":""},{"path":"measures-of-dispersion.html","id":"for-discrete-grouped-data","chapter":"5 Measures of Dispersion","heading":"5.5.1.1 For discrete grouped data","text":"\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}x}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}x}_{} \\right)^{2} \\right\\}\\]\\(f_{}\\) frequency ith observationExample 5.6: frequency distributions seed yield 50 sesamum\nplants given . Find standard deviation.Solution:\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}x}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}x}_{} \\right)^{2} \\right\\}\\]\\[{sample\\ variance,\\ s}^{2} = \\frac{1}{50 - 1}\\left\\{ 1537 - \\frac{271^{2}}{50} \\right\\} = 1.3914\\]\\(standard\\ deviation,\\ s = \\sqrt{1.3914}\\) = 1.179","code":""},{"path":"measures-of-dispersion.html","id":"for-continuous-grouped-data","chapter":"5 Measures of Dispersion","heading":"5.5.1.2 For continuous grouped data","text":"\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}d}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}d}_{} \\right)^{2} \\right\\}\\]\\(f_{}\\) frequency ith class, c class\ninterval, \\(d_{} = \\frac{x_{} - }{c}\\), \\(x_{}\\) class mark, \\(\\)\nclass mark highest frequencyExample 5.7: frequency distributions seed yield 50 sesamum\nplants given . Find standard deviationSolution:n =50; c =1A = 5\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}d}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}d}_{} \\right)^{2} \\right\\}\\]\\[{sample\\ variance,\\ s}^{2} = \\frac{1}{49}\\left( 77 - \\frac{\\left( 21 \\right)^{2}}{50} \\right) = \\ 1.3914\\]\\(standard\\ deviation,\\ s = \\sqrt{1.3914}\\) = 1.179","code":""},{"path":"measures-of-dispersion.html","id":"merits-and-demerits-of-standard-deviation","chapter":"5 Measures of Dispersion","heading":"5.5.2 Merits and Demerits of Standard Deviation","text":"MeritsIt rigidly defined value always definite based \nobservationsIt rigidly defined value always definite based \nobservationsAs based arithmetic mean, merits \narithmetic mean.based arithmetic mean, merits \narithmetic mean.important widely used measure dispersion.important widely used measure dispersion.possible algebraic treatment.possible algebraic treatment.less affected fluctuations sampling hence\nstable.less affected fluctuations sampling hence\nstable.basis measuring coefficient correlation \nmeasures.basis measuring coefficient correlation \nmeasures.DemeritsIt easy understand difficult calculate.easy understand difficult calculate.gives weight extreme values values \nsquared .gives weight extreme values values \nsquared .absolute measure variability, used \npurpose comparison.absolute measure variability, used \npurpose comparison.","code":""},{"path":"measures-of-dispersion.html","id":"coefficient-of-variation-relative-measure-of-dispersion","chapter":"5 Measures of Dispersion","heading":"5.6 Coefficient of Variation (relative measure of dispersion)","text":"Standard deviation absolute measure dispersion. \nexpressed terms units original figures collected\nstated. standard deviation heights plants \ncompared standard deviation weights grains, \nexpressed different units, .e heights centimetre \nweights kilograms.Therefore standard deviation must converted relative\nmeasure dispersion purpose comparison. relative\nmeasure known coefficient variation. coefficient \nvariation obtained dividing standard deviation mean \nexpressed percentage.\\[\\mathbf{\\text{Coefficient variation}}\\left( \\mathbf{C}\\mathbf{.}\\mathbf{V} \\right)\\mathbf{=}\\frac{\\mathbf{\\text{standard deviation}}}{\\mathbf{\\text{mean}}}\\mathbf{\\times 100}\\]want compare variability two series, can use\nC.V. series groups data C.V. greater indicate\ngroup variable, less stable, less uniform, less\nconsistent less homogeneous. C.V. less, indicates \ngroup less variable stable uniform \nconsistent homogeneous.Example 5.8: Consider measurement yield plant height \npaddy variety. mean standard deviation yield 50 kg \n10 kg respectively. mean standard deviation plant height \n55 cm 5 cm respectively. Compare variability.Solution:measurements yield plant height different units.\nHence variability can compared using coefficient \nvariation.yield, CV=\\(\\ \\frac{10}{50} \\times 100 =\\) 20%plant height, CV=\n\\(\\frac{5}{55} \\times 100 = \\ \\)9.1%yield subject variation plant height.******************************************************************","code":""},{"path":"skewness-and-kurtosis.html","id":"skewness-and-kurtosis","chapter":"6 Skewness and Kurtosis","heading":"6 Skewness and Kurtosis","text":"previous lectures learned numerical measures central\ntendency dispersion, measures shape?histogram can give general idea shape \ndistribution values data. need numerical measures\nidentify shape distribution. numerical measures \ndeal shape distribution Skewness Kurtosis.","code":""},{"path":"skewness-and-kurtosis.html","id":"skewness","chapter":"6 Skewness and Kurtosis","heading":"6.1 Skewness","text":"Skewness measure symmetry, precisely, lack \nsymmetry. may ask, symmetric distribution looks\nlike. Histogram symmetric distribution showed :\nFigure 6.1: Histogram symmetric distribution\ndistribution, data set, symmetric looks \nleft right centre point. discussion including\nunimodal cases.symmetric distribution skewness = 0; mean = median = mode\nFigure 6.2: symmetric distribution\nExample data set skewness = 0 (symmetric distribution)\nFigure 6.3: Data set skewness = 0\n","code":""},{"path":"skewness-and-kurtosis.html","id":"left-skewed-or-negatively-skewed","chapter":"6 Skewness and Kurtosis","heading":"6.1.1 Left-skewed or negatively skewed","text":"negatively skewed data set distribution, left tail longer;\nmass distribution concentrated right figure.\ndistribution said left-skewed, left-tailed, skewed \nleft, considering long tail left side. See\nfigure , can also see Mean < Median < Mode.\nFigure 6.4: Left skewed negatively skewed distribution\nExample data set negative skewness\nFigure 6.5: Negatively skewed data set\n","code":""},{"path":"skewness-and-kurtosis.html","id":"right-skewed-or-positively-skewed","chapter":"6 Skewness and Kurtosis","heading":"6.1.2 Right-skewed or positively skewed","text":"positively skewed data set distribution, right tail \nlonger; mass distribution concentrated left \nfigure. distribution said right-skewed, right-tailed, \nskewed right, considering long tail right\nside. See figure , can also see Mean > Median > Mode\nFigure 6.6: Right skewed positively skewed distribution\nExample data set positive skewness\nFigure 6.7: Data set positive skewness (right skewed)\n","code":""},{"path":"skewness-and-kurtosis.html","id":"measures-of-skewness","chapter":"6 Skewness and Kurtosis","heading":"6.2 Measures of Skewness","text":"direction extent skewness can measured various ways. \nshall discuss four measures.","code":""},{"path":"skewness-and-kurtosis.html","id":"karl-pearsons-coefficient-of-skewness-s_k","chapter":"6 Skewness and Kurtosis","heading":"6.2.1 Karl Pearson‚Äôs coefficient of Skewness (\\(S_{k}\\))","text":"noticed mean, median mode equal \nskewed distribution. Karl Pearson's measure skewness based\nupon divergence mean mode skewed distribution.\\[S_{k} = \\frac{mean - mode}{\\text{standard deviation}}\\]sign \\(S_{k}\\) gives direction skewness magnitude\ngives extent skewness. \\(S_{k}\\) > 0, distribution \npositively skewed, \\(S_{k}\\) < 0 negatively skewed.formula since mode used, problem mode\ndefined distribution find \\(S_{k}\\). empirical\nrelation mean, median mode states , moderately\nsymmetrical distribution\\(\\ mean - mode \\approx 3(mean - median)\\). \nformula can written \\[S_{k} = \\frac{3(mean - median)}{\\text{standard deviation}}\\]Example 6.1: Compute Karl Pearson's coefficient skewness \nfollowing data:Solution:Mean,\n\\(\\overline{x} = \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}}\\)\n= \\(\\frac{11482}{187} = 61.40\\)\\({sample\\ variance,\\ s}^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}x}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}x}_{} \\right)^{2} \\right\\}\\)=\n\\(\\frac{705588 - \\frac{\\left( 11482 \\right)^{2}}{187}}{186} = 3.123\\)\\(standard\\ deviation,\\ s = \\sqrt{3.123} = 1.179\\)Median: See cumulative frequencies, value just greater\n\\(\\ \\left( \\frac{n + 1}{2} \\right)\\) , corresponding value \n\\(x\\) \\(Q_{2}\\), median\\(\\left( \\frac{n + 1}{2} \\right) = \\frac{187 + 1}{2}\\ \\)= \\(\\frac{188}{2}\\)\n= 94\\[S_{k} = \\frac{3(mean - median)}{\\text{standard deviation}}\\]\\[S_{k} = \\frac{3(61.40 - 61)}{1.179} = \\frac{1.2}{1.179} = 1.017\\]Hence, Karl Pearson's coefficient skewness \\(S_{k}\\)=\\(1.017\\), Thus\ndistribution positively skewed.","code":""},{"path":"skewness-and-kurtosis.html","id":"bowleys-measure-of-skewness-sq","chapter":"6 Skewness and Kurtosis","heading":"6.2.2 Bowley's measure of Skewness (SQ)","text":"Karl Pearson's coefficient skewness commonly used skewness\nmeasure. However, order use must know mean, mode (\nmedian) standard deviation data. Sometimes might \ninformation; instead might information \nquartiles. ‚Äôs case, can use Bowley‚Äôs measure Skewness\nalternative find asymmetry \ndistribution. ‚Äôs useful extreme data values\n(outliers) open-ended distribution.\\[{Bowley‚Äôs\\ measure\\ \\ Skewness,\\ S}_{Q} = \\frac{\\left( Q_{3} - Q_{2} \\right) - \\left( Q_{2} - Q_{1} \\right)}{\\left( Q_{3} - Q_{2} \\right) + \\left( Q_{2} - Q_{1} \\right)}\\]\\(Q_{1}\\)= 1st quartile; \\(Q_{2}\\) = median; \\(Q_{3}\\)= 3rd quartileEquation can modified \\[S_{Q} = \\frac{Q_{3} - 2Q_{2} + Q_{1}}{Q_{3} - Q_{1}}\\]\\(S_{Q}\\)= 0 means curve symmetrical.\\(S_{Q}\\)= 0 means curve symmetrical.\\(S_{Q}\\) > 0 means curve positively skewed.\\(S_{Q}\\) > 0 means curve positively skewed.\\(S_{Q}\\)< 0 means curve negatively skewed.\\(S_{Q}\\)< 0 means curve negatively skewed.Example 6.1 given , Bowley's measure Skewness can \ncalculated followsCalculation \\(\\text{Q}_{1}\\), \\(Q_{2}\\), \\(Q_{3}\\) given Section 4.8\\[{Q}_{1} = 60\\]\\[Q_{2} = 61\\]\\[Q_{3} = 63\\]\\[S_{Q} = \\frac{63 - (2 \\times 61) + 60}{63 - 60} = \\ \\frac{1}{3} = 0.33\\]Since \\(S_{Q}\\) > 0 means curve positively skewed.","code":""},{"path":"skewness-and-kurtosis.html","id":"kellys-measure-of-skewness-sp","chapter":"6 Skewness and Kurtosis","heading":"6.2.3 Kelly's Measure of Skewness (Sp)","text":"Bowley's measure skewness based middle 50% \nobservations; leaves 25% observations extreme \ndistribution. improvement Bowley's measure, Kelly \nsuggested measure based Percentiles, including P10 P90\n10% observations extreme ignored.\\[{Kelly's\\ Measure\\ \\ Skewness,\\ S}_{p} = \\frac{\\left( P_{90} - P_{50} \\right) - \\left( P_{50} - P_{10} \\right)}{\\left( P_{90} - P_{50} \\right) + \\left( P_{50} - P_{10} \\right)}\\]","code":""},{"path":"skewness-and-kurtosis.html","id":"measure-based-on-moments","chapter":"6 Skewness and Kurtosis","heading":"6.2.4 Measure based on moments","text":"going measuring skewness using moments, one know \nmoment :","code":""},{"path":"skewness-and-kurtosis.html","id":"moments","chapter":"6 Skewness and Kurtosis","heading":"6.2.4.1 Moments","text":"rth moment mean distribution, denoted \nŒºr given \\[\\mu_{r} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{r}}}{N}\\]\\(f_{}\\) frequency ith observation class\nmark\\(\\ x_{}\\), \\(N = \\sum_{}^{}f_{}\\), number observationsMoment mean also called Central MomentIf r = 0,\n\\(\\mu_{0} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{0}}}{N}\\)\n= 1If r = 1,\n\\(\\mu_{1} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{1}}}{N}\\)\n= 0 (sum deviation mean zero)r = 2,\n\\(\\mu_{2} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{2}}}{N}\\)\n= \\(\\sigma^{2}\\), Population varianceIn short values following moments mean areMoments meanFor Example 6.1 given , calculate third central moment, \\(\\mu_{3}\\)Mean = 61.40\\[\\mu_{3} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{3}}}{N} = \\ \\frac{49.832}{187} = 0.266\\]","code":""},{"path":"skewness-and-kurtosis.html","id":"moment-measure-of-skewness-mathbfbeta_1textandgamma_1mathbf","chapter":"6 Skewness and Kurtosis","heading":"6.2.4.2 Moment Measure of Skewness \\(\\mathbf{(}\\beta_{1}\\text{and}\\)\\(\\gamma_{1}\\mathbf{)}\\)","text":"moment measure skewness based property , \nsymmetrical distribution, odd ordered central moments equal \nzero. note \\(\\mu_{1}\\) = 0, every distribution, therefore, \nlowest order moment can provide absolute measure skewness\n\\(\\text{Œº}_{3}\\). measures skewness based\n\\(\\text{Œº}_{3}\\).\\[\\beta_{1} = \\frac{\\mu_{3}^{2}}{\\mu_{2}^{3}}\\]Pronounced ‚Äòbeta one‚Äô\\(\\beta_{1}\\)= 0 means curve symmetrical. greater \nvalue \\(\\beta_{1}\\)skewed distribution. One serious\nlimitation \\(\\beta_{1}\\)tell direction \nskewness, .e., whether positive negative. Since\n\\(\\text{Œº}_{2}\\) always positive \\(\\mu_{3}^{2}\\) positive,\n\\(\\beta_{1}\\) positive always. drawback removed \ncalculating\\(\\text{Œ≥}_{1}\\), called Karl Pearson‚Äôs\\(\\text{ Œ≥}_{1}\\),\npronounced ‚Äògamma one‚Äô.\\[\\gamma_{1} = \\sqrt{\\beta_{1}} = \\frac{\\mu_{3}}{\\mu_{2}^{3}}\\]\\(\\mu_{3}\\) positive \\(\\gamma_{1}\\) positive, \\(\\mu_{3}\\) \nnegative \\(\\gamma_{1}\\) negative\\(\\gamma_{1}\\)= 0 means curve symmetrical.\\(\\gamma_{1}\\)= 0 means curve symmetrical.\\(\\gamma_{1}\\) > 0 means curve positively skewed.\\(\\gamma_{1}\\) > 0 means curve positively skewed.\\(\\gamma_{1}\\)< 0 means curve negatively skewed.\\(\\gamma_{1}\\)< 0 means curve negatively skewed.Example 6.1 given , skewness can examined \\(\\mu_{3}\\)= 0.226\\(\\mu_{2}\\)= 3.123\\(\\beta_{1} = \\frac{\\mu_{3}^{2}}{\\mu_{2}^{3}}\\) =\n\\(\\frac{\\left( 0.226 \\right)^{2}}{\\left( 3.123 \\right)^{3}} = \\ \\frac{0.051}{30.46} = 0.0016\\)\\(\\gamma_{1} = \\sqrt{\\beta_{1}} = \\ \\sqrt{0.0016} = + 0.04\\)Since \\(\\mu_{3}\\) positive \\(\\gamma_{1}\\)positive. Since\n\\(\\gamma_{1}\\)slightly greater 0, distribution slightly\nskewed right.","code":""},{"path":"skewness-and-kurtosis.html","id":"kurtosis","chapter":"6 Skewness and Kurtosis","heading":"6.3 Kurtosis","text":"Kurtosis another measure shape distribution. Whereas\nskewness measures lack symmetry frequency curve \ndistribution, kurtosis measure relative peakedness \nfrequency curve. Various frequency curves can divided three\ncategories depending upon shape peak.\nFigure 6.8: Three categories frequency curves depending upon shape peak\nKurtosis refers degree flatness peakedness curve. \nmeasured relative peakedness normal curve. normal curve considered mesokurtic. curve \npeaked normal curve, called leptokurtic. curve \nflat-topped normal curve, called platykurtic.condition peakedness (leptokurtic) flatness (platykurtic) \ncalled kurtosis excess.Measure kurtosis given ‚Äòbeta two‚Äô given Karl Pearson\\(\\beta_{2} = \\frac{\\mu_{4}}{\\mu_{2}^{2}}\\)\\(\\mu_{4}\\) 4th central moment, \\(\\mu_{2}\\) 2nd\ncentral moment\\(\\beta_{2}\\)= 3 means curve mesokurtic.\\(\\beta_{2}\\)= 3 means curve mesokurtic.\\(\\beta_{2}\\) > 3 means curve leptokurtic.\\(\\beta_{2}\\) > 3 means curve leptokurtic.\\(\\beta_{2}\\)< 3 means curve platykurtic.\\(\\beta_{2}\\)< 3 means curve platykurtic.Another measure kurtosis gamma two, \\(\\gamma_{2} = \\beta_{2} - 3\\ \\)\\(\\gamma_{2}\\)= 0 means curve mesokurtic.\\(\\gamma_{2}\\)= 0 means curve mesokurtic.\\(\\gamma_{2}\\) > 0 means curve leptokurtic.\\(\\gamma_{2}\\) > 0 means curve leptokurtic.\\(\\gamma_{2}\\)< 0 means curve platykurtic.\\(\\gamma_{2}\\)< 0 means curve platykurtic.Example 6.1 given , kurtosis can examined followsMean,\\(\\ \\overline{x}\\ \\)= 61.40\\(\\mu_{2}\\) = 3.123 (calculation shown previous example)\\(\\mu_{4} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{4}}}{N} = \\frac{4312.747}{187} = 23.062\\)\\(\\beta_{2} = \\frac{\\mu_{4}}{\\mu_{2}^{2}} = \\frac{23.062}{\\left( 3.123 \\right)^{2}} = 2.364\\)\\(\\beta_{2}\\) 2.364, close 3, distribution can considered\nslightly platykurtic close symmetric.can verify frequency curve Example 6.1 , can seen\nslightly right tailed (positively skewed)\nFigure 6.9: frequency curve Example 6.1\n","code":""},{"path":"measures-of-association.html","id":"measures-of-association","chapter":"7 Measures of Association","heading":"7 Measures of Association","text":"","code":""},{"path":"measures-of-association.html","id":"scatter-diagram","chapter":"7 Measures of Association","heading":"7.1 Scatter Diagram","text":"Consider two variables x y, use scatter diagram \ninvestigate whether relation two variables. \nvariables x y plotted along X-axis Y-axis\nrespectively X-Y plane graph sheet resultant diagram \ndots known scatter diagram. scatter diagram can\nsay whether association X Y.Example 7.1: Consider data Sepal length (x) Sepal width (y) Iris setosa.\nFigure 7.1: Scatter diagram data Example 7.1\n","code":""},{"path":"measures-of-association.html","id":"correlation","chapter":"7 Measures of Association","heading":"7.2 Correlation","text":"Correlation statistical technique used analyzing behaviour\ntwo variables. correlation measures degree \ncloseness linear relationship two variables \nnumerical magnitude.Correlation measure enable us compare linear relationship\ntwo variables using single number. two \nquantities vary related manner movements one tend \naccompanied movements , said \ncorrelated.","code":""},{"path":"measures-of-association.html","id":"positive-correlation","chapter":"7 Measures of Association","heading":"7.2.1 Positive correlation","text":"Positive correlation relationship two variables \nvariables move direction. positive correlation\nexists one variable decreases variable decreases, \none variable increases increases.Examples positive correlation: consider two variables x yThe time spend running treadmill \\[Running time\n(*x*)\\], calories burn \\[calories burned (*y*)\\].\ncan see x increases y also increasesThe time spend running treadmill \\[Running time\n(*x*)\\], calories burn \\[calories burned (*y*)\\].\ncan see x increases y also increasesShorter people \\[Height (*x*)\\] smaller shoe sizes \\[shoe size\n(*y*)\\]. can see x decreases y also decreases.Shorter people \\[Height (*x*)\\] smaller shoe sizes \\[shoe size\n(*y*)\\]. can see x decreases y also decreases.hours spend direct sunlight \\[Hours sunlight\n(*x*)\\], tan \\[melanin content(*y*)\\]. can\nsee x increases y also increasesThe hours spend direct sunlight \\[Hours sunlight\n(*x*)\\], tan \\[melanin content(*y*)\\]. can\nsee x increases y also increasesAs temperature goes \\[Temperature (*x*)\\], ice cream sales\n\\[sales (*y*)\\], also go .temperature goes \\[Temperature (*x*)\\], ice cream sales\n\\[sales (*y*)\\], also go .","code":""},{"path":"measures-of-association.html","id":"negative-correlation","chapter":"7 Measures of Association","heading":"7.2.2 Negative correlation","text":"Negative correlation relationship two variables \none variable increases decreases, vice versa.Examples negative correlation: consider two variables x yA student many absences \\[. days absent (*x*)\\] \ndecrease grades \\[grades (*x*)\\]. can see x\nincreases y decreases.student many absences \\[. days absent (*x*)\\] \ndecrease grades \\[grades (*x*)\\]. can see x\nincreases y decreases.weather gets colder \\[Average monthly temperature (*x*)\\], air\nconditioning costs decrease \\[Price .C (*y*)\\].weather gets colder \\[Average monthly temperature (*x*)\\], air\nconditioning costs decrease \\[Price .C (*y*)\\].chicken increases age \\[chicken age (*x*)\\], number \neggs produces \\[. eggs produced (*y*)\\] decreases.chicken increases age \\[chicken age (*x*)\\], number \neggs produces \\[. eggs produced (*y*)\\] decreases.car decreases speed \\[average car speed(*x*)\\], travel time\n(y) destination increases.car decreases speed \\[average car speed(*x*)\\], travel time\n(y) destination increases.","code":""},{"path":"measures-of-association.html","id":"other-types-of-correlation","chapter":"7 Measures of Association","heading":"7.3 Other types of correlation","text":"","code":""},{"path":"measures-of-association.html","id":"simple-and-multiple","chapter":"7 Measures of Association","heading":"7.3.1 Simple and Multiple","text":"simple correlation relationship\nconfined two variables . multiple correlation \nrelationship two variables judged.","code":""},{"path":"measures-of-association.html","id":"partial-and-total","chapter":"7 Measures of Association","heading":"7.3.2 Partial and total","text":"two types correlations multiple\ncorrelation analysis.partial correlation relationship two variables \nexamined eliminating linear effect correlated\nvariables.total correlation based relevant variables.Correlation measures linear relationship variables","code":""},{"path":"measures-of-association.html","id":"linear-relationship","chapter":"7 Measures of Association","heading":"7.4 Linear relationship","text":"linear relationship (linear association)\nstatistical term used describe straight-line relationship\nvariables.Linear relationships can expressed either graphical format \nvariable plotted X-Y plane gives straight line relation\ntwo variables (consider x y) can expressed \nequation straight line (y = + bx) (**clear\ndiscuss regression)Example 7.2: Consider following example ice cream salesThe local ice cream shop keeps track much ice cream sell\nversus temperature day; figures last\n12 days:rest discussion using example .","code":""},{"path":"measures-of-association.html","id":"methods-of-measurement-of-correlation","chapter":"7 Measures of Association","heading":"7.5 Methods of measurement of correlation","text":"","code":""},{"path":"measures-of-association.html","id":"scatter-diagram-or-graphic-method","chapter":"7 Measures of Association","heading":"7.5.1 Scatter diagram or Graphic method","text":"\nFigure 7.2: Scatter plot Example 7.2\nFigure 7.2 can see linear association two variables .e.\ntemperature ice cream sales. can shown using line \n. clear temperature increases sales increases,\nindicating positive correlation.\nFigure 7.3: Linear relationship variables\nexample clear scatter diagram gives idea \nlinear association variables, can also used graphical\ntool see whether correlation present .Perfect Correlation: change value one\nvariable, value variable changed fixed\nproportion correlation said perfect\ncorrelation. perfect correlation, points lie \nstraight line. scatter diagram Example 7.2, can see \nperfect linear relationship points exactly \nline, points scattered form still \ndirection (positive).Direction correlation can identified using scatter diagram shown Figure 7.4\nFigure 7.4: Scatter plot nature relationship\n","code":""},{"path":"measures-of-association.html","id":"karl-pearsons-coefficient-of-correlation-r","chapter":"7 Measures of Association","heading":"7.5.2 Karl Pearson‚Äôs coefficient of Correlation (r)","text":"important widely used measure correlation. \nmeasure intensity degree linear relationship two\nvariables developed Karl Pearson, British Biometrician - known\nPearson‚Äôs Correlation coefficient denoted r\nexpressed ratio covariance product standard deviations two variables.","code":""},{"path":"measures-of-association.html","id":"covariance","chapter":"7 Measures of Association","heading":"7.5.2.1 Covariance","text":"Covariance measure joint linear variability two\nvariables. Consider two variables x y n observations\n, covariance given formulaCovariance (x,y) =\n\\(\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}\\)covariance = 0 joint variability linear\nrelationship. unit covariance product units \ntwo variables.Covariance two variables x y denoted Cov(x, y).\nCovariance measure used find correlation coefficient.correlation coefficient two variables (x y) \ncalculated \\[r=\\frac{cov(x,y)}{sd(x)sd(y)}\\]sd. standard deviation.\\[r = \\frac{\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}}{\\sqrt{\\frac{1}{n}\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}\\frac{1}{n}\\sum_{= 1}^{n}\\left( y_{} - \\overline{y} \\right)^{2}}}\\]\n#### Properties correlation coefficient (r)pure number independent origin scale \nunits observations.pure number independent origin scale \nunits observations.always lies ‚àí1 +1 (absolute value exceed\nunity). ‚àí1 ‚â§ r ‚â§ +1It always lies ‚àí1 +1 (absolute value exceed\nunity). ‚àí1 ‚â§ r ‚â§ +1r = +1, indicates perfect positive correlation. r = ‚àí1,\nindicates perfect negative correlation. r = 0, indicates \ncorrelation.r = +1, indicates perfect positive correlation. r = ‚àí1,\nindicates perfect negative correlation. r = 0, indicates \ncorrelation.correlation zero linear relationship\nvariables.correlation zero linear relationship\nvariables.meaningful relation variables value\ncorrelation obtained also meaningless. (example \nfertilizer price increases, Kohili‚Äôs batting average also increases,\nknow practical relationship variables,\nstill may get correlation measure called spurious\ncorrelation)meaningful relation variables value\ncorrelation obtained also meaningless. (example \nfertilizer price increases, Kohili‚Äôs batting average also increases,\nknow practical relationship variables,\nstill may get correlation measure called spurious\ncorrelation)Simplified formula computation correlation coefficient can\nderived modifying formulaA Simplified formula computation correlation coefficient can\nderived modifying formula\\[r = \\frac{n\\left( \\sum_{= 1}^{n}{x_{}y_{}} \\right) - \\sum_{= 1}^{n}{x_{}\\sum_{= 1}^{n}y_{}}}{\\sqrt{\\left\\lbrack n\\sum_{= 1}^{n}{x_{}^{2} - \\left( \\sum_{= 1}^{n}x_{} \\right)^{2}} \\right\\rbrack\\left\\lbrack n\\sum_{= 1}^{n}{y_{}^{2} - \\left( \\sum_{= 1}^{n}y_{} \\right)^{2}} \\right\\rbrack}}\\]Example 7.3: Consider Example 7.2 ice cream sales; find\ncorrelation coefficient (r)n =12\\[mean,\\overline{x} = \\ \\frac{224.1}{12} = 18.675\\]\\[mean,\\overline{y} = \\ \\frac{4829}{12} = 402.416\\]Cov (x,y) =\n\\(\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}\\)\\(\\sum_{= 1}^{12}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)} = 5325.03\\)Cov (x,y) = \\(\\frac{5325.03}{12} = 443.752\\)\\[Standard\\ deviation,\\ S.D\\left( x \\right) = \\ \\sqrt{\\frac{1}{n}\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}} = \\sqrt{\\frac{176.983}{12}} = 3.840\\]\\[Standard\\ deviation,\\ S.D\\left( y \\right) = \\ \\sqrt{\\frac{1}{n}\\sum_{= 1}^{n}\\left( y_{} - \\overline{y} \\right)^{2}} = \\sqrt{\\frac{174754.9}{12}} = 120.676\\]\\(r = \\frac{443.752}{3.840\\  \\times 120.676} = 0.95751\\), indicates\nstrong positive correlation","code":""},{"path":"measures-of-association.html","id":"spearmans-rank-order-correlation-coefficient-œÅ","chapter":"7 Measures of Association","heading":"7.5.3 Spearman‚Äôs Rank order correlation coefficient (œÅ)","text":"Spearman correlation evaluates monotonic relationship \ntwo continuous ordinal variables.Note: monotonic relation?monotonic relationship, variables tend move \nrelative direction, necessarily constant rate. linear\nrelationship, variables move direction constant\nrate.Linear relationship monotonic monotonic relations \nlinear. can see plots better understanding.\nFigure 7.5: Linear Monotonic relationship\nSpearman correlation coefficient based ranked values \nvariable rather raw data. spearman correlation\nmeasures monotonic relationship variables, pearsons\ncorrelation coefficient measures linear relationship . use\nSpearman‚Äôs correlation coefficient data must ordinal,\ninterval ratio scale.two cases calculating œÅ. One case tied rank\ntied rank","code":""},{"path":"measures-of-association.html","id":"no-tied-rank-case","chapter":"7 Measures of Association","heading":"7.5.3.1 No tied rank case","text":"two distinct observations \nvalue, thus given rank, said tiedThe formula Spearman rank correlation coefficient \ntied ranks :\\[\\rho = 1 - \\frac{6\\sum_{= 1}^{n}d_{}^{2}}{n\\left( n^{2} - 1 \\right)}\\]\\(d_{}\\) difference ranks ith pair \nobservationExample 7.4: Calculation Spearman‚Äôs rank correlation \ntied rank explained step step using example belowThe scores nine students physics math follows:Physics: 35, 23, 47, 17, 10, 43, 9, 6, 28Mathematics: 30, 33, 45, 23, 8, 49, 12, 4, 31Compute student‚Äôs ranks two subjects compute Spearman\nrank correlation.Step 1: Find ranks individual subject. Rank scores\ngreatest smallest; assign rank 1 highest score, 2 \nnext highest :MathematicsStep 2: Add column d, data. d difference\nranks. example, first student‚Äôs physics rank 3 \nmath rank 5, difference -2. next column, square \nd values.MathematicsStep 4: Sum (add ) d2 values. 4 + 4 + 1 + 0 + 1 +\n1 + 1 + 0 + 0 = 12. ‚Äôll need formula (\n\\(\\sum_{= 1}^{n}d_{}^{2}\\) just ‚Äúsum d2values, n=\n9‚Äù).Step 5: Insert values formula.\\[\\rho = 1 - \\frac{6\\sum_{= 1}^{n}d_{}^{2}}{n\\left( n^{2} - 1 \\right)}\\]\\[\\rho = 1 - \\frac{6 \\times 12}{9\\left( 81 - 1 \\right)} = 0.90\\]Spearman‚Äôs Rank Correlation set data 0.9.Spearman‚Äôs Rank Correlation also lies ‚àí1 +1 always. ‚àí1 ‚â§ œÅ\n‚â§+1","code":""},{"path":"measures-of-association.html","id":"tied-rank-case","chapter":"7 Measures of Association","heading":"7.5.3.2 Tied rank case","text":"Calculation Spearman‚Äôs rank correlation \ntied rank explained step step using example belowExample 7.5: scores nine students physics mathematics follows:PhysicsMathematicsStep 1: Consider marks Physics, ranked usualPhysicsYou can see value 23 repeated, may equal ranks, \naverage two ranks 5 6 given ;\n\\(\\left( \\frac{5 + 6}{2} \\right)\\ \\)= 5.5PhysicsSimilarly marks mathematics can see 33 repeated thrice.MathematicsMathematicsYou can see value 33 repeated thrice, average three\nranks 3, 4 5 given \\(\\left( \\frac{3 + 4 + 5}{3} \\right)\\ \\)= 4MathematicsStep 2: Change formula\\[\\rho = 1 - \\frac{6\\left( \\sum_{= 1}^{n}d_{}^{2} + T_{x} + T_{y} \\right)}{n\\left( n^{2} - 1 \\right)}\\]m individuals tied (rank), s sets\nranks X- series ,\n\\(T_{x} = \\ \\frac{1}{12}\\sum_{= 1}^{s}{m_{}\\left( m_{}^{2} - 1 \\right)}\\)example marks Physics (x) two 23 values tied\ntherefore m = 2; since one set s =1\\(T_{x} = \\ \\frac{1}{12}\\left( 2 \\times (2^{2} - 1 \\right)\\) = 0.5If w individuals tied (rank), s‚Äô sets\nranks Y- series ,\n\\(T_{y} = \\ \\frac{1}{12}\\sum_{= 1}^{s'}{w_{}\\left( w_{}^{2} - 1 \\right)}\\)example marks Mathematics (y) three 33 values tied\ntherefore w = 3; since one set s =1\\(T_{y} = \\ \\frac{1}{12}\\left( 3 \\times (3^{2} - 1 \\right)\\) = 2Step 2: Calculate d use formulaPhysicsMathematics\\(\\rho = 1 - \\frac{6\\left( \\sum_{= 1}^{n}d_{}^{2} + T_{x} + T_{y} \\right)}{n\\left( n^{2} - 1 \\right)} = 1 - \\frac{6 \\times \\left( 44.5 + 0.5 + 2 \\right)}{9\\left( 9^{2} - 1 \\right)} = \\ 1 - \\frac{282}{720}\\)\n= 0.60834","code":""},{"path":"measures-of-association.html","id":"kendalls-rank-correlation-coefficient-œÑ","chapter":"7 Measures of Association","heading":"7.5.4 Kendall‚Äôs Rank Correlation Coefficient (œÑ)","text":"Kendall‚Äôs rank correlation coefficient also known Kendall‚Äôs Tau \ncoefficient concordance. lies 0 1, 0 ‚â§ œÑ ‚â§ 1. \nseveral sets ranks , can used test association.k sets rankings may determine association among\nusing Kendall‚Äôs coefficient Concordance (œÑ). \nmeasure useful study reliability scorings made \nnumber Judges.Arrange data table row representing ranks\nassigned (judge), say, n number objects. Let \nk number sets rankings object given k judges. \nKendall‚Äôs coefficient concordance œÑ computed \\[\\tau = \\frac{12\\left\\lbrack \\sum_{= 1}^{n}{R_{}^{2} - \\frac{\\left( \\sum_{= 1}^{n}R_{} \\right)^{2}}{n}} \\right\\rbrack}{k^{2}n\\left( n^{2} - 1 \\right)}\\]Example 7.6: crop production competition, 10 entries farmers\nranked agricultural scientists (judges). Find degree \nagreement among scientist competition result given .Solution:k = number judges = 4n = number farmers =10\\(\\left( \\sum_{= 1}^{10}R_{} \\right)^{2}\\ \\)= (220)2 = 48400\\(\\sum_{= 1}^{10}R_{}^{2}\\) = 5900\\[\\tau = \\frac{12\\left\\lbrack \\sum_{= 1}^{n}{R_{}^{2} - \\frac{\\left( \\sum_{= 1}^{n}R_{} \\right)^{2}}{n}} \\right\\rbrack}{k^{2}n\\left( n^{2} - 1 \\right)}\\]=\\(\\tau = \\frac{12\\left\\lbrack 5900 - \\frac{48400}{10} \\right\\rbrack}{16 \\times 10\\left( 100 - 1 \\right)}\\)\n= 0.803Since \\(\\tau\\) nearly equal 1, ranks given judges almost\n*************************************************","code":""},{"path":"regression-analysis.html","id":"regression-analysis","chapter":"8 Regression Analysis","heading":"8 Regression Analysis","text":"Regression analysis method using observations (data records)\nquantify relationship target variable also referred \ndependent variable, set independent variables.","code":""},{"path":"regression-analysis.html","id":"definition","chapter":"8 Regression Analysis","heading":"8.1 Definition","text":"Regression analysis mathematical measure \naverage relationship two variables terms \noriginal units data","code":""},{"path":"regression-analysis.html","id":"simple-regression","chapter":"8 Regression Analysis","heading":"8.2 Simple regression","text":"two variables involved; Regression\ncan defined functional relationship two variables,\none may represent cause may represent effect. \nvariable representing cause known independent variable \ndenoted ‚ÄòX‚Äô. variable ‚ÄòX‚Äô also known predictor variable,\nregressor explanatory variable. variable representing effect \nknown dependent variable denoted Y. example consider\nyield fertilizer dose, yield can considered dependent\nvariable (Y) fertilizer dose can considered independent\nvariable (X).","code":""},{"path":"regression-analysis.html","id":"two-types-of-variables","chapter":"8 Regression Analysis","heading":"8.3 Two types of variables","text":"regression analysis \nvariable whose values need predicted (Y) called dependent variable variable used prediction (X) called\nindependent variable.two independent variables present regression\ncalled Multiple Regression. two variables present\ncalled simple regression.","code":""},{"path":"regression-analysis.html","id":"detailed-explanation","chapter":"8 Regression Analysis","heading":"8.4 Detailed explanation","text":"Correlation statistical measure determines degree \nassociation two variables. Regression hand side describes\nindependent variable numerically related dependent\nvariable.Regression can simply defined technique fitting best line \nline best fit estimate value one variable basis \nanother variable. Now best line? line best fit?explaining , consider example Ice cream sales:Example 8.1: local ice cream shop keeps track much ice\ncream sell versus temperature day; \nfigures last 12 days:can use regression analysis answer following questionsWhat Ice cream sales temperature 20o Celsius?functional form relationship Temperature Ice\ncream sales?\nFigure 8.1: Scatter diagram data Example 8.1\ncan draw line denote functional relationship temperature sales\nFigure 8.2: lines drawn show functional relationship\ncan see shown can draw number lines, \nbest fit line?can say best fit line line passes \npoints distance point line minimum. Using\nregression technique easily draw line. proceeding\nknow concept error residuals.","code":""},{"path":"regression-analysis.html","id":"error-and-residual","chapter":"8 Regression Analysis","heading":"8.5 Error and residual","text":"error difference observed value true value\n(true value unobserved population mean population \nsample observations taken). residual difference\nobserved value predicted value (model\n\\[fitted line\\]). Error measured residual can ; \nresidual considered estimate error.\nFigure 8.3: Error depicted best fit line\ndistance observation (ei) fitted line can \nconsidered residual (error). Best fit line can obtained \nminimizing distance. can achieved using mathematical\ntechnique ‚Äúprinciple least squares‚Äù.","code":""},{"path":"regression-analysis.html","id":"straight-lines","chapter":"8 Regression Analysis","heading":"8.6 Straight lines","text":"straight line simplest figure geometry.Mathematical equation straight line Y= + bX.Two important features line slope intercept. \nY-intercept, intercept line y-value point\ncrosses y-axis. b slope line, \nnumber measures \"steepness. change Y \nunit change X along line. regression b called \nregression coefficientIntercept ()\nFigure 8.4: Intercept line\nSlope (b)\nFigure 8.5: Slope line\nb can considered finger print line; \nvalues can easily identify line.now problem simple, find line best, estimate &\nb, error ei observation minimized. \nuse method least squares.","code":""},{"path":"regression-analysis.html","id":"method-of-least-squares","chapter":"8 Regression Analysis","heading":"8.7 Method of least squares","text":"considering error term ei; equation straight line isyi=+bxi+ei;ei ith error term corresponding yi, \n=1,2,...,nLine best fit can obtained estimating b \nminimizing error sum ‚ÄôŒ£ei‚Äô. theorem Œ£ei =0; \nb estimated minimising Œ£ei 2","code":""},{"path":"regression-analysis.html","id":"principle-of-least-squares","chapter":"8 Regression Analysis","heading":"8.8 Principle of least squares","text":"statistical method used determine\nline best fit minimizing ¬†sum squares error term\nŒ£ei 2yi=+bxi+ei;ei = yi ‚Äì (+bxi)ei2 = {yi ‚Äì (+bxi)}2Œ£ ei2 = Œ£ {yi ‚Äì (+bxi)}2Œ£ ei2 called error sum squares. minimizing\nerror sum squares, hence name principle least squares.want minimize, E = Œ£ ei2 = Œ£ {yi ‚Äì\n(+bxi)}2i.e. need find b E minimumE can minimized taking derivative respect \nb equating zero. get two equations,\nequations termed normal equations solving \nnormal equations give formula b.discussing calculation part . taking derivatives \nget two equations (Normal equations) :\\[\\sum_{= 1}^{n}{y_{} = n\\mathbf{} + \\mathbf{b}\\sum_{= 1}^{n}x_{}}\\]\\[\\sum_{= 1}^{n}{y_{}x_{} = \\mathbf{}\\sum_{= 1}^{n}x_{} + \\mathbf{b}\\sum_{= 1}^{n}x_{}^{2}}\\]solving equations getRegression coefficient, b =\n\\(\\frac{\\sum_{= 1}^{n}{y_{}x_{} - \\frac{\\sum_{= 1}^{n}{y_{}\\sum_{= 1}^{n}x_{}}}{n}}}{\\sum_{= 1}^{n}x_{}^{2} - \\frac{\\left( \\sum_{= 1}^{n}x_{} \\right)^{2}}{n}}\\)=\\(\\frac{cov(x,y)}{var(x)}\\)\\[\\mathbf{b =}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(x)}}\n\\]\\[\\mathbf{=}\\overline{\\mathbf{y}}\\mathbf{- b}\\overline{\\mathbf{x}}\\]\\(\\overline{y}\\) = mean y; \\(\\overline{x}\\) = mean x","code":""},{"path":"regression-analysis.html","id":"two-lines-of-regression","chapter":"8 Regression Analysis","heading":"8.9 Two lines of regression","text":"two lines regression- y x x y.Regression y xConsider two variables x y, considering y \ndependent variable x independent variable equation\n:y = + bxThis used predict unknown value variable y value \nvariable x known. Usually b denoted byx\\[\\mathbf{b}_{\\mathbf{\\text{yx}}}\\mathbf{=}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(x)}}\\]Consider Example 8.1; considering ice cream sales dependent variable\ntemperature independent variable\nFigure 8.6: Scatter diagram data Example 8.1\nRegression x yConsider two variables x y, considering x \ndependent variable y independent variable equation\n:x= c + ; c intercept m \nslopeThis used predict unknown value variable x value \nvariable y known. Usually b denoted bxy\\[\\mathbf{b}_{\\mathbf{\\text{xy}}}\\mathbf{=}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(y)}}\\]Consider Example 8.1; considering temperature dependent variable \nice cream sales independent variable\nFigure 8.7: Scatter diagram data Example 8.1\ncan see regression different. depends \nexperimenter choose dependent independent variable. \nexample evident considering temperature dependent variable\nmeaningless, .e. usefulness predicting\ntemperature based ice cream sales?. selection dependent \nindependent variable entirely discretion experimenter based \nobjective study.","code":""},{"path":"regression-analysis.html","id":"assumptions-of-regression","chapter":"8 Regression Analysis","heading":"8.10 Assumptions of Regression","text":"y dependent variable x independent variable\nthenThe x‚Äôs non-random fixed constantsThe x‚Äôs non-random fixed constantsAt fixed value x corresponding values y \nnormal distribution mean.fixed value x corresponding values y \nnormal distribution mean.given x, variance y .given x, variance y .values y observed different levels x completely\nindependent.values y observed different levels x completely\nindependent.","code":""},{"path":"regression-analysis.html","id":"properties-of-regression-coefficients","chapter":"8 Regression Analysis","heading":"8.11 Properties of Regression coefficients","text":"correlation coefficient x y geometric\nmean two regression coefficients byx bxy\\[r = \\sqrt{b_{\\text{yx}}b_{\\text{xy}}}\\]Regression coefficients independent change origin \nscale.Regression coefficients independent change origin \nscale.one regression coefficient greater unit, \nmust less unit vice versa. .e. \nregression coefficients can less unity \ngreater unity, .e. byx >1 bxy <1\nbxy >1, byx <1.one regression coefficient greater unit, \nmust less unit vice versa. .e. \nregression coefficients can less unity \ngreater unity, .e. byx >1 bxy <1\nbxy >1, byx <1.Also one regression coefficient positive must \npositive (case correlation coefficient positive) \none regression coefficient negative must negative\n(case correlation coefficient negative).Also one regression coefficient positive must \npositive (case correlation coefficient positive) \none regression coefficient negative must negative\n(case correlation coefficient negative).","code":""},{"path":"regression-analysis.html","id":"uses-of-regression","chapter":"8 Regression Analysis","heading":"8.12 Uses of Regression","text":"Prediction: regression analysis useful predicting \nvalue one variable given value another variable. \npredictions useful difficult expensive measure\ndependent variable, Y.Identify strength relationship: regression might used\nidentify strength effect independent variable(s)\ndependent variable. Like strength relationship \ndose effect, sales marketing spending, age income.Forecast effects impact changes: , regression\nanalysis helps us understand much dependent variable changes\nchange one independent variables. typical question\n, ‚Äúmuch additional sales income get additional 1000\nspent marketingPredicts trends future values: regression analysis can \nused predict trend future values, like ‚Äúprice \ngold 6 months?‚Äù","code":""},{"path":"regression-analysis.html","id":"example-problem","chapter":"8 Regression Analysis","heading":"8.13 Example problem","text":"Now consider example 8.1 answer questionsWhat functional form relationship Temperature \nIce cream sales?functional form relationship Temperature \nIce cream sales?Ice cream sales temperature 20o Celsius?Ice cream sales temperature 20o Celsius?SolutionFit model considering Ice cream sales dependent variable (y)\ntemperature independent variable (x). Fitting model means\nestimating b using equation.Fit model considering Ice cream sales dependent variable (y)\ntemperature independent variable (x). Fitting model means\nestimating b using equation.fitting model put 20 x value get \npredicted y valueAfter fitting model put 20 x value get \npredicted y valueModel: y = +bxTemperatureSalesn =12\\[mean,\\overline{x} = \\ \\frac{224.1}{12} = 18.675\\]\\[mean,\\overline{y} = \\ \\frac{4829}{12} = 402.416\\]Cov (x,y) =\n\\(\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}\\)\\(\\sum_{= 1}^{12}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)} = 5325.03\\)Cov (x,y) = \\(\\frac{5325.03}{12} = 443.752\\)\\[variance\\ \\ x,\\ var\\left( x \\right) = \\ \\frac{1}{n}\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2} = \\frac{176.983}{12} = 14.7485\\]\\[\\mathbf{b =}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(x)}}\\]\\[\\mathbf{b =}\\frac{443.752}{14.7485}\\mathbf{=}30.088\\]\\[\\mathbf{=}\\overline{\\mathbf{y}}\\mathbf{- b}\\overline{\\mathbf{x}}\\]\\[\\mathbf{=}402.416 - 30.088\\left( 18.675 \\right) = \\  - 159.477\\]model \\[y = \\  - 159.477 + 30.088x\\]\\[Ice\\ cream\\ sales = \\  - 159.477 + 30.088(Temperature)\\]Ice cream sales temperature 20o Celsius\\[x = 20\\]\\(y = \\  - 159.477 + 30.088(20)\\) = 442.283So predicted ice cream sales 20o Celsius $442.283**************************************************************","code":""},{"path":"probability.html","id":"probability","chapter":"9 Probability","heading":"9 Probability","text":"Many events can‚Äôt predicted total certainty. best can say likely happen, using idea probability.","code":""},{"path":"probability.html","id":"tossing-a-coin","chapter":"9 Probability","heading":"9.1 Tossing a Coin","text":"coin tossed, two possible outcomes:heads (H) orheads (H) ortails (T)tails (T)say probability coin landing¬†H¬†¬†¬Ω probability coin landing¬†T¬†¬†¬Ω","code":""},{"path":"probability.html","id":"throwing-dice","chapter":"9 Probability","heading":"9.2 Throwing Dice","text":"single¬†die¬†thrown, six possible\noutcomes: 1, 2, 3, 4, 5, 6. probability getting one ¬†\\(\\frac{1}{6}\\)\nFigure 9.1: Image dice (plural)\n","code":""},{"path":"probability.html","id":"playing-cards","chapter":"9 Probability","heading":"9.3 Playing cards","text":"standard deck playing cards consists 52 cards divided 4 suits (Spades, Hearts, Diamonds, Clubs) 13 cards \nFigure 9.2: Image spade symbol\nSpades : - 13 cards, include 9 numbered cards 2 10 picture cards Ace, King, Queen, Jack\nFigure 9.3: Image hearts symbol\nHearts : - 13 cards\nFigure 9.4: Image diamond symbol\nDiamonds :- 13 cards\nFigure 9.5: Image clubs symbol\nClubs :- 13 cardsRed cards:- 26 cards Black cards:- 26 cards","code":""},{"path":"probability.html","id":"probability-1","chapter":"9 Probability","heading":"9.4 Probability","text":"general:Probability event happening =\n\\(\\frac{\\text{Number ways can happen}}{\\text{Total  number outcomes}}\\)","code":""},{"path":"probability.html","id":"example-1","chapter":"9 Probability","heading":"Example 1","text":"chances rolling \"4\" dieNumber ways can happen: 1¬†(1 face \"4\" )Total number outcomes: 6¬†(6 faces altogether)probability =¬†\\(\\frac{1}{6}\\)","code":""},{"path":"probability.html","id":"example-2","chapter":"9 Probability","heading":"Example 2","text":"5 marbles bag: 4 blue, 1 red. probability blue marble gets picked?Number ways can happen: 4¬†(4 blues)Total number outcomes: 5¬†(5 marbles total)probability =¬†\\(\\frac{4}{5}\\) = 0.8","code":""},{"path":"probability.html","id":"must-learn-definitions","chapter":"9 Probability","heading":"Must learn definitions","text":"Random experiment: random experiment experiment process outcome predicted certainty.Eg: Tossing coin; Tossing coin five times; Choosing card deck\ncards; Tossing die.Sample space: sample space (denoted S) random experiment set possible outcomes.Eg: Throwing coin generates sample space; S={H,T},Throwing die: S= {1,2,3,4,5,6}Sample point: Just one possible outcomes. Eg: heads, 5 Clubs cards. 6 different sample points sample space throwing \ndie.Exercise1: Check yourselfIf die tossedWhat sample space?probability getting 1?probability obtaining even number?probability getting 7?\nFigure 9.6: Sample space throwing die\n","code":""},{"path":"probability.html","id":"event","chapter":"9 Probability","heading":"9.4.1 Event","text":"One outcomes random experimentAn event can just one outcome:Getting Tail tossing coinGetting Tail tossing coinRolling \"5\"Rolling \"5\"event can include one outcome:Choosing \"King\" deck cards (4 Kings)Choosing \"King\" deck cards (4 Kings)Rolling \"even number\" (2, 4 6)Rolling \"even number\" (2, 4 6)","code":""},{"path":"probability.html","id":"example","chapter":"9 Probability","heading":"Example","text":"Ram wants see many times \"double\" (dice number) comes throwing 2 dice.Sample Space possible Outcomes (36 Sample Points):{1,1} {1,2} {1,3} {1,4} ... {6,3} {6,4} {6,5} {6,6}Event Ram looking \"double\", dice number. made 6 Sample Points:{1,1} {2,2} {3,3} {4,4} {5,5} {6,6}","code":""},{"path":"probability.html","id":"types-of-events","chapter":"9 Probability","heading":"9.4.1.1 Types of Events","text":"Independent EventsEvents can \"Independent\", meaning event affected events.Eg: toss coin comes \"Heads\" three times. chance next toss also \"Head\"?chance simply¬†¬Ω¬†(0.5) just like toss coin. Past affect current toss.Dependent EventsBut events can \"dependent\", means can affected previous events. taking one card deck less\ncards available, probabilities change!look chances getting King deck cardsFor 1st card chance drawing King 4 52But 2nd card:1st card King, 2nd card less likely \nKing, 3 51 cards left Kings.1st card King, 2nd card slightly \nlikely King, 4 51 cards left King.NoteReplacement: put card back drawing chances change, events independent.Without Replacement: chances change, events dependent.Mutually Exclusive EventsMutually Exclusive means get events time. \neither one , bothExamples:Turning left right Mutually Exclusive (\ntime)Turning left right Mutually Exclusive (\ntime)Heads Tails Mutually ExclusiveHeads Tails Mutually ExclusiveKings Aces Mutually ExclusiveKings Aces Mutually ExclusiveWhat Mutually ExclusiveKings Hearts Mutually Exclusive, can King Hearts!Exhaustive EventsA set events called exhaustive events together make \nentire sample space.exampleSample space tossing die \\(S\\)= {1, 2, 3, 4, 5, 6}theEvent : getting even number; {2, 4, 6}Event B: getting odd number; {1, 3, 5}exhaustive events together makes entire sample space.Equally likely eventsEqually likely events events theoretical\nprobability occurring.example: Tossing coinEvent : getting headEvent B: getting tailBoth events equal probability occurring; events termed equally likely events","code":""},{"path":"probability.html","id":"definition-of-probability","chapter":"9 Probability","heading":"9.5 Definition of Probability","text":"","code":""},{"path":"probability.html","id":"mathematical-definition","chapter":"9 Probability","heading":"9.5.1 Mathematical Definition","text":"experiment \\(n\\) exhaustive, mutually exclusive equally likely outcomes, \\(m\\) outcomes favourable happening event \\(E\\), probability \\(p\\) happening E given \\[P\\left( E \\right) = p = \\ \\frac{m}{n}\\]\\(p\\) termed probability success.","code":""},{"path":"probability.html","id":"example-3","chapter":"9 Probability","heading":"Example","text":"coin tossed, two possible outcomes Heads Tails. Outcomes exhaustive, mutually exclusive equally likely.Consider event \\(E\\) : getting headProbability event \\(E\\), \\(p(E)\\) denoted \\(p\\)\\(p\\) given definition :\\(m\\) = number outcomes favourable happening event \\(E\\) = 1\\(n\\) = number outcomes (Head Tail) = 2\\[P\\left( E \\right) = p = \\ \\frac{m}{n}\\]\\[P\\left( E \\right) = p = \\ \\frac{1}{2}\\].e. probability getting Head ¬Ωdefinition following limitationsWhat happen outcomes equally likely. example\ntossing biased die.happen outcomes equally likely. example\ntossing biased die.Probability defined total cases ‚Äòn‚Äô unknown \ntends infinity. example; probability raining\ntomorrow?Probability defined total cases ‚Äòn‚Äô unknown \ntends infinity. example; probability raining\ntomorrow?overcome limitations, definitions given","code":""},{"path":"probability.html","id":"statistical-definition","chapter":"9 Probability","heading":"9.5.2 Statistical Definition","text":"probability 'p' happening E given \\[P\\left( E \\right) = \\lim_{n \\rightarrow \\infty}\\frac{m}{n}\\]\\(n\\) number times process (e.g., tossing die) \nperformed tends infinity, \\(m\\) number times \noutcome ‚Äò\\(E\\)‚Äô happens.definition also limitationsIn cases, experiment never practice carried \n.cases, experiment never practice carried \n.leaves open question large \\(n\\) get\ngood approximation.leaves open question large \\(n\\) get\ngood approximation.overcome limitations, another approach probability \nintroduced Russian mathematician .N. Kolmogorov. approach \nprecise definition given, instead certain axioms postulates \nprobability calculations based.","code":""},{"path":"probability.html","id":"axiomatic-approach","chapter":"9 Probability","heading":"9.5.3 Axiomatic Approach","text":"Whole field probability theory based following three axiomsProbability event, P (\\(E\\)) lies 0 1.\n\\(0 \\leq P(E) \\leq 1\\)Probability event, P (\\(E\\)) lies 0 1.\n\\(0 \\leq P(E) \\leq 1\\)Probability entire sample space 1. \n\\(P\\left( S \\right) = 1\\)Probability entire sample space 1. \n\\(P\\left( S \\right) = 1\\)B mutually exclusive events probability occurrence either B denoted \\(P(\\cup B)\\) shall given \\(P\\left( \\cup B \\right) = P\\left( \\right) + P(B)\\)B mutually exclusive events probability occurrence either B denoted \\(P(\\cup B)\\) shall given \\(P\\left( \\cup B \\right) = P\\left( \\right) + P(B)\\)","code":""},{"path":"probability.html","id":"some-interesting-facts-on-probability","chapter":"9 Probability","heading":"Some interesting facts on probability","text":"Probability \\(p\\) happening event known probability success probability \\(q\\)‚Äô \nnon-happening event probability failure.Probability \\(p\\) happening event known probability success probability \\(q\\)‚Äô \nnon-happening event probability failure.\\(p\\) well \\(q\\) non-negative exceed unity. .e., 0 ‚â§ \\(p\\) ‚â§ 1 0 ‚â§ \\(q\\) ‚â§ 1. Thus, probability occurrence event lies 0 1\\[including 0 1\\]\\(p\\) well \\(q\\) non-negative exceed unity. .e., 0 ‚â§ \\(p\\) ‚â§ 1 0 ‚â§ \\(q\\) ‚â§ 1. Thus, probability occurrence event lies 0 1\\[including 0 1\\]Probability impossible event 0 sure event 1. \\(p()\\) = 1, event certainly going happen \\(p()\\) = 0, event certainly going happen.Probability impossible event 0 sure event 1. \\(p()\\) = 1, event certainly going happen \\(p()\\) = 0, event certainly going happen.number (\\(m\\)) favorable outcomes event greater total number outcomes (\\(n\\)).number (\\(m\\)) favorable outcomes event greater total number outcomes (\\(n\\)).\nFigure 9.7: Sample space throwing die\n","code":""},{"path":"probability.html","id":"additional-problems","chapter":"9 Probability","heading":"Additional Problems","text":"simultaneous toss two coins, find probability \\(\\) getting 2 heads. (ii) exactly 1 head ?SolutionHere, possible outcomes HH, HT, TH, TT. .e., Total number possible outcomes = 4.\\(\\) Number outcomes favorable event (2 heads) = 1 (.e.,HH). .e.\\(p\\)(2 heads) = 1/4 .\\(ii\\) Now event consisting exactly one head two favourable cases, namely HT TH\\(p\\)(exactly one head) = 2 /4 = 1/ 2In single throw two dice, probability sum 9?SolutionThe number possible outcomes 6 √ó 6 = 36.1,1 1,2 1,3 1,4 1,5 1.62,1 2,2 2,3 2,4 2,5 2,63,1 3,2 3,3 3,4 3,5 3,64,1 4,2 4,3 4,4 4,5 4,65,1 5,2 5,3 5,4 5,5 5,66,1 6,2 6,3 6,4 6,5 6,6Event : sum 9four outcomes sum 9, (5,4), (6,3), (3,6), (4,5)Probability (sum 9) = 4/36= 1/9From bag containing 10 red, 4 blue 6 black balls, ball drawn random. probability drawing () red ball? (ii) blue ball? (iii) black ball?20 balls . , total number possible outcomes 20\\(\\) Number red balls = 10,\\(p\\)(getting red ball ) = 10/20 = ¬Ω\\(ii\\) Number blue balls = 4\\(p\\)(getting blue ball ) = 4/20 = 1/5\\(iii\\) Number balls black = 14\\(p\\)(black ball ) = 14/20 = 7/10","code":""},{"path":"probability.html","id":"event-relations","chapter":"9 Probability","heading":"9.6 Event relations","text":"Consider tossing die. event getting even number, sample points 2, 4 6 favorable event . remaining sample points 1, 3 5 favorable event . Therefore, occur event occur. experiment, outcomes favorable event \ncalled complement denoted '' Ac","code":""},{"path":"probability.html","id":"event-a-or-b","chapter":"9 Probability","heading":"9.6.1 Event A or B","text":"Denoted (\\(\\cup\\) B), spelled union BLet us consider example throwing die. event getting\nmultiple 2 B another event getting multiple 3. \noutcomes 2, 4 6 favourable event outcomes 3 \n6 favourable event B..e.= {2, 4, 6}B= {3, 6}Íì¥ B = { 2, 3, 4,6}, event getting even number B another\nevent getting odd number, thenA = { 2, 4, 6 }B = { 1, 3, 5 }Íì¥ B = {1, 2, 3, 4,5,6}","code":""},{"path":"probability.html","id":"event-a-and-b","chapter":"9 Probability","heading":"9.6.2 Event A and B","text":"Denoted (Íìµ B) spelled intersection B. throwing die event getting multiple 2 B event getting multiple 3. outcomes favorable 2, 4, 6 outcomes favorable B 3, 6. 6 present B Íìµ B = 6\nFigure 9.8: Venn diagram showing intersection\n","code":""},{"path":"probability.html","id":"additive-law-of-probability","chapter":"9 Probability","heading":"9.7 Additive law of Probability","text":"two events B sample space S,\\(p\\)(AÍì¥B) = \\(p\\)()+\\(p\\)(B) ‚àí \\(p\\)(AÍìµB)mutually exclusive case \\(p\\)P(AÍìµB)=0; case \\(p\\)(AÍì¥B )= \\(p\\)()+\\(p\\)(B ).","code":""},{"path":"probability.html","id":"example-4","chapter":"9 Probability","heading":"Example","text":"card drawn well-shuffled deck 52 cards. probability either spade king?denotes event drawing 'spade card'. B denotes events drawing 'king' respectively. event consists 13 sample points, whereas event B consists 4 sample points.\\(p\\)()= 13/52\\(p\\)(B)= 4/52\\(p\\)( AÍìµB) = 1/52\\(p\\)(AÍì¥B)= \\(p\\)()+\\(p\\)(B) ‚àí \\(p\\)(AÍìµB) = 13/52+4/52-1/52 = 4/13In single throw two dice, find probability total 9 11.Let events = total 9 B= total 11.Events mutually exclusive Íìµ B = 0Now \\(p\\)() = \\(p\\) \\[(3, 6), (4, 5), (5, 4), (6, 3)\\] = 4 /36\\(p\\)(B) = \\(p\\)\\[ (5, 6), (6, 5)\\] = 2 /36Thus, \\(p\\)( AÍì¥B) = 4 /36 +2/36 =6/36=1/6","code":""},{"path":"probability.html","id":"multiplication-law-of-probability","chapter":"9 Probability","heading":"9.8 Multiplication law of probability","text":"B independent events, \\(p\\)(AÍìµB) = \\(p\\)(). \\(p\\)(B)called Multiplication law probabilityA die tossed twice. Find probability number greater 4 throw.Let us denote , event 'number greater 4' first throw. B event 'number greater 4' second throw. Clearly B independent events. first throw, two outcomes, namely, 5 6 favourable event .‚à¥ \\(p\\)() =2/6 = 1/3Similarly second throw, two outcomes, namely, 5 6 favourable event .‚à¥ \\(p\\)(B) = 2/6 = 1/3Hence, \\(p\\)(B) = \\(p\\)(AÍìµB) = \\(p\\)().\\(p\\)(B) = 1 /3 √ó1 /3 = 1/9","code":""},{"path":"probability.html","id":"probability-using-combinations","chapter":"9 Probability","heading":"Probability using combinations","text":"Knowledge combinations can applied find total number possible outcomes.nCr\\(= \\frac{n!}{r!(n - 1)!}\\)example\n3C2\\(= \\frac{3!}{2!(3 - 2)!} = \\frac{3 \\times 2 \\times 1}{2 \\times 1(1)} = 3\\)Now let us see example used probabilityA bag contains 3 red, 6 white 7 blue balls. probability two balls drawn white blue?Total number balls = 3 + 6 + 7 = 16Out 16 balls, 2 balls can drawn 16C2 waysi.e. 16C2 = 120.6 white balls, 1 ball can drawn 6C1 ways 7 blue balls, one can drawn 7C1 ways. Since former case associated later case, therefore total number favorable cases 6C1 √ó 7C1Therefore required probability= (6C1 √ó 7C1) / 16C2 = 42/120 = 7/20Find probability getting red balls, bag containing 5 red 4 black balls, two balls drawnTotal number balls = 9Out 9 balls, 2 balls can drawn 9C2 waysNo ways red balls can taken = 5C2Hence \\(p\\)(red balls ) = 5C2 / 9C2 = 5/18","code":""},{"path":"probability.html","id":"answer","chapter":"9 Probability","heading":"9.9 Answers of exercise 1","text":"{1, 2, 3, 4, 5, 6}{1, 2, 3, 4, 5, 6}1/61/63/6 = 1/23/6 = 1/200********************************************************","code":""},{"path":"probability-distributions.html","id":"probability-distributions","chapter":"10 Probability Distributions","heading":"10 Probability Distributions","text":"","code":""},{"path":"probability-distributions.html","id":"random-experiment","chapter":"10 Probability Distributions","heading":"10.1 Random experiment","text":"random experiment experiment \nprocess outcome predicted certainty. \nsample space (denoted S) random experiment set \npossible outcomes.Example: Tossing coin, Throwing die etc","code":""},{"path":"probability-distributions.html","id":"random-variable","chapter":"10 Probability Distributions","heading":"10.2 Random variable","text":"Random Variable set possible\nvalues random experiment. Random variable usually denoted ‚Äò\\(X\\)‚Äô\nFigure 10.1: Tossing coin\nRandom Variable whole set values take values, randomly. like Algebra Variable, Algebra variable, like X, unknown value. Random Variable different.Probability random variable can represented \\(p(X = x)\\) \\(p(x)\\).Small letter \\(x\\) denotes value taken random variable \\(X\\).example throwing die onceHere can define random variable interest. Defining random variable like:Let \\(X\\) number appearing throwing dice ; \\(x\\) = {1, 2, 3, 4, 5, 6}. random variable \\(X\\) can take values shown . case equally likely, probability anyone 1/6. , \\(p(X = x)\\) = \\(p(x)\\) =1/6Let \\(X\\) number appearing throwing dice ; \\(x\\) = {1, 2, 3, 4, 5, 6}. random variable \\(X\\) can take values shown . case equally likely, probability anyone 1/6. , \\(p(X = x)\\) = \\(p(x)\\) =1/6Let \\(X\\) even number appearing throwing dice ; \\(X\\) = {2, 4, 6}. random variable \\(X\\) can take values shown . \\(p(X = x)\\) = \\(p(x)\\) = 3/6Let \\(X\\) even number appearing throwing dice ; \\(X\\) = {2, 4, 6}. random variable \\(X\\) can take values shown . \\(p(X = x)\\) = \\(p(x)\\) = 3/6","code":""},{"path":"probability-distributions.html","id":"probability-distributions-1","chapter":"10 Probability Distributions","heading":"10.3 Probability distributions","text":"probability distribution list possible outcomes random variable along corresponding probability values.Example: - Tossing coin 3 timesDefining random variable:Let X number heads appearing throwing dice\ncase 0 heads, 1, 2 3 heads, sample space \\(X\\) = {0, 1, 2, 3} equally likely.total 8 possible cases tossing coin three times shown table \\(p(X = x)\\) = \\(\\frac{: \\ \\ times\\ X\\ takes\\ value\\ x}{8}\\)\\(p(X = 3)\\) = 1/8; \\(p(X = 2)\\) = 3/8; \\(p(X = 1)\\) = 3/8; \\(p(X = 0)\\) = 1/8A¬†probability distribution list possible outcomes random variable along corresponding probability values.Table shows probability distribution.example discrete probability distribution \\(X\\) takes discrete values. \\(x\\) takes continuous values, termed continuous probability distribution.","code":""},{"path":"probability-distributions.html","id":"probability-mass-function","chapter":"10 Probability Distributions","heading":"10.4 Probability mass function","text":"use probability function describe discrete probability distribution, call probability mass function (commonly abbreviated p.m.f). probability mass function, returns probability outcome. Therefore, probability mass function written : \\(p(x)\\) = \\(p(X = x)\\). variable \\(X\\) discrete nature.","code":""},{"path":"probability-distributions.html","id":"probability-density-function","chapter":"10 Probability Distributions","heading":"10.5 Probability density function","text":"use probability function describe continuous probability distribution, call probability density function (commonly abbreviated p.d.f).","code":""},{"path":"probability-distributions.html","id":"expected-value-of-a-random-variable","chapter":"10 Probability Distributions","heading":"10.6 Expected value of a random variable","text":"Expected value exactly might think means. return can expect kind action. ¬†expected value¬†¬†random variable long-run average value repetitions ¬†experiment¬†represents.¬†example, expected value rolling six-sided¬†die 3.5, average numbers come\n3.5 number rolls approaches infinity. Expected value random variable \\(X\\) denoted \\(E(X)\\).\nformula calculating Expected Value random variable ¬†multiple probabilities isE(\\(X\\)) = \\(\\sum_{= 1}^{\\infty}xp(x)\\) (Discrete Case)E(\\(X\\)) = \\(\\sum_{= 1}^{\\infty}xp(x)\\) (Discrete Case)E(\\(X\\)) = \\(\\int_{- \\infty}^{\\infty}{x p(x)\\ \\text{dx }}\\)\n(continuous case); \\(- \\infty \\leq x \\leq \\infty\\)E(\\(X\\)) = \\(\\int_{- \\infty}^{\\infty}{x p(x)\\ \\text{dx }}\\)\n(continuous case); \\(- \\infty \\leq x \\leq \\infty\\)random variable \\(X\\) lies \\({-\\infty}\\) \\({+\\infty}\\)","code":""},{"path":"probability-distributions.html","id":"example-5","chapter":"10 Probability Distributions","heading":"Example","text":"Find Expected value \\(X\\) tossing single unfair dieSolution:E(\\(X\\)) = \\(\\sum_{= 1}^{6}xp(x)\\)\n= 0.1+0.2+0.3+0.4+0.5+3 = 4.5Find E(X) following case ()","code":""},{"path":"probability-distributions.html","id":"discrete-probability-distributions","chapter":"10 Probability Distributions","heading":"10.7 Discrete probability distributions","text":"seen ¬†probability distribution¬†list possible outcomes random variable along corresponding probability values. represented table. convenient can expressed equation. value x given can calculate corresponding probability equation. several discrete distributions depending situations. discussion limited following discrete distributions.Bernoulli distributionBernoulli distributionBinomial distributionBinomial distributionPoisson distributionPoisson distribution","code":""},{"path":"probability-distributions.html","id":"bernoulli-distribution","chapter":"10 Probability Distributions","heading":"10.7.1 Bernoulli distribution","text":"Bernoulli distribution ¬†discrete probability distribution. distribution applies random experiment two outcomes (usually called ‚ÄúSuccess‚Äù ‚ÄúFailure‚Äù).example tossing coin two outcomes can termed success failure experimenterSuccess: getting headFailure: getting tailThe probability mass function distribution \\(p(X = x)\\) = \\(p(x)\\) = \\(p\\)\\(x\\)\\((1-p)\\)\\(1-x\\), \\(X\\) takes two values = 0,1The¬†expected value¬†random variable, \\(X\\), Bernoulli distribution : E(\\(X\\)) = \\(p\\) ¬†variance¬†Bernoulli random variable : var(\\(X\\)) = \\(p(1 - p)\\).","code":""},{"path":"probability-distributions.html","id":"example-6","chapter":"10 Probability Distributions","heading":"Example","text":"Find probability assuming Bernoulli distribution \nbiased coin probability success (getting head) \\(p\\) = 0.4Let \\(X\\) random variable takes value 0 getting tail\n(failure) takes value 1 getting head (success). using \nequationp(\\(X\\) = \\(x\\)) = p(\\(X\\)) = p\\(X\\)\\((1-p)\\)\\(1-x\\)P(\\(X\\) = 0) = (0.4)0(1-0.4)1 =0.6P(\\(X\\) = 1) = (0.4)1(1-0.4)0 =0.4A¬†Bernoulli trial¬†one simplest experiments can conduct\nprobability statistics. ‚Äôs experiment can one\ntwo possible outcomes. example, ‚ÄúYes‚Äù ‚Äú‚Äù ‚ÄúHeads‚Äù \n‚ÄúTails.‚Äù","code":""},{"path":"probability-distributions.html","id":"binomial-distribution","chapter":"10 Probability Distributions","heading":"10.7.2 Binomial distribution","text":"Binomial distribution can thought simply probability \nSUCCESS FAILURE outcome experiment survey repeated\nmultiple times; .e. Binomial distribution happens, Bernoulli\ntrial repeated \\(n\\) number times. binomial type \ndistribution two possible outcomes (prefix ‚Äúbi‚Äù means two,\ntwice). example coin toss repeated 5 times, random\nvariable, \\(X\\) = : heads follow binomial distribution \\(n\\)\n= 5.Binomial distributions must also meet following three criteria:number observations trials fixed. (\\(n\\))number observations trials fixed. (\\(n\\))observation trial ¬†independentEach observation trial ¬†independentThe¬†probability success¬†(tails, heads, fail pass) ¬†exactly ¬†one trial another (\\(p\\))¬†probability success¬†(tails, heads, fail pass) ¬†exactly ¬†one trial another (\\(p\\))probability mass function distribution \\(p(x)\\) = \\(n\\)\\(c\\)\\(x\\)\\(p\\)\\(x\\)\\(q\\)\\(n-x\\) = \\(\\frac{n!}{\\left( n - x \\right)!x!\\ }\\) p\\(x\\)\\(q\\)\\(n-x\\)x takes values = 0,1,2,..., \\(n\\)\\(n\\)= number trials\\(x\\)= number success desired\\(p\\)= probability getting success one trial\\(q\\) = \\(1-p\\) = probability getting failure one trialE(\\(X\\)) = \\(np\\)V(\\(X\\)) = \\(npq\\); \\(n\\) \\(p\\) important parameters binomial\ndistribution.Mean binomial distribution \\(np\\) variance \\(npq\\)","code":""},{"path":"probability-distributions.html","id":"example-7","chapter":"10 Probability Distributions","heading":"Example","text":"coin tossed 10 times. probability \ngetting exactly 6 heads?,\\(n\\) = 10\\(x\\) = 6\\(p\\) = ¬Ω\\(q\\) = \\(1-p\\) = ¬Ωfind \\(p(X=6)\\); using formula \\(p(X=x)\\) \\(=\\) \\(n\\)\\(c\\)\\(x\\)\\(p\\)\\(x\\)\\(q\\)\\(n-x\\)\\(p(X=6)\\) = 10\\(c\\)6\\(\\left( \\frac{1}{2} \\right)^{6}\\left( \\frac{1}{2} \\right)^{10 - 6}\\)=\n0.2050","code":""},{"path":"probability-distributions.html","id":"poisson-distribution","chapter":"10 Probability Distributions","heading":"10.7.3 Poisson distribution","text":"Discovered French Mathematician Simeon Denis Poisson (1781\n-1840). developed describe number times gambler \nwin rarely won game chance large number tries, .e.¬†Poisson\ndistribution deals rare events.Poisson distribution first applied study number death \nhorse kicking Prussian army.applications examples Poisson distribution usedPest incidencePest incidenceBirth defects genetic mutationsBirth defects genetic mutationsRare diseasesRare diseasesCar accidentsCar accidentsTraffic flow ideal gap distanceTraffic flow ideal gap distanceNumber typing errors pageNumber typing errors pageHairs found McDonald's hamburgersHairs found McDonald's hamburgersSpread endangered animal AfricaSpread endangered animal AfricaFailure machine one monthFailure machine one monthA random variable \\(X\\) said follow Poisson distribution; \nassumes non-negative values probability mass function given\n:\\[p\\left( X = x \\right) = \\frac{e^{- \\lambda}.\\lambda^{x}}{x!}\\]\\(x\\) = 0,1,2,‚Ä¶ ., ‚àû,\\(e\\) = 2.7183.\\(Œª\\) : Average number successes occurring given time interval \nregion Poisson distributionIt discrete distribution single parameter ŒªThe mean variance Poisson distribution \nequal ¬†\\(Œª\\)","code":""},{"path":"probability-distributions.html","id":"example-8","chapter":"10 Probability Distributions","heading":"Example","text":"average number homes sold Realty company 2\nhomes per day. Assuming Poisson distribution probability\nexactly 3 homes sold tomorrow?Solution:\\(Œª\\) = 2; since 2 homes sold per day, average.\\(x\\) = 3; since want find probability 3 homes sold\ntomorrow.\\(e\\) = 2.71828; since e constant equal approximately 2.71828.\\[p\\left( X = x \\right) = \\frac{e^{- \\lambda}.\\lambda^{x}}{x!}\\]\\(p(X=3)\\) = \\(\\ \\frac{{2.71828}^{- 2}{\\  \\times \\ 2}^{3}}{3!}\\)\\(p(X = 3)\\) = 0.180","code":""},{"path":"probability-distributions.html","id":"questions","chapter":"10 Probability Distributions","heading":"Questions","text":"random variable X follows Poisson distribution mean 3.4, find \\(p(X = 6)\\)random variable X follows Poisson distribution mean 3.4, find \\(p(X = 6)\\)number industrial injuries per working week particular factory known follow Poisson distribution mean 0.5. Find probability particular week :number industrial injuries per working week particular factory known follow Poisson distribution mean 0.5. Find probability particular week :\\(\\) Less 2 accidents \\[Hint: p(X\\<2) = p(X = 0) + p(X = 1)\\]\\(ii\\) 2 accidents \\[Hint: p(X \\>2) = 1-{p(X = 0) + p(X = 1)\n+ p(X =2)}\\]company known past experience 3% bulbs produced defective. Assuming Poisson distribution find probability getting following sample 100 bulbs:\n\\[Hint:  \\ Œª = n √ó p = 100 √ó 0.03\\]\ndefective \\[Hint: let X number defectives; x = 0\\]\n1 defective \\[Hint: x = 1\\]\n2 Defectives \\[Hint: x = 2\\]\n3 defectives\\[Hint: x = 3\\]\ncompany known past experience 3% bulbs produced defective. Assuming Poisson distribution find probability getting following sample 100 bulbs:\n\\[Hint:  \\ Œª = n √ó p = 100 √ó 0.03\\]defective \\[Hint: let X number defectives; x = 0\\]defective \\[Hint: let X number defectives; x = 0\\]1 defective \\[Hint: x = 1\\]1 defective \\[Hint: x = 1\\]2 Defectives \\[Hint: x = 2\\]2 Defectives \\[Hint: x = 2\\]3 defectives\\[Hint: x = 3\\]3 defectives\\[Hint: x = 3\\]Distributions Bernoulli, Binomial Poisson discussed far discrete distributions.","code":""},{"path":"probability-distributions.html","id":"continuous-probability-distributions","chapter":"10 Probability Distributions","heading":"10.8 Continuous probability distributions","text":"random variable X continuous, corresponding probability\ndistribution termed continuous probability distribution. \nseveral continuous distributions. discussion limited \nNormal distribution.","code":""},{"path":"probability-distributions.html","id":"normal-distribution","chapter":"10 Probability Distributions","heading":"10.8.1 Normal distribution","text":"normal distribution defined following probability density function (probability density function explained section)\\[f\\left( x \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{- \\frac{{(x - \\mu)}^{2}}{2\\sigma^{2}}}\\], \\(- \\infty < x < + \\infty\\) ; \\(Œº\\) population mean \\(œÉ\\)2 \nvariance, e = 2.718.random variable \\(X\\) follows normal distribution, write:\\(X\\)~\\(N\\)(\\(Œº\\), \\(œÉ\\)2)particular, normal distribution \\(Œº\\) = 0 \\(œÉ\\)2 = 1 called\nstandard normal distribution, denoted \\(X\\)~\\(N\\)(0,1).","code":""},{"path":"probability-distributions.html","id":"properties-of-normal-distribution","chapter":"10 Probability Distributions","heading":"Properties of Normal distribution","text":"normal distribution frequently used among probability laws. normal distribution can found many practical problemsIf plot density \\(f(x)\\) \\(x\\) graph bell shaped always\nFigure 10.2: Normal distribution curve\nMany things closely follow Normal Distribution:Yield cropsYield cropsheights peopleheights peoplesize things produced machinessize things produced machineserrors measurementserrors measurementsblood pressureblood pressuremarks testmarks testThe Normal Distribution :mean = median = modeMean located centre curve. mean = median = mode, \nlocate towards centre\nFigure 10.3: Mean=Median=Mode normal distribution\nNormal distribution symmetric centre, 50% values less mean 50% greater mean\nFigure 10.4: Normal distribution symmetric distribution\n","code":""},{"path":"probability-distributions.html","id":"standardisation-of-normal-distribution","chapter":"10 Probability Distributions","heading":"10.8.1.1 Standardisation of Normal distribution","text":"standard normal distribution special case normal\ndistribution mean zero standard deviation 1.distribution also known Z-distribution. value \nstandard normal distribution known standard score Z-score.standard score Z-score represents number standard deviations mean specific observation falls.convert value Standard Score (\"z-score\"):First subtract mean,First subtract mean,divide Standard DeviationThen divide Standard DeviationAnd called \"Standardizing\".\nFigure 10.5: Standardization Normal distribution\nExample: survey daily travel time results (minutes):\\(X\\): 26, 33, 65, 28, 34, 55, 25, 44, 50, 36, 26, 37, 43, 62, 35, 38, 45, 32, 28, 34Convert standard scores (Z-score).Mean 38.8 minutes, Standard Deviation 11.4First subtract mean observationFirst subtract mean observationThen divide Standard DeviationThen divide Standard Deviation\nFigure 10.6: Z score values X\n¬†z-score formula¬†using :\\[z = \\frac{x - \\mu}{\\sigma}\\]z \"z-score\" (Standard Score)z \"z-score\" (Standard Score)\\(x\\) value standardized\\(x\\) value standardized\\(Œº\\) ('mu\") mean\\(Œº\\) ('mu\") mean\\(œÉ\\) (\"sigma\") standard deviation\\(œÉ\\) (\"sigma\") standard deviation","code":""},{"path":"probability-distributions.html","id":"parameters-of-normal-distribution","chapter":"10 Probability Distributions","heading":"Parameters of Normal distribution","text":"probability distribution, parameters normal distribution define shape probabilities entirely. normal distribution two parameters, mean (\\(Œº\\)) standard deviation (\\(œÉ\\)). normal distribution just one form. Instead, shape changes based parameter values\nFigure 10.7: Shape changes normal distribution based different means\nStandard deviation:standard deviation measure variability. defines width normal distribution. determines far away mean values tend fall. represents typical distance observations average\nFigure 10.8: Shape changes normal distribution based different standard deviation\nnormally distributed data, standard deviation can used determine proportion values fall within \nspecified number standard deviations mean. example, \nnormal distribution, 68 % observation falls within +/- 1 standard\ndeviation mean. property called Area Property.","code":""},{"path":"probability-distributions.html","id":"area-property","chapter":"10 Probability Distributions","heading":"Area Property","text":"\nFigure 10.9: Area property normal distribution\nshort properties Normal distributionNormal distribution curve bell shapedNormal distribution curve bell shapedNormal distribution symmetric, skewed.Normal distribution symmetric, skewed.mean, median, mode equal.mean, median, mode equal.Half population less mean half greater\nmean.Half population less mean half greater\nmean.Area property allows determine proportion values \nfall within certain distances mean.Area property allows determine proportion values \nfall within certain distances mean.","code":""},{"path":"probability-distributions.html","id":"solved-example","chapter":"10 Probability Distributions","heading":"Solved example","text":"1. z-score value 27, given set mean 24, \nstandard deviation 2?SolutionTo find z-score need divide difference value,\n27, mean, 24, standard deviation set, 2.\\[z = \\frac{27 - 24}{2} = \\frac{3}{2} = 1.5\\]indicates 27 +1.5 standard deviations mean.Using z-value calculate probability; z-value tables etc \ndiscussed practical session.***********************************","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"11 Hypothesis Testing","heading":"11 Hypothesis Testing","text":"Statistical analysis integral part scientific\ninvestigation. statistical analysis can conducted, researcher must generate guess, hypothesis going .process begins working hypothesis. Working hypothesis direct statement research idea. example, plant biologist may think plant height may affected applying different fertilizers. might say: \"Plants different fertilizers grow different heights\".","code":""},{"path":"hypothesis-testing.html","id":"the-falsification-principle","chapter":"11 Hypothesis Testing","heading":"The Falsification Principle","text":"falsification principle Proposed Karl Popper, way demarcating science non-science. suggests theory considered scientific must able tested proven false. example, hypothesis \"swans white,\" can falsified observing black swan. According Popper, science attempt disprove theory, rather attempt continually support theoretical hypotheses.","code":""},{"path":"hypothesis-testing.html","id":"null-hypothesis","chapter":"11 Hypothesis Testing","heading":"11.1 Null hypothesis","text":"considering Popperian Principle Falsification, need translate working hypothesis framework consisting two hypotheses. hypotheses termed Null hypothesis Alternative hypothesis. clear example .Null hypothesis hypothesis either rejected accepted based experiment.example biologist may state null hypothesis average height (mean height) plants different fertilizers .alternative hypothesis (biologist hopes show) average height (mean height) plants different fertilizers equal, .e. fertilizer treatments produced plants different mean heights.¬†Now experiment biologist may either reject null hypothesis\naccept . turn result acceptance rejection \nalternative hypothesis accordingly.finally concentrate either rejecting accepting null\nhypothesis.","code":""},{"path":"hypothesis-testing.html","id":"definitions","chapter":"11 Hypothesis Testing","heading":"Definitions","text":"Null hypothesis: statement 'effect' 'difference'.\" often symbolized H0. hypothesis researcher trying disprove.Alternative hypothesis: simply inverse, opposite, null hypothesis. often symbolized H1.","code":""},{"path":"hypothesis-testing.html","id":"example-9","chapter":"11 Hypothesis Testing","heading":"Example","text":"long ago, people believed world flat. research problem whether Earth flat?Null hypothesis, H0: Earth flat.Alternate hypothesis, H1: world round.Several scientists, including Copernicus, set disprove null hypothesis. eventually led rejection null acceptance alternate.","code":""},{"path":"hypothesis-testing.html","id":"state","chapter":"11 Hypothesis Testing","heading":"11.1.1 Stating hypothesis","text":"Problem 1: researcher thinks certain chemical applied twice week flowering fruit tree. Average fruit weight plant 8 kg.Let us see null hypothesis problem formulatedStep 1: Figure hypothesis problem. hypothesis usually hidden word problem, sometimes statement expect happen experiment.hypothesis question ‚Äúresearcher expects average fruit weight per plant 8 kg.‚ÄùŒº (pronounced ‚Äômu') denotes average yield say, null hypothesis can stated asH0: Œº = 8Step 2: alternative hypothesis. State, happen hypothesis doesn‚Äôt come true? average fruit weight equal 8 kg, one assumed possibility weight less 8 kg (assuming possibility chemical \nincrease yield). alternative hypothesis can stated asH1: Œº < 8But researcher doesn‚Äôt idea happen?Problem 2: researcher studying effect chemical plant yield. chemical used pesticide. wants prove chemical effect yield. chemical can dangerous may boost yield.Step 1: Figure hypothesis problem.hypothesis question ‚Äúresearcher expects change average yield application chemical 0‚ÄùŒº denotes ‚Äòchange‚Äô average yield chemical applicationNull hypothesis can stated asH0: Œº = 0Step 2: alternative hypothesis. Researcher idea whether chemical increase decrease yield. just wants prove chemical effect yield. alternative hypothesis beH1: Œº \\(\\mathbf{\\neq}\\) 0","code":""},{"path":"hypothesis-testing.html","id":"hypo","chapter":"11 Hypothesis Testing","heading":"11.2 Hypothesis testing problem","text":"hypothesis testing decision two alternatives, one called null hypothesis alternative hypothesis, must made. make decision, experiment performed. hypothesis testing acceptance rejection null hypothesis can based decision rule.Example: coin . need check whether coin biased unbiased. Unbiased means 50:50 chance landing head tail tossing .First formulate null hypothesis alternative hypothesis.coin unbiased probability obtaining head 0.5; .e. \\(p\\) = 0.5Therefore null hypothesis alternative hypothesis beH0: \\(p\\) = 0.5H1: \\(p\\) \\(\\mathbf{\\neq}\\) 0.5You designed experiment , toss coin 10 times note outcome. conducting experiment got outcome given belowIn hypothesis testing acceptance rejection null hypothesis can based decision rule. calculate test statistic sample (outcome experiment). Decision rule based statistic.Test statistic: Test statistic quantity computed values sample function sample values based decision made null hypothesis.example decision rule, might decide reject null hypothesis accept alternative hypothesis, 8 heads occur 10 tosses coin. , reject null hypothesis \\(p\\) \\(\\geq \\frac{8}{10} = 0.8\\).experiment testNumber heads = 8Number tosses = 10Test statistic calculated, \\(p\\) = 8/10 = 0.8So based decision rule reject null hypothesis, assume coin biased.Now example sake formulated decision rule. frame decision rule simply. lot factors need considered, like confident reject null hypothesis based sample observations taken (number tosses 10). discuss coming sections.","code":""},{"path":"hypothesis-testing.html","id":"errors-in-hypothesis-testing","chapter":"11 Hypothesis Testing","heading":"11.3 Errors in Hypothesis testing","text":"respect hypothesis testing two errors can occur\n:null hypothesis actually true decision based testing process concluded null hypothesis false rejectedThe null hypothesis actually true decision based testing process concluded null hypothesis false rejectedThe null hypothesis actually false testing process concludes true accepted.null hypothesis actually false testing process concludes true accepted.two errors called Type Type II errors.¬†Type error: Reject null hypothesis, actually trueType II error: Accept null hypothesis, actually false¬†","code":""},{"path":"hypothesis-testing.html","id":"level-of-significance-and-power-of-test","chapter":"11 Hypothesis Testing","heading":"11.4 Level of significance and power of test","text":"Level significance (Œ±)significance level, also denoted Œ± (alpha), probability rejecting null hypothesis true. .e. Level significance probability Type error.Power test (1- Œ≤)probability Type II error denoted Œ≤. ‚Äò1-Œ≤‚Äô termed power test. Power test probability rejecting null hypothesis false. Œ± Œ≤ plays role deciding decision rule hypothesis testing.Seriousness type type II errorsWhich error serious? understand interrelationship Type Type II error, determine error severe consequences situation, consider following example.medical researcher wants compare effectiveness two medications.Null hypothesis (H0): Œº1= Œº2The two medications equally effective.Alternative hypothesis (H1): Œº1‚â† Œº2The two medications equally effective.Type error occurs researcher rejects null hypothesis concludes two medications different , fact, .¬†much serious consequence.Type II error occurs researcher concludes medications , fact, different.\nerror potentially life-threatening, less-effective medication sold public instead effective one.Now example, diagnosing cancerNull hypothesis (H0): Patient cancerAlternative hypothesis (H1): Patient cancerA Type error occurs researcher rejects null hypothesis andconcludes patient cancer, actually healthyType II error occurs researcher concludes patient cancer cancerHere errors can serious consequencesDepending situation seriousness Type Type II error may change. cases Type severe, Type II may severe cases.Now let us look critically situation. try reduce Type error Type II error increase. try reduce Type II error Type increase. make decision, must identify error serious. commit Type 1 error, reject \nnull hypothesis true. false positive, like fire alarm rings fire. Type II error happens fail reject null true. false negative - like alarm fails sound fire.Let us return question error, Type Type II, worse.Consider person accused crime waiting judgement hanged.H0: innocentH1: hangedWhat judge makes Type error?\ninnocent hanged!!!!judge makes Type II error?criminal set free!!!course want let guilty person hook, people say sentencing innocent person punishment worse consequence. Hence, many textbooks instructors say Type (false positive) worse Type II (false\nnegative) error. assuming Type worse least worsening situation.practice, fix Type error selecting suitable probability Œ± experiment. reduce Type II error taking adequate sample size. Usually fix Œ± =0.05 0.01Note: exam question pops , error serious? Answer Type . honest answer - ‚Äòdepends‚Äô","code":""},{"path":"hypothesis-testing.html","id":"region-of-acceptance-and-rejection","chapter":"11 Hypothesis Testing","heading":"11.5 Region of acceptance and rejection","text":"test statistic calculate sample probability distribution. example consider coin tossing experiment discussed earlier. test statistic calculated person different person B, may get different outcomes performing experiment. particular value test statistic \nprobability. probability distribution test statistic termed sampling distribution test statistic.reject null hypothesis, test statistic falls particular area sampling distribution, area sampling distribution test statistic called region rejection. region sampling distribution test statistic called\nregion acceptance, test statistic falls area accept null hypothesis.\nFigure 11.1: Acceptance rejection region sampling distribution test statistic\nSize region rejection equal level significance = Œ±Size region acceptance equal = 1¬≠- Œ±The region rejection also known critical region. value test statistic reject null hypothesis called critical value.\nFigure 11.2: Critical value test statistic\n","code":""},{"path":"hypothesis-testing.html","id":"two-tailed-and-single-tailed-test","chapter":"11 Hypothesis Testing","heading":"11.6 Two tailed and single tailed test","text":"statistical test based two competing hypotheses: null hypothesis H0 alternative hypothesis H1. type alternative hypothesis H1 defines test one-tailed two-tailed. based alternative hypothesis type test determined.","code":""},{"path":"hypothesis-testing.html","id":"one-tailed-tests","chapter":"11 Hypothesis Testing","heading":"One-tailed tests","text":"Consider problem 1 section 11.1.1, alternative hypothesis stated asH1: Œº <8For alternative hypothesis reject null hypothesis test statistics falls towards left side sampling distribution, test left tailed test.H1: Œº > 8For alternative hypothesis reject null hypothesis test statistics falls towards right side sampling distribution, test right tailed test.Left tailed test: critical region towards left side sampling distribution test statistic\nFigure 11.3: Left tailed test: Critical region towards left side (Shown sampling distribution student t left tailed 20 degrees freedom)\nRight tailed test: critical region towards right side sampling distribution test statistic\nFigure 11.4: Right tailed test: Critical region towards right side (Shown sampling distribution student t right tailed 20 degrees freedom)\nTwo-tailed testsConsider problem 2 section 11.1.1, alternative hypothesisH1: Œº \\(\\mathbf{\\neq}\\) 0Consider another alternative hypothesisH1: Œº \\(\\mathbf{\\neq}\\) 8In cases critical region lies sides. Size side Œ±/2. Together total size Œ±\nFigure 11.5: Two tailed test: Critical region side\n","code":""},{"path":"hypothesis-testing.html","id":"decision-rule","chapter":"11 Hypothesis Testing","heading":"11.7 Decision rule","text":"calculation test statistic experiment, make decision null hypothesis?Decision rule largely determined level significance Œ±. good test one low probability committing Type error (.e., small Œ±) high power (1-Œ≤, high power).Power determined sample size experiment. Based Œ± select critical value test statistic, calculated value falls critical value reject null hypothesis (right tailed test). calculated value falls critical value \nreject null hypothesis (left tailed test). calculated value falls critical value sides, reject null hypothesis (two tailed test)","code":""},{"path":"hypothesis-testing.html","id":"an-example","chapter":"11 Hypothesis Testing","heading":"11.8 An example","text":"Consider example testing whether coin biased section 11.2Here going identify whether coin biased based just 10 tossing. (10 sample size).Let null hypothesis beH0: \\(p\\) = 0.5H1: \\(p\\) \\(\\neq\\) 0.5Let \\(X\\) number heads, let us see probability null hypothesis \\(X\\) take value 1 10.know binomial theorem:\\(p(X=x)\\) = \\(n\\)\\(c\\)\\(x\\)\\(p\\)\\(x\\)\\(q\\)\\(n-x\\) = \\(\\frac{n!}{\\left( n - x \\right)!x!\\ }\\) \\(p\\)\\(x\\)\\(q\\)\\(n-x\\)experiment n=10, null hypothesis true \\(p\\) = 0.5. probability distribution \\(X\\) , null hypothesis trueAbove distribution \\(X\\) can plotted ,\nFigure 11.6: Sampling distribution test statistic X\nŒ± level significance. experiment selected Œ± = 0.05. two tailed test critical value value \\(X\\), area Œ±/2 = 0.025. using probability distribution table can see area beyond \\(X\\) = 8 \\(X\\) = 2 approximately area 0.025. critical values\n\\(X\\) = 8 \\(X\\) = 2. means number heads 8 2 reject null hypothesis level significance Œ± =0.05.\nFigure 11.7: Probability distribution X critical region\nCommonly used test statistics \\(t\\), \\(F\\), \\(Z\\) œá2 (pronounced chi-square). Critical values test statistics already available tables.","code":""},{"path":"hypothesis-testing.html","id":"confidence-interval","chapter":"11 Hypothesis Testing","heading":"11.9 Confidence Interval","text":"rejecting null hypothesis level significance Œ±. Meaning can attach 100(1-Œ±)% confidence conclusion. rejecting null hypothesis level significance Œ±=0.05, 100(1-0.05)% .e. 95% confident result. Œ±=0.01 100(1-0.01)% .e. 99% confident\nresult. can also interpreted , can attach 95% confidence means, experiment repeated infinite number times, 95% chance get conclusion.","code":""},{"path":"hypothesis-testing.html","id":"steps-in-hypothesis-testing","chapter":"11 Hypothesis Testing","heading":"11.10 Steps in hypothesis testing","text":"Now conclusion following 7 steps hypothesis testingStep 1: State Null HypothesisStep 2: State Alternative HypothesisStep 3: Set level significance (Œ±)Step 4: Collect Data (experiment scientific methods)Step 5: Calculate test statisticStep 6: Identify critical region test statistic specified level significance (Œ±)Step 7: Compare test statistic critical value test statistic.Step 8: calculated value test statistic greater critical value test statistic reject null hypothesis 100(1 ‚Äì Œ± )% confidence. Otherwise state don‚Äôt enough evidence reject null hypothesis.Don‚Äôt panic, things take little time digest. gone much discussing concept detail. exam point view try answer followingWhat hypothesis?hypothesis?Define null alternative hypothesisDefine null alternative hypothesisDefine test statisticDefine test statisticTwo errors hypothesis testing, definition.Two errors hypothesis testing, definition.Define one-tailed two-tailed testDefine one-tailed two-tailed testDefine Critical region Critical valueDefine Critical region Critical valueDefine Power test, Level significanceDefine Power test, Level significanceWrite steps hypothesis testingWrite steps hypothesis testing","code":""},{"path":"large-sample-test.html","id":"large-sample-test","chapter":"12 Large sample test","heading":"12 Large sample test","text":"sample sample size \\(n\\) greater 30 (\\(n\\) ‚â• 30) known large sample. study sampling distribution statistic large sample known large sample theory","code":""},{"path":"large-sample-test.html","id":"large-sample-tests","chapter":"12 Large sample test","heading":"12.1 Large Sample Tests","text":"section shall discuss application \\(Z\\)-test. test statistic \\(Z\\) calculated large sample follows standard normal distribution. \\(Z\\) \\(\\sim N(0,1)\\)Test single proportionTest single proportionTest equality two proportionsTest equality two proportionsTest single meanTest single meanTest equality two meansTest equality two meansTest equality two standard deviationsTest equality two standard deviations","code":""},{"path":"large-sample-test.html","id":"decision","chapter":"12 Large sample test","heading":"12.1.1 Decision rule for Z test","text":"Let \\(Z\\) calculated value Œ± level significance, reject null hypothesis \\(Z\\) > \\(Z\\)Œ±/2 ; two tailed test\\(Z\\) > \\(Z\\)Œ±/2 ; two tailed test\\(Z\\) > \\(Z\\)Œ± ; right tailed test\\(Z\\) > \\(Z\\)Œ± ; right tailed test\\(Z\\) < - \\(Z\\)Œ± ; left tailed test\\(Z\\) < - \\(Z\\)Œ± ; left tailed testTable values (critical values) Z specified level significance shown ","code":""},{"path":"large-sample-test.html","id":"test-for-a-single-population-proportion","chapter":"12 Large sample test","heading":"12.1.2 Test for a single population proportion","text":"Consider population proportion value, say \\(P\\); \\(P\\) \nunknown, take random sample size \\(n\\) population\ncalculate sample proportion \\(p\\). want test whether \npopulation proportion \\(P\\), unknown equal \\(P\\)0, based \nsample proportion \\(p\\).null hypothesis tested isH0 : \\(P\\) = \\(P\\)0The alternative hypothesis may eitherH1 : \\(P\\) < \\(P\\)0 (called left tailed alternative)OrH1 : \\(P\\) > \\(P\\)0 (called right tailed alternative)OrH1 : \\(P\\) ‚â† \\(P\\)0 (called two tailed alternative)test, calculate test statistic, \\(Z\\) using following\nformula.\\[Z = \\frac{p - P_{0}}{\\sqrt{\\text{pq}}}\\]\\(q\\) = 1 - \\(p\\)null hypothesis true \\(Z\\) follow standard normal distribution mean 0 variance 1. \\(Z\\) \\(\\sim N(0,1)\\)calculated \\(Z\\) compared critical\nvalue \\(Z\\) table standard normal distribution. calculated value higher table value, reject null hypothesis. calculated value less table value, conclude don‚Äôt enough evidence sample reject null hypothesis. See section 12.1.1","code":""},{"path":"large-sample-test.html","id":"example-1-1","chapter":"12 Large sample test","heading":"Example 1","text":"sample 500 apples taken large consignment 60 found bad. Test whether proportion bad apples consignment 0.1 5% level significance.Solution:Sample size (\\(n\\)) = 500Proportion bad apples sample (\\(p\\)) = 60/500 = 0.12Proportion good apples sample (\\(q\\)) = 1 - 0.12 = 0.88We want test whether proportion bad apple population =\n0.1 ; \\(P\\)0 = 0.1The null hypothesis tested isH0 : \\(P\\) = 0.1If want prove proportion bad apples 0.1 \ncan use right tailed test alternative hypothesis belowH1 : \\(P\\) > 0.1So null hypothesis rejected, accept alternative hypothesisLevel significance given 5% , Œ±=0.05Now calculate \\(Z\\)\\(Z = \\frac{p - P_{0}}{\\sqrt{\\text{pq}}}\\)\\(Z = \\frac{0.12 - 0.1}{\\sqrt{0.12 \\times 0.88}}\\)\\(= 0.0615\\)case table value Z 1.645, right tailed test. calculated value Z less table value. , conclude , don‚Äôt enough evidence reject null hypothesis. , can stated proportion bad apples population 0.1.","code":""},{"path":"large-sample-test.html","id":"self-excercise","chapter":"12 Large sample test","heading":"Self excercise","text":"random sample 500 plants taken large experimental field 65 plants found affected yellowing disease. disease rate significant? (Œ± = 0.05)","code":""},{"path":"large-sample-test.html","id":"test-for-equality-of-two-proportions","chapter":"12 Large sample test","heading":"12.2 Test for equality of two proportions","text":"Consider two populations proportion values, say \\(P\\)1 \\(P\\)2 ; unknown, take random sample sizes \\(n\\)1 \\(n\\)2 populations respectively calculate sample proportions \\(p\\)1 \\(p\\)2. want test whether population proportions \\(P\\)1 \\(P\\)2 equal, based sample\nproportions \\(p\\)1 \\(p\\)2.null hypothesis tested isH0 : \\(P\\)1 = \\(P\\)2The alternative hypothesis may eitherH1 : \\(P\\)1 < \\(P\\)2 (called left tailed alternative)OrH1 : \\(P\\)1> \\(P\\)2 (called right tailed alternative)OrH1 : \\(P\\)1‚â† \\(P\\)2 (called two tailed alternative)test, calculate test statistic, \\(Z\\) using following formula.\\[Z = \\frac{p_{1} - p_{2}}{\\sqrt{\\widehat{P}\\widehat{Q}\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\]\\(\\widehat{P} = \\frac{n_{1}p_{1} + n_{1}p_{1}}{n_{1} + n_{2}}\\) \n\\(\\widehat{P} = 1 - \\widehat{Q}\\)null hypothesis true \\(Z\\) follow standard normal distribution mean 0 variance 1. \\(Z\\) ~ N(0,1).calculated value \\(z\\) compared critical\nvalue \\(Z\\) table standard normal distribution. See section 12.1.1. calculated value higher table value, reject null hypothesis. calculated value less table value, conclude don‚Äôt enough evidence sample reject null hypothesis.","code":""},{"path":"large-sample-test.html","id":"example-2-1","chapter":"12 Large sample test","heading":"Example 2","text":"order assess adoption new variety paddy farmers, survey conducted locality. survey covered 80 farmers large land holding 250 farmers small land holding. observed 50 big farmers 78 small farmers adopted new paddy variety. Test whether significant difference\nadoption behaviour two groups farmers (Take Œ± = 0.01)Solution:Sample size first population, framers large land holding\n\\(n\\)1) = 80Sample size first population, framers small land holding\n(\\(n\\)2) = 250Proportion framers large land holding adopted paddy variety\n(\\(p\\)1) = 50/80 = 0.625Proportion framers large land holding adopted paddy variety\n(\\(p\\)2) =78/250 = 0.312We want test whether proportion significantly different\npopulations ,H0 : \\(P\\)1= \\(P\\)2Here alternate hypothesis isH1 : \\(P\\)1 ‚â† \\(P\\)2So, two tailed test\\(\\widehat{P} = \\frac{n_{1}p_{1} + n_{1}p_{1}}{n_{1} + n_{2}}\\)\\(\\widehat{P} = \\frac{80 \\times 0.625 + 250 \\times 0.312}{80 + 250}\\)\\(= 0.3879\\)\\(\\widehat{Q} = 1 - 0.3879 = 0.6121\\)null hypothesis rejected, accept alternative hypothesisLevel significance given 1% , Œ±=0.01Now calculate \\(Z\\) using formula\\(Z = \\frac{p_{1} - p_{2}}{\\sqrt{\\widehat{P}\\widehat{Q}\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\)\\(= \\frac{0.625 - 0.312}{\\sqrt{0.3879 \\times 0.6121\\left( \\frac{1}{80} + \\frac{1}{250} \\right)}}\\)\\(= \\frac{0.313}{\\sqrt{0.2374 \\times 0.0165}}\\)\\(= 5.008\\)Since two tailed test, look critical value \\(Z\\) Œ±/2. case look value \\(Z\\) Œ±/2 = 0.01/2 = 0.005, 2.576 (see section 12.1.1). Since \ncalculated value (5.008) greater table value (2.576) reject null hypothesis conclude significant difference two population proportions.","code":""},{"path":"large-sample-test.html","id":"test-for-a-single-population-mean","chapter":"12 Large sample test","heading":"12.3 Test for a single population mean","text":"Consider population mean, say \\(Œº\\); \\(Œº\\) unknown, take random sample size n population calculate sample mean, denoted \\(\\overline{x}\\). want test whether population mean \\(Œº\\), unknown equal known constant \\(Œº\\)0, based sample mean \\(\\overline{x}\\).null hypothesis tested isH0 : \\(Œº\\) = \\(Œº\\)0The alternative hypothesis may eitherH1 : \\(Œº\\) < \\(Œº\\)0 (called left tailed alternative)OrH1 : \\(Œº\\) > \\(Œº\\)0 (called right tailed alternative)OrH1 : \\(Œº\\) ‚â† \\(Œº\\)0 (called two tailed alternative)particular test two casesCase population standard deviation œÉ knownCase population standard deviation œÉ knownCase population standard deviation œÉ unknownCase population standard deviation œÉ unknownWe discuss case ","code":""},{"path":"large-sample-test.html","id":"population-standard-deviation-œÉ-is-known","chapter":"12 Large sample test","heading":"Population standard deviation œÉ is known","text":"Population standard deviation œÉ known test, \ncalculate test statistic, \\(Z\\) using following formula.\\[Z = \\frac{\\overline{x} - \\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}}\\]null hypothesis true \\(Z\\) follow standard normal distribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule given section 12.1.1","code":""},{"path":"large-sample-test.html","id":"population-standard-deviation-œÉ-is-unknown","chapter":"12 Large sample test","heading":"Population standard deviation œÉ is unknown","text":"Population standard deviation œÉ unknown test, ùúé replaced sample estimate s, calculate test statistic, \\(Z\\) using following formula.\\[Z = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}}\\]null hypothesis true \\(Z\\) follow standard normal distribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule given section 12.1.1","code":""},{"path":"large-sample-test.html","id":"example-3-1","chapter":"12 Large sample test","heading":"Example 3","text":"sample 900 items mean 3.4 cm standard deviation 2.61cm. Test whether population mean 3.25cm 5% significance level.Solution:Null hypothesis, H0 : \\(Œº\\) = 3.25Alternate hypothesis, H1 : \\(Œº\\) ‚â† 3.25; two tailed testSample size (\\(n\\)) = 900Sample mean, \\(\\overline{x}\\) = 3.4Sample standard deviation (\\(s\\)) = 2.61Population mean (\\(Œº\\)0) = 3.25Level significance, Œ± = 0.05\\(Z = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}}\\)\\(Z = \\frac{3.4 - 3.25}{\\frac{2.61}{\\sqrt{900}}} = 1.724\\)Since two tailed test, look critical value \\(Z\\) Œ±/2. case look value Z Œ±/2=0.05/2 =0.025, 1.96 (Decision rule see section 12.1.1). Since \ncalculated value (1.724) less table value (1.96), conclude , don‚Äôt enough evidence reject null hypothesis. , can stated mean 3.25 cm.","code":""},{"path":"large-sample-test.html","id":"self-exercise","chapter":"12 Large sample test","heading":"Self exercise","text":"paddy field sample 36 plants selected random. plants, panicle lengths observed. mean standard deviation measurements 18.7cm 1.25cm respectively. Test whether mean length panicle paddy 19cm. (Œ± = 0.05)","code":""},{"path":"large-sample-test.html","id":"test-for-equality-of-two-means","chapter":"12 Large sample test","heading":"12.4 Test for equality of two means","text":"Let two normally distributed populations means \\(¬µ\\)1 \\(¬µ\\)2 standard deviations \\(œÉ\\)1 \\(œÉ\\)2 respectively. Let samples sizes \\(n\\)1 \\(n\\)2 taken populations. Let sample means \\(\\overline{x}_{1}\\) \\(\\overline{x}_{2}\\) respectively. want test whether population means significantly different based sample means.null hypothesis tested isH0 : \\(¬µ\\)1 = \\(¬µ\\)2The alternative hypothesis may eitherH1 : \\(¬µ\\)1 < \\(¬µ\\)2 (called left tailed alternative)OrH1 : \\(¬µ\\)1> \\(¬µ\\)2 (called right tailed alternative)OrH1 : \\(¬µ\\)1‚â† \\(¬µ\\)2 (called two tailed alternative)two cases testCase population standard deviations equal \\(œÉ\\)1 ‚â† \\(œÉ\\)2Case population standard deviations equal \\(œÉ\\)1 ‚â† \\(œÉ\\)2Case population standard deviations equal \\(œÉ\\)1 = \\(œÉ\\)2 = \\(œÉ\\)Case population standard deviations equal \\(œÉ\\)1 = \\(œÉ\\)2 = \\(œÉ\\)","code":""},{"path":"large-sample-test.html","id":"when-the-population-standard-deviations-are-equal","chapter":"12 Large sample test","heading":"When the population standard deviations are equal","text":"case, calculate test statistic, \\(Z\\) using following formula.\\[Z = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sigma\\sqrt{\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\]null hypothesis true \\(Z\\) follow standard normal distribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule tests (section 12.1.1).Note: \\(œÉ\\) unknown replaced estimate\n\\(\\sqrt{\\frac{n_{1}s_{1}^{2} + n_{2}s_{2}^{2}}{n_{1} + n_{2}}}\\) \\(n\\)1 \\(n\\)2 sample sizes, \\(s\\)1 \\(s\\)2 sample standard deviations.","code":""},{"path":"large-sample-test.html","id":"when-the-population-standard-deviations-are-not-equal","chapter":"12 Large sample test","heading":"12.4.1 When the population standard deviations are not equal","text":"case, calculate test statistic, \\(Z\\) using following formula.\\[Z = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sqrt{\\left( \\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{\\sigma_{2}^{2}}{n_{2}} \\right)}}\\]null hypothesis true \\(Z\\) follow standard normal distribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule tests (section 12.1.1).","code":""},{"path":"large-sample-test.html","id":"example-4-1","chapter":"12 Large sample test","heading":"Example 4","text":"means two single large samples 1000 2000 members 67.5 inches 68 inches respectively. Can samples regarded drawn population standard deviation 2.5 inches. (Test 5% significance level).Solution:Null hypothesis, \\(H\\)0 : \\(Œº\\)1 = \\(Œº\\)2Alternate hypothesis, H1 : : \\(Œº\\)1‚â† \\(Œº\\)2; two tailed testSample size (\\(n\\)1) = 2000Sample size (\\(n\\)2) = 1000Sample mean first group, \\({\\overline{x}}_{1}\\) = 68Sample mean second group, \\({\\overline{x}}_{2}\\) = 67.5Population standard deviation (\\(œÉ\\)) = 2.5Level significance, Œ± = 0.05we calculate test statistic, \\(Z\\) using following formula.\\(Z = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sigma\\sqrt{\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\)\\(Z = \\frac{68 - 67.5}{2.5\\sqrt{\\left( \\frac{1}{2000} + \\frac{1}{1000} \\right)}} = \\frac{0.5}{0.00375} = 133.33\\)Since two tailed test, look critical value \\(Z\\) Œ±/2. case look value \\(Z\\) Œ±/2=0.05/2 =0.025, 1.96 (see section 12.1.1). Calculated value (133.33) much greater table value, reject null hypothesis. may conclude samples drawn population.","code":""},{"path":"large-sample-test.html","id":"self-exercise-1","chapter":"12 Large sample test","heading":"Self exercise","text":"Two random samples drawn two populations following data obtained. Test whether population means equal. \\(n\\)1 = 400, \\(n\\)2 = 400, \\(\\overline{x}_{1}\\) = 250, \\(\\overline{x}_{2}\\) = 220, $_{1}, \\(s\\)1 = 40, \\(s\\)2 = 55","code":""},{"path":"small-sample-tests.html","id":"small-sample-tests","chapter":"13 Small sample tests","heading":"13 Small sample tests","text":"sample size n less 30 (n < 30) known small\nsample. small samples sampling distributions statistic\ncommonly used œá2 (Chi-square), F t distribution. study\nsampling distribution statistic small samples known \nsmall sample theory.Small Sample Tests (sample size (n) < 30)2. Tests based Student t distribution (t-tests)Assumptions t-test:parent population sample drawn normalThe parent population sample drawn normalThe sample random sampleThe sample random sampleThe population standard deviation, œÉ unknownThe population standard deviation, œÉ unknown2.1 Test single population meanConsider population mean, say Œº; Œº unknown,\ntake random sample size n population \ncalculate sample mean, denoted \\(\\overline{x}\\). want test\nwhether population mean Œº, unknown equal known\nconstant Œº0, based sample mean \\(\\overline{x}\\). sample\nsize less 30.null hypothesis tested isH0 : Œº = Œº0The alternative hypothesis may eitherH1 : Œº < Œº0 (called left tailed alternative)OrH1 : Œº > Œº0 (called right tailed alternative)OrH1 : Œº ‚â† Œº0 (called two tailed alternative)\\[t = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}}\\],\n\\(s^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1}\\)null hypothesis t follows t distribution n-1 degrees\nfreedom2.1.1 Decision rule t testLet t calculated value, degrees freedom = n-1, Œ± \nlevel significance, reject null hypothesis |t| > tŒ±/2 ; two tailed test|t| > tŒ±/2 ; two tailed testt > tŒ± ; right tailed testt > tŒ± ; right tailed testt < - tŒ± ; left tailed testt < - tŒ± ; left tailed testWhere tŒ± tŒ±/2 can obtained table Student t\ndistribution given degrees freedom, n-1 level \nsignificance Œ±. calculated value test statistic less\ncritical values table. may reject null hypothesis.\nOtherwise, may accept .Example 9:Based field experiments, new variety green gram expected \ngive yield 12 quintals per hectare. variety tested 10\nrandomly selected farmers‚Äô fields. yields (quintals per hectare)\nrecorded 14.3, 12.6, 13.7, 10.9, 13.7, 12, 11.4, 12, 12.6, \n13.1. results conform expectation?Solution:Null hypothesis, H0 : Œº = 12Alternate hypothesis, H1 : Œº ‚â† 12; two tailed testSample size (n) = 10Sample mean, \\(\\overline{x}\\) =\n\\(\\frac{\\sum_{= 1}^{n}x_{}}{n} = \\ \\)(14.3+12.6+‚Ä¶+13.1)/10 =\n126.3/10=12.63Sample standard deviation (s) = 1.08536Œº0 = 12Level significance, Œ± = 0.05Calculation sample mean sample standard deviation\\[t = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n - 1}}}\\]\\[t = \\frac{12.63 - 12}{\\frac{1.085306}{\\sqrt{10 - 1}}} = \\frac{0.63}{0.3432} = 1.835\\]Table value t corresponding 5% level significance 9\ndegrees freedom 2.262 (two tailed test) ‚Äì see table 1.1 \nend chapter.Since calculated value (1.835) less table value (2.262),\nconclude , don‚Äôt enough evidence reject null\nhypothesis. , can stated mean 12 quintals per hectare.Example 10: Try yourselfThe mean weekly sales soap bars departmental stores 146.3\nbars per store. advertising campaign mean weekly sales \n22 stores typical week 153.7 showed standard deviation\n17.2. advertisement campaign successful?2.2 Test equality two meansLet two normally distributed populations means ¬µ1 \n¬µ2. Let population standard deviations equal unknown. Let\nsamples sizes n1 n2 taken populations.\nLet sample means ùë•ÃÖ1 ùëéùëõùëë ùë•ÃÖ2 respectively. want test\nwhether population means significantly different based\nsample means.two cases situationPopulation variances equalPopulation variances equalPopulation variances unequalPopulation variances unequalBefore proceeding t-test F test performed test homogeneity \npopulation variance (See section).2.2.1 Case population variances equal (homogenous)null hypothesis tested isH0 : Œº1 = Œº2The alternative hypothesis may eitherH1 : Œº1 < Œº2 (called left tailed alternative)OrH1 : Œº1> Œº2 (called right tailed alternative)OrH1 : Œº1‚â† Œº2 (called two tailed alternative)calculate test statistic, \\(t\\) using following formula.\\[t = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{s\\sqrt{\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\],\n\\(s^{2} = \\frac{\\sum_{= 1}^{n_{1}}\\left( x_{1i} - {\\overline{x}}_{1} \\right)^{2} + \\sum_{= 1}^{n_{2}}\\left( x_{2i} - {\\overline{x}}_{2} \\right)^{2}}{n_{1} + n_{2} - 2}\\),\n\\(x_{1i}\\) \\(x_{2i}\\) sample observations population 1 & 2,\nrespectively.null hypothesis t follows t distribution \n\\(n_{1} + n_{2} - 2\\ \\)degrees freedom. Decision rule \nprevious t- test (section 2.1.2).2.2.2 Case population variances unequalThe Welch t-test adaptation Student‚Äôs t-test. used \ncompare means two groups, variances different.null hypothesis tested isH0 : Œº1 = Œº2The alternative hypothesis may eitherH1 : Œº1 < Œº2 (called left tailed alternative)OrH1 : Œº1> Œº2 (called right tailed alternative)OrH1 : Œº1‚â† Œº2 (called two tailed alternative)calculate test statistic, \\(t\\) using following formula.\\[t = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sqrt{\\left( \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} \\right)}}\\]\\(s_{1}\\)\\(\\text{\\ s}_{2}\\) sample standard deviations two\npopulations, respectively.degrees freedom Welch t-test calculated follows:\\[\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} \\right)^{2}}{\\frac{s_{1}^{4}}{n_{1}^{2}\\left( n_{2} - 1 \\right)} + \\frac{s_{2}^{4}}{n_{2}^{2}\\left( n_{1} - 1 \\right)}}\\]t value determined, read t table \ncritical value Student‚Äôs t distribution corresponding \nsignificance level. Decision rule previous t- test\n(section 2.1.2).Example 11:order compare effectiveness two sources nitrogen, namely\nammonium chloride urea grain yield paddy, experiment \nconducted. results grain yield paddy (kg/plot) \ntwo treatments given .Ammonium chloride: 13.4, 10.9, 11.2, 11.8, 14, 15.3, 14.2, 12.6, 17,\n16.2, 16.5, 15.7Urea: 12, 11.7, 10.7, 11.2, 14.8, 14.4, 13.9, 13.7, 16.9, 16, 15.6,\n162.3 Paired t-testPaired Student‚Äôs t-test used compare means two related\nsamples. two values (pair values) \nsamples. example, 20 cows received treatment 3 months. \nquestion test whether treatment impact milk\nyield cow end 3 months treatment. milk yield \n20 cows measured treatment. gives\nus 20 sets values treatment 20 sets values \ntreatment. case, order test whether \nsignificant difference , paired t-test can \nused; two sets values compared related. \npair values cow (one treatment).Suppose two correlated random samples x1, x2, ...,\nxn y1, y2, ..., yn. want test whether \npopulation means significantly different.Welch t-test adaptation Student‚Äôs t-test. used \ncompare means two groups, variances different.null hypothesis tested isH0 : Œº1 = Œº2The alternative hypothesis may eitherH1 : Œº1 < Œº2 (called left tailed alternative)OrH1 : Œº1> Œº2 (called right tailed alternative)OrH1 : Œº1‚â† Œº2 (called two tailed alternative)calculate test statistic, \\(t\\) using following formula.\\[t = \\frac{|d|}{\\frac{s}{\\sqrt{n}}}\\]\\(d_{} = x_{} - y_{}\\),\n\\(\\overline{d} = \\frac{\\sum_{= 1}^{n}d_{}}{n}\\),\n\\(s^{2} = \\frac{\\sum_{= 1}^{n}\\left( d_{} - \\overline{d} \\right)^{2}}{n - 1}\\)null hypothesis t follows t distribution \n\\(n - 1\\ \\)degrees freedom. Decision rule previous\nt- test (section 2.1.2).Example 12:experiment plots divided two equal parts. One part\nreceived soil treatment second part received soil treatment B.\nplot planted sorghum. sorghum yield (kg/plot) \nobserved shown . Test effectiveness soil treatments \nsorghum yieldSolution:Null hypothesis, H0 : Œº1 = Œº2, , significant\ndifference effects two soil treatmentsAlternate hypothesis, H1 : : Œº1‚â† Œº2; two tailed test, \nsignificant difference effects two soil treatmentsLevel significance, Œ± = 0.05\\[t = \\frac{|d|}{\\frac{s}{\\sqrt{n}}}\\]\\[t = \\frac{| - 2|}{\\frac{1.309}{\\sqrt{8}}}\\]\\[= \\frac{2}{\\frac{1.309}{2.828}}\\]\\[= \\frac{2}{0.4629}\\]\\[= 4.321\\]Table value t 7 degrees freedom 5% level significance\n2.365As calculated value (4.321) greater table value (2.365). \nreject null hypothesis H0. conclude significant\ndifference two soil treatments B. Soil\ntreatment B increases yield sorghum significantly.Example 13: Try yourselfA certain stimulus administered 12 patients resulted \nfollowing increase Blood pressure: 5, 2, 8, -1, 3, 0, -2, 1, 5, 0, 4,\n6. Can concluded stimulus , general accompanied\nincrease Blood pressure? (tip: difference \\(d_{}\\)given)2.4 Testing significance correlation coefficientLet two normally distributed populations means ¬µ1 \n¬µ2 standard deviations œÉ1 œÉ2 respectively. Let\ncorrelation two populations œÅ. want test null\nhypothesis population correlation coefficient zero (œÅ =0). \ncan use t- test purpose. don‚Äôt enough evidence \nsample reject null hypothesis, may conclude \nsignificant correlation populations ((œÅ ‚â† 0).null hypothesis tested isH0 : œÅ = 0The alternative hypothesisH1 : œÅ ‚â† 0 (two tailed alternative)\\[t = \\frac{r\\sqrt{n - 2}}{\\sqrt{1 - r^{2}}}\\]null hypothesis t follows t distribution \n\\(n - 2\\ \\)degrees freedom. reject null hypothesis, \ncalculated value greater table value t corresponding \n\\(n - 2\\ \\)degrees freedom level significance (Œ±) \\[case Œ± =\n0.05\\]Example 14:coefficient correlation 0.2 derived random sample \n625 pairs observations. Test whether population correlation\ncoefficient significant .Solution:Null hypothesis, H0 : œÅ = 0 (Population correlation coefficient \nzero)Alternative hypothesis, H1 : œÅ ‚â† 0 (Population correlation\ncoefficient zero)Sample correlation coefficient (\\(r\\)) = 0.2Number pairs (n) = 625\\[t = \\frac{r\\sqrt{n - 2}}{\\sqrt{1 - r^{2}}}\\]\\[= \\frac{0.2\\sqrt{625 - 2}}{\\sqrt{1 - 0.04}}\\  = 5.095\\]Sample size large (>30) t distribution can approximated z\ndistribution. Critical value two tailed test 5% level \nsignificance 1.96. calculated value 1.96, \nreject null hypothesis conclude , significant\ncorrelation population.3. Chi square test (œá2)Chi-square tests based sampling distribution called\nchi-square distribution (œá2 distribution). œá2tests based \nfollowing assumptionsThe sample observations independent.sample observations independent.total frequency reasonably large, say, greater\n50.total frequency reasonably large, say, greater\n50.theoretical cell frequencies less 5. \ntheoretical cell frequency less 5, application\nœá2tests, pooled preceding succeeding frequencies\npooled frequency 5 finally adjust \ndegrees freedom lost pooling.theoretical cell frequencies less 5. \ntheoretical cell frequency less 5, application\nœá2tests, pooled preceding succeeding frequencies\npooled frequency 5 finally adjust \ndegrees freedom lost pooling.Constraints cell frequencies linear. (eg., ‚àë ùëÇùëñ = ‚àë\nùê∏ùëñ (O E represents observed expected\nfrequencies)Constraints cell frequencies linear. (eg., ‚àë ùëÇùëñ = ‚àë\nùê∏ùëñ (O E represents observed expected\nfrequencies)Note:œá2 tests make assumptions regarding parent\npopulation observations taken. tests \ninvolve population parameter. Hence tests known \nnon-parametric tests distribution free tests.Degrees freedom œá2 tests: Degrees freedom œá2 tests\nrefers number independent variates make \nstatistic. degrees freedom general total number \nobservations less number independent constraints imposed \nobservations. example, k number independent\nconstraints set data n observations, degrees \nfreedom = n-k.Three important chi-square tests:Chi-square test goodness fitChi-square test goodness fitChi-square test independence attributesChi-square test independence attributesChi-square test variance.Chi-square test variance.3.1 Chi square test (œá2) goodness fitA powerful test testing significance discrepancy\ntheory experiment given Prof.¬†Karl Pearson 1900\nknown ‚Äúœá2 tests goodness fit‚Äù.want test null hypothesis, H0: significance\ntheory experimentAgainst alternative hypothesis H1: significance \ntheory experimentIf Oi (=1,2,...,n) set observed frequencies Ei\n(=1,2,...,n) corresponding set expected (theoretical)\nfrequencies, Karl Pearson‚Äôs chi-square test statistic given \\[\\chi^{2} = \\sum_{= 1}^{n}\\frac{\\left( O_{} - E_{} \\right)^{2}}{E_{}}\\]Oi represents ith observed frequency Ei\nrepresents corresponding expected frequency according \nassumption regarding theory behind data. null hypothesis\nchi-square follows chi-square distribution n-1 degrees \nfreedom.3.1.1 Decision rule goodness fit test.Let \\(\\chi_{\\text{cal}}^{2}\\) calculated value, degrees freedom\n= n-1, Œ± level significance, reject null\nhypothesis \\(\\chi_{\\text{cal}}^{2}\\) > \\(\\chi_{\\text{tab}}^{2}\\); \n\\(\\chi_{\\text{tab}}^{2}\\) table value \\(\\chi^{2}\\)n-1 degrees\nfreedom. case \\(\\chi^{2}\\) test one tailed test used.Example 15:plant genetics, interest may test whether observed\nsegregation ratios deviate significantly mendelian ratios. \nsituations want test agreement observed \ntheoretical frequency, test called test goodness fit.\ncross parents genetic constitution AAbb aaBB,\nphenotypes sample classified follows:expected occur 9: 3: 3: 1 ratio. data agree \ntheoretical ratio?Solution:\\[\\chi^{2} = \\sum_{= 1}^{n}\\frac{\\left( O_{} - E_{} \\right)^{2}}{E_{}}\\]\\[\\chi^{2} = 0.676\\]\\(\\chi_{\\text{cal}}^{2}\\)= 0.676, table value chi-square 4-1=3\ndegrees freedom 5% level significance 7.815. won‚Äôt\nreject null hypothesis, H0: significance \ntheory experiment. Conclude data follows 9:3:3:1Example 16: Try yourselfThe number yeast cells counted haemocytometer compared \ntheoretical value given . experimental result support\ntheory.3.2 Chi square test (œá2) independence attributesThe Chi-square test independence checks whether two attributes \nlikely related . example, chemical treatment \ngermination can two attributes. want know whether chemical\ntreatment influence germination, can use chi-square test.\npurpose, need data arranged form contingency\ntable.3.2.1 Contingency tableA contingency table consists collection cells containing counts.\ncontingency table tabular representation categorical data. \ncontingency table usually shows frequencies particular combinations\nvalues two discrete random variables X Y. cell \ntable represents mutually exclusive combination X-Y values.Example 17: Contingency tableIn order determine possible effect chemical treatment \nrate germination cotton seeds pot culture experiment \nconducted. results given form contingency\ntable given . (X = Germination, Y = Chemical Treatment).\nAttribute X two class X1 = Germinated, X2 = germinated.\nAttribute Y two class Y1 = Treated, Y2 = Untreated.Y= Chemical treatmentGerminated(X1)Germinated(X2)Treated(Y1)Untreated(Y2)Let us consider two attributes & B, divided r classes A1,\nA2, ..., Ar B divided s classes B1, B2,\n..., Bs. various cell frequencies can expressed form\ntable (called r √ó s contingency table) shown ...................*Total**(AiBj) = number persons (items) possessing attributes\nAi (=1,2,..., r) Bj (j =1,2,...,s)(Ai) = number persons (items) possessing attribute Ai (\n=1,2,..., r)(Bj) = number persons (items) possessing attribute Bj\n(j =1,2,..., s)‚àë()ùëñ = ‚àë(B)ùëó = ùëÅ, total frequency.3.2.1.1 Expected frequenciesThe expected frequencies corresponding observed frequency\n(AiBj) calculated formula,\\[E_{\\text{ij}} = \\frac{\\left( A_{} \\right)\\left( B_{j} \\right)}{N}\\]3.2.1.2 Degrees freedomDegrees freedom r √ó s contingency table = (r ‚Äì 1)(s\n‚Äì 1)Test procedureThe null hypothesis tested H0: two attributes \nconsideration independent.alternative hypothesis H1: two attributes \nconsideration independent.Test statistic used \\[\\chi^{2} = \\sum_{= 1}^{r}{\\sum_{j = 1}^{s}\\frac{\\left( O_{\\text{ij}} - E_{\\text{ij}} \\right)^{2}}{E_{\\text{ij}}}}\\],\\(O_{\\text{ij}}\\) = observed frequencies\\(E_{\\text{ij}}\\ \\)= Expected frequenciess = number rowsr = number columnsIt can verified \n\\(\\sum_{= 1}^{r}{\\sum_{j = 1}^{s}O_{\\text{ij}}} = \\sum_{= 1}^{r}{\\sum_{j = 1}^{s}E_{\\text{ij}}}\\)null hypothesis test statistic follows chi-square distribution\n(r ‚Äì 1)√ó(s ‚Äì 1) degrees freedom. Decision rule \n3.1.1Example 18:survey, random sample 198 farms classified three\nclasses according tenure status : owned, rented mixed. \nalso classified according level soil fertility : high\nfertile, moderately fertile low fertile farms. results given\n. Test whether tenure status depends soil fertilitySolution:Calculation expected values (\\(E_{\\text{ij}})\\) cell \nmultiplying corresponding row total column total divided total\nfrequency table\\(\\chi_{\\text{cal}}^{2}\\)= 26.3, table value chi-square (3-1)(3-1)\n= 4 degrees freedom 5% level significance 9.488. Since \ncalculated value greater table value, reject null\nhypothesis, conclude two attributes consideration \nindependent.3.2.2 Chi-square test 2√ó2 contingency table2 x 2 contingency tableWhen number rows number columns equal 2; \ntermed 2 x 2 contingency table. following form \nshown example 17. General form can represented shown .\nConsider two attributes B classes A1, A2 \nB1, B2 respectively. , b, c, d frequencies cellR1, R2 C1, C2 row totals column totals\nrespectively. n total number observations.case 2 x 2 contingency table \\(\\chi^{2}\\ \\)can directly found\nusing short cut formula.null hypothesis tested H0: two attributes \nconsideration independent.alternative hypothesis H1: two attributes \nconsideration independent.\\[\\chi^{2} = \\frac{n\\left( ad - bc \\right)^{2}}{C_{1}C_{2}R_{1}R_{2}}\\]null hypothesis test statistic follows chi-square distribution\n(2 ‚Äì 1) √ó (2 ‚Äì 1) = 1 degrees freedom.3.2.2.1 Yate‚Äôs correction continuityIn 2 X 2 contingency table, number degrees freedom (2-1)\n√ó (2-1) = 1. one cell frequencies less 5, ,\nuse pooling method results \\(\\chi^{2}\\)0 degrees freedom (1\ndegrees freedom lost due pooling) meaningless. \ncase apply correction due Yates usually known Yates‚Äô\ncorrection continuity. Yate‚Äôs correction made adding 0.5\nleast cell frequency adjusting cell frequencies \ncolumn row totals remain . formula test\nstatistic equation (15) now modified given .Test statistic used \\[\\chi^{2} = \\frac{{n\\left( \\left| ad - bc \\right| - \\frac{n}{2} \\right)}^{2}}{C_{1}C_{2}R_{1}R_{2}}\\]Solution Example 17H0: treatment improve germination rate cotton\nseeds. (independent)H1: chemical treatment improves germination rate cotton\nseeds.\\[\\chi^{2} = \\frac{{300\\left( \\left| 118 \\times 40 - 22 \\times 120 \\right| - \\frac{300}{2} \\right)}^{2}}{238 \\times 62 \\times 140 \\times 160}\\]\\[= 3.927\\]\\(\\chi_{\\text{cal}}^{2}\\)= 3.927, table value chi-square (2-1) √ó\n(2-1) = 1 degrees freedom 5% level significance 3.841.\nSince calculated value less table value, don‚Äôt \nenough evidence reject null hypothesis. chemical treatment\nimprove germination rate cotton seeds significantly.Example 19: Try yourselfIn experiment effect growth regulator fruit setting \nmuskmelon, following results obtained. Test whether fruit\nsetting muskmelon application growth regulator \nindependent 5% level.3.3 Chi-square test population varianceConsider normal population mean, say Œº variance\nœÉ2, Œº œÉ2 unknown, take random sample\nsize n population. want test whether population\nvariance œÉ2, unknown equal known constant\nœÉ20, based sample variance.Null hypothesis H0: œÉ2 = œÉ20Against alternative hypothesis H1: œÉ2 > œÉ20The test statistic \\[\\chi^{2} = \\frac{ns^{2}}{\\sigma_{0}^{2}}\\]\n\\(s^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1}\\)\nsample varianceUnder null hypothesis test statistic follows chi-square distribution\nn-1 degrees freedom. Decision rule section 3.1.1Example 20: Try yourselfTest null hypothesis œÉ2 = 0.16 alternative\nhypothesis œÉ2 > 0.16, given \\(s^{2}\\) = 0.01719 random\nsample size 11 normal population.4. F - test testing equality two population variancesLet two normally distributed populations means ¬µ1 \n¬µ2 variances œÉ12 œÉ22 respectively. Let\nsamples sizes n1 n2 taken populations. \nwant test whether population variances significantly\ndifferent based sample variances.Null hypothesis H0: œÉ21 = œÉ22Against alternative hypothesis H1: œÉ21 > œÉ22Test statistic \\[F = \\frac{s_{1}^{2}}{s_{2}^{2}}\\]null hypothesis test statistic follows F distribution \n\\(n_{1} - 1\\) \\(n_{2} - 1\\) degrees freedom.4.1 Decision rule F - test.calculated value greater table value F specified\nlevel significance two degrees freedom (.e. \\(n_{1} - 1\\)\n\\(n_{2} - 1\\)) reject null hypothesis.Note:\\(s_{2}^{2} >\\) \\(s_{1}^{2}\\) test statistic \\[F = \\frac{s_{2}^{2}}{s_{1}^{2}}\\]null hypothesis test statistic follows F distribution \n\\(n_{2} - 1\\) \\(n_{1} - 1\\) degrees freedom.Example 20: Try yourselfFor random sample representing one normal population, \\(n_{1}\\)\n= 11, \\(s_{1}^{2}\\) = 21.87. another random sample representing\nsecond normal population, \\(\\ n_{2}\\)= 8 \\(s_{2}^{2}\\) =\n15.36. Test equality variances.","code":""},{"path":"design-of-experiments.html","id":"design-of-experiments","chapter":"14 Design of experiments","heading":"14 Design of experiments","text":"","code":""},{"path":"design-of-experiments.html","id":"introduction-1","chapter":"14 Design of experiments","heading":"14.1 Introduction","text":"Design Experiments integral component agricultural research. scientifically designed experiment valuable tool advancement gaining new knowledge technology development. ‚Äúeffective use tools statistical design experiments paved way green revolution‚Äù ‚Äì words father green revolution India, Dr.¬†M.S. Swaminathan, shows important Design analysis experiments well statistical science agricultural experiments. carefully designed experiment able answer queries researcher accuracy reliability efficient use available resources experimenters. Thus, successful experimentation, highly desirable scientists researchers scientific disciplines, including agricultural sciences, understand basic principles designing experiment analysis resultant data completed experiment. may emphasized researcher always consult statistician , experimentation, convinced enough using design experiment analysis technique data.scientific investigation involves formulation certain assertions (hypotheses) whose validity examined data generated experiment conducted purpose. term ‚Äôexperiment‚Äôdefined systematic procedure carried controlled conditions order discover unknown effect, test establish hypothesis, illustrate known effect.Experiments can designed many different ways collect information. Design experiments (DOE) systematic method determine relationship factors affecting process output process. words, used find cause--effect relationships.DOE structured approach conducting experiments. Mainly aims atValidityValidityReliabilityReliabilityReplicabilityReplicabilityOptimalityOptimality","code":""},{"path":"design-of-experiments.html","id":"a-simple-example","chapter":"14 Design of experiments","heading":"14.2 A simple example","text":"\nFigure 14.1: Poultry manure, cow dung coirpith compost\ndecided conduct experiment. Consider layman knowledge design experiments. , selected 3 potted plants experiment. 3 organic manures applied potted plants.\nFigure 14.2: Treatments given potted plants shown\n\nFigure 14.3: Yield observed plants\n, based experiment, can say poultry manure best?, based experiment, can say poultry manure best?somebody repeats experiment somewhere results different?somebody repeats experiment somewhere results different?variance due experimental error?variance due experimental error?experimenter wants show poultry manure best? allotted healthy plant poultry manure.experimenter wants show poultry manure best? allotted healthy plant poultry manure.Can experiment like validity?Can experiment like validity?Can make conclusion experiment?Can make conclusion experiment?Answer question gives importance proper designing experiments. Nobody scientific fraternity going accept experiment. experiment validity validity proved statistical theories. issues can well taken care proper designing experiments. discussing basic principles design, shown, experiment looks like proper designing.Design experiment means design experiment. sense observations measurements obtained answer query valid, efficient economical way. designing experiment analysis obtained data inseparable. experiment designed properly keeping mind question, data generated valid proper analysis data provides valid statistical inferences. experiment well designed, validity statistical inferences questionable may invalid.","code":""},{"path":"design-of-experiments.html","id":"importance-of-doe","chapter":"14 Design of experiments","heading":"14.3 Importance of DoE","text":"Reduce, control provides estimate experimental ErrorReduce, control provides estimate experimental ErrorGives structured approachGives structured approachIt reduce cost experiment considerable reliabilityIt reduce cost experiment considerable reliabilityProduces statistically valid resultsProduces statistically valid resultsAllows accommodate changesAllows accommodate changesReduce complexityReduce complexityImproves accountabilityImproves accountability","code":""},{"path":"design-of-experiments.html","id":"characteristics-of-a-good-design","chapter":"14 Design of experiments","heading":"14.4 Characteristics of a good design","text":"Provides unbiased estimates factor effects associated uncertaintiesProvides unbiased estimates factor effects associated uncertaintiesEnables experimenter detect important differencesEnables experimenter detect important differencesIncludes plan analysis reporting resultsIncludes plan analysis reporting resultsGives results easy interpretGives results easy interpretPermits conclusions wide validityPermits conclusions wide validityMinimal resource usageMinimal resource usageIs simple possibleIs simple possibleStatistical design experiments refers process\nplanning experiment appropriate data\ncollected analyzed statistical methods,\nresulting valid objective conclusions. \nstatistical approach experimental design necessary\nwish draw meaningful conclusions \ndata. problem involves data subject\nexperimental errors, statistical methods \nobjective approach analysis.Creation controlled conditions main\ncharacteristic feature experimentation DOE\nspecifies nature control operations \nexperiments. Proper designing ensures \nassumptions required appropriate interpretations\ndata satisfied thus increasing accuracy \nsensitivity results.two aspects experimental problem:\ndesign experiment statistical\nanalysis data. two subjects closely\nrelated method analysis depends\ndirectly design employed.\nFigure 14.4: DoE Statistical analysis\n","code":""},{"path":"design-of-experiments.html","id":"brief-history","chapter":"14 Design of experiments","heading":"14.5 Brief history","text":"statistical principles underlying design experiments pioneered R. . Fisher 1920s 1930s Rothamsted Experimental Station, agricultural research station around fourty kilometres north London. Fisher shown way draw valid conclusions field experiments nuisance variables temperature, soil conditions, rainfall present. introduced concept analysis variance (ANOVA) partitioning variation present data () due attributable factors, (b) due chance factors. methodologies colleague Frank Yates developed now widely used. methodologies profound impact agricultural sciences research.Though experimental design initially introduced agricultural context, method applied successfully industry since 1940s. George Box co-workers developed experimental design procedures optimizing chemical processes, particularly response surface designs chemical process industries.Recently, experimental designs also used clinical trials. evolved 1960s medical advances previously based unreliable data. example, doctors used examine patients publish papers based data. biases resulting kinds studies became known. led move toward making randomized double-blind clinical trial standard approval new product, medical device, procedure. scientific application valid designing analysis following proper statistical methods became important clinical trials.recently experimental design techniques started gaining popularity area computer-aided design engineering using computer/simulation models including applications manufacturing industries.","code":""},{"path":"design-of-experiments.html","id":"some-terms-involved","chapter":"14 Design of experiments","heading":"14.6 Some terms involved","text":"","code":""},{"path":"design-of-experiments.html","id":"treatments","chapter":"14 Design of experiments","heading":"14.6.1 Treatments","text":"term treatments used denote different objects, methods processes among comparison made. example, experimenter wants identify among objects/methods/process best based experiment; objects/methods/process called treatment. clearly anything compare experiment known treatment.examples treatments different kinds fertilizer agronomic experiments, different irrigation methods levels irrigation, different fungicides pest management experiments , doses different drugs chemicals laboratory experiments, different varieties crops, different pesticides, grazing systems animals, different tree species agro-forestry experiments, different concentrations solute chemical experiments etc.","code":""},{"path":"design-of-experiments.html","id":"control","chapter":"14 Design of experiments","heading":"14.6.2 Control","text":"control treatment standard treatment used baseline basis comparison treatments. control treatment might treatment currently use, might treatment . example, study new pesticides use standard pesticide control treatment, experiment involving fertilizers may one treatment fertilizers . clinical trials, control treatment generally placebo.","code":""},{"path":"design-of-experiments.html","id":"experimental-units","chapter":"14 Design of experiments","heading":"14.6.3 Experimental units","text":"Experimental units subjects objects treatments applied. example, plots land receiving fertilizer, groups animals receiving different feeds, batches chemicals receiving different temperatures, pots glasshouse experiments, Petri dishes tissues culture bacteria micro-organisms laboratory experiments, etc","code":""},{"path":"design-of-experiments.html","id":"response","chapter":"14 Design of experiments","heading":"14.6.4 Response","text":"Responses measurable outcomes, observed applying treatment experimental unit. Alternatively, response measure find happened\nexperiment. experiment, may one response. examples responses grain yield straw yield, nitrogen content plants biomass plants, quality parameters produce, percentage plants infested disease, weight gain animals, etc.","code":""},{"path":"design-of-experiments.html","id":"factors","chapter":"14 Design of experiments","heading":"14.6.5 Factors","text":"Factors variables whose influence response variable studied experiment. one factor studied experiment experiment called single factor experiment. one factor studied simultaneously experiment, experiment called multi-factor factorial experiment. term factor commonly used case factorial experiments. example, temperature concentration chemicals chemical experiment two factors, Nitrogen, Phosphorus Potassium fertilizers three factors agronomic experiment.Dose time application chemical formulation two factors laboratory experiment.","code":""},{"path":"design-of-experiments.html","id":"factor-levels","chapter":"14 Design of experiments","heading":"14.6.6 Factor levels","text":"term factor levels simply levels used denote values settings factor takes factorial experiment. example, doses nitrogenous fertilizer 0 kg/ha, 30 kg/ ha, 80 kg/ha three levels factor fertilizer. 10%, 20%, 30%, 40% concentration solute solution four levels factor solute laboratory experiment. Presence polythene sheet surface soil absence two levels factor management practice water management study.","code":""},{"path":"design-of-experiments.html","id":"observational-unit","chapter":"14 Design of experiments","heading":"14.6.7 Observational Unit","text":"observational unit unit response variables measured. Observational units often experimental units, may true always. mistake confusing observational unit experimental unit leads pseudo-replication discussed paper (Hurlbert 1984). Consider experiment investigate effects ultraviolet (UV) levels growth smolt. experiment conducted two tanks one tank receives high levels UV light tank receives UV light. Fish placed tank end experiment growths individual fish measured. experiment, tanks experimental units observational units smolts. treatments, presence absence UV light, applied tanks individual fish whole group fish simultaneously exposed UV radiation. tank effect completely confounded treatment effect separated. Another example inorganic fertilizers applied plots field containing plants. time harvest, plants plot harvested. sample plants harvested. case plot experimental unit fertilizers applied observational units plants sampled.","code":""},{"path":"design-of-experiments.html","id":"experimental-error","chapter":"14 Design of experiments","heading":"14.7 Experimental error","text":"explain experimental error consider example given (Gomez Gomez 1984).\nConsider plant breeder wishes compare yield new rice variety standard variety B known tested properties. lays two plots equal size, side side, sows one variety variety B. Grain yield plot measured variety higher yield judged better. Despite simplicity common-sense appeal procedure just outlined, one important flaw. presumes difference yields two plots caused varieties nothing else. certainly true. Even variety planted plots, yield differ. factors, soil fertility, moisture, damage insects, diseases, birds also affect rice yields. factors affect yields, satisfactory evaluation two varieties must involve procedure can separate varietal difference sources variation. , plant breeder must able design experiment allows decide whether difference observed caused varietal difference factors.logic behind decision simple. Two rice varieties planted two adjacent plots considered different yielding ability observed yield difference larger expected, plots planted variety.Hence, researcher needs know yield difference plots planted different varieties, also yield difference plots planted variety. difference among experimental plots treated alike called experimental error. error primary basis deciding whether observed difference real just due chance. Clearly, every experiment must designed measure experimental error.Response experimental units receiving treatment may even similar conditions. variations responses may due various reasons. factors like heterogeneity soil, climatic factors genetic differences, etc also may cause variations (known extraneous factors).Definition: variations response caused extraneous factors known experimental error.aim designing experiment minimize experimental error.","code":""},{"path":"design-of-experiments.html","id":"basic-principles-of-design","chapter":"14 Design of experiments","heading":"14.8 Basic principles of design","text":"three basic principles designing experiment namely randomization, replication local control (blocking).","code":""},{"path":"design-of-experiments.html","id":"rand","chapter":"14 Design of experiments","heading":"14.8.1 Randomization","text":"Randomization means random assignment conditions study treatments subjects experimental units. principle randomization involves allocation treatment experimental units random avoid bias experiment resulting influence extraneous unknown factor may affect experiment.development analysis variance (ANOVA), assume errors random independent. turn, observations also become random randomization.observations independent identically distributed normal variate important assumption hypothesis testing problems involving test statistics F (Snedecor‚Äôs F) t (Student‚Äôs t). major purpose randomization.Randomization forms basis valid experiment replication also needed validity experiment. randomization process every experimental unit equal chance receiving treatment, called complete randomization.Consider example suppose want randomly allot 3 treatments 3 experimental units. ? easy; just label units 1 3. Make lot equal size labelling 1,2 3. Put labels bowl pick eyes closed. Now 1 comes; first treatment alloted 1st unit. simple technique randomization. Random number tables computer generated random numbers can also used.\nFigure 14.5: Taking lot bowl also procees randomization\n","code":""},{"path":"design-of-experiments.html","id":"rep","chapter":"14 Design of experiments","heading":"14.8.2 Replication","text":"replication principle, treatment repeated number times obtain valid reliable estimate possible one observation . Replication provides efficient way increasing precision experiment. precision increases increase number observations. Replication provides observations treatment used, \nincreases precision.Replication enables experimenter obtain valid estimate \nexperimental error. Estimate experimental error permits statistical\ninference; example, performing tests significance obtaining\nconfidence interval, etc. replication, \nresearcher able estimate experimental error. \nseen later Chapters, estimated\nexperimental error null hypotheses tested.\nFigure 14.6: Treatments alloted four plots, replication treatment 2\nresults experiment shown Figure 1.7. yield kg per plot given bracket.\nFigure 14.7: yield kg per plot given bracket\nexperiment, experimental error can estimated \n\\(\\frac{\\left( 6 - 8 \\right)^{2} + {(5 - 4)}^{2}}{2} = \\frac{4 + 1}{2} = 2.5\\);\ndenominator 2 number replications.can also calculated square difference observation\ncorresponding treatment mean, mean \n\\(\\frac{6 + 8}{2} = 7\\); mean B \\(\\frac{5 + 4}{2} = 4.5\\). sum\nsquare difference observation treatment mean \ntaken shown \n\\(\\left( 6 - 7 \\right)^{2} + \\left( 8 - 7 \\right)^{2} + \\ \\left( 5 - 4.5 \\right)^{2} + \\ {(4 - 4.5)}^{2} = 2.5\\)Thus, replication helps estimate experimental error. Increasing \nsize experiment increasing replication also helps \nincrease precision estimating pairwise differences among \ntreatment effects. . Replication provides efficient way increasing\nprecision experiment. precision increases \nincrease number observations. Replication provides \nobservations treatment used, increases precision.","code":""},{"path":"design-of-experiments.html","id":"local","chapter":"14 Design of experiments","heading":"14.8.3 Local control (error control)","text":"good experiment incorporates possible means minimizing \nexperimental error; ability detect experimental error\nincreases size experimental error decreases. putting\nexperimental units similar possible together \ngroup (commonly referred block) assigning treatments\nblock separately independently, variation among blocks can\nmeasured removed experimental error. field experiments\nsubstantial variation within experimental field can \nexpected, significant reduction experimental error usually\nachieved use proper blocking.replication used local control reduce experimental\nerror. example, experimental units divided different\ngroups homogeneous within blocks, \nvariation among blocks eliminated ideally, error\ncomponent contain variation due treatments . \n, turn, increase efficiency.field experiment 4 treatments 5 replications. Consider field fertility gradient left right shown figure 1.8.\nFigure 14.8: field fertility gradient left right\nHomogeneity can achieved dividing field groups shown figure 1.9. Now vertical strips can considered block. Plots formed block, treatment allotted randomly. randomization performed blocks. can see example replication equal number blocks, equal 5. Randomization achieved blocks. Local control achieved grouping treatments homogeneous blocks, fertilizer gradient . typical example Randomized Block Design (RBD), discussed chapter\nFigure 14.9: Plots grouped blocks\n","code":""},{"path":"design-of-experiments.html","id":"other-methods-of-error-control","chapter":"14 Design of experiments","heading":"14.9 Other methods of error control","text":"","code":""},{"path":"design-of-experiments.html","id":"border-effect","chapter":"14 Design of experiments","heading":"14.9.1 Border effect","text":"Plants outer areas borders plot get influence treatment applied adjacent plot, may alter response character interest plants (example yield plants may higher), phenomenon called border effect. example, plot particular fertilizer applied treatment adjacent one another fertilizer applied, due seepage plants boarder areas influence fertilizer adjacent plot, may affect yield attributes border plants. Usually taking observations, border plants discarded.","code":""},{"path":"design-of-experiments.html","id":"proper-plot-technique","chapter":"14 Design of experiments","heading":"14.9.2 Proper Plot Technique","text":"essential factors, treatments maintained uniformly experimental units. example, field experiments, required factors soil nutrients, solar energy, plant population, pest incidence, almost infinite number environmental factors maintained uniformly plots experiment. requirement impossible satisfy, however ensure variability among experimental plots minimum, important sources variability taken care using good plot technique. field experiments crops, important sources variability considered among plots treated alike soil heterogeneity, competition effects, mechanical errors.","code":""},{"path":"design-of-experiments.html","id":"data-analysis","chapter":"14 Design of experiments","heading":"14.9.3 Data Analysis","text":"Proper choice data analysis helps controlling error, blocking effective. Covariance analysis commonly used purpose. measuring one covariates- characters whose functional relationships character primary interest known, analysis covariance (ANCOVA) can reduce variability among experimental units adjusting values common value covariates.example, animal feeding trial, initial weight animals usually differs. Using initial weight covariate, final weight animals subjected various feeds (.e., treatments) can adjusted values attained experimental animals started body weight. , rice field experiment rats damaged test plots, covariance analysis rat damage covariate can adjust plot yields levels rat damage plot.","code":""},{"path":"uniformity-trials.html","id":"uniformity-trials","chapter":"15 Uniformity trials","heading":"15 Uniformity trials","text":"Research worker perform experiments identical conditions obtain valid results even uniform land can select, still inherent variations soil. , order maintain homogeneity, experimenter good idea nature extent fertility variation land. can obtained results known uniformity trials.Uniformity trials can also planned determine suitable size shape plot number plots block. Uniformity trials enable us idea fertility variation field.","code":""},{"path":"uniformity-trials.html","id":"how-uniformity-trial-is-performed","chapter":"15 Uniformity trials","heading":"15.1 How uniformity trial is performed","text":"Uniformity trial conducted know nature soil fertility gradient. uniformity trial, particular variety crop sown entire experiment field uniformly managed throughout growing season without applying fertilizer. time harvest substantial border removed sides field. remainder field divided small plots termed basic units. size basic unit decided judgment, depending crop. Smaller basic unit accurate study heterogeneity possible. produce basic unit harvested recorded separately basic unit. yield differences basic units taken measure soil heterogeneity study area.Several types analysis available evaluate pattern soil heterogeneity based uniformity trial data. discuss procedures detail.","code":""},{"path":"uniformity-trials.html","id":"fertility-contour-map","chapter":"15 Uniformity trials","heading":"15.2 Fertility Contour Map","text":"approach describe heterogeneity land construct fertility contour map. simple informative presentation soil heterogeneity. constructed taking moving averages yields unit plots demarcating regions fertility \nconsidering areas, yield magnitude. Taking moving average reduce large random variation expected small plots.","code":""},{"path":"uniformity-trials.html","id":"serial-correlation","chapter":"15 Uniformity trials","heading":"15.3 Serial Correlation","text":"Serial correlation procedure generally used\ntest randomness data set. However, also useful characterization trend soil fertility using uniformity trial data. Horizontal vertical serial correlations calculated. correlations give idea whether fertility gradient pronounced horizontally vertically. low serial correlation indicates fertile areas occur spots high value indicates gradient.\\[r_{s} = \\frac{\\sum_{= 1}^{n}{X_{}X_{+ 1} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}}{\\sum_{= 1}^{n}{X_{}}^{2} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}\\]\\(X_{}\\) value ith basic unit \\(n\\) number basic units.","code":""},{"path":"uniformity-trials.html","id":"mean-square-between-strips","chapter":"15 Uniformity trials","heading":"15.4 Mean square between strips","text":"method simpler compute objective serial correlation. Units first combined horizontal vertical strips. Variability strips measured direction mean square strips. relative size two mean squares indicates\nwhether fertility gradient pronounced horizontally vertically.\\[\\text{Sum square}\\left( \\text{Vertical} \\right)\\frac{\\sum_{= 1}^{c}V_{}^{2}}{r} - \\frac{G^{2}}{n}\\ \\]\\[\\text{Sum square}\\left( \\text{Horizontal} \\right)\\frac{\\sum_{= 1}^{c}H_{}^{2}}{c} - \\frac{G^{2}}{n}\\ \\]\\[\\text{mean square}\\left( \\text{Vertical} \\right) = \\frac{\\text{Sum square}\\left( \\text{vertical} \\right)}{c - 1}\\]\\[\\text{mean square}\\left( \\text{Horizontal} \\right) = \\frac{\\text{Sum square}\\left( \\text{Horizontal} \\right)}{r - 1}\\]\\(V_{},H_{}\\) sum total basic units vertical horizontal strips respectively; \\(r\\) number rows \\(c\\) number columns. \\(n\\) total number basic units; \\(n = r \\times c\\). \\(G\\) total basic units.","code":""},{"path":"uniformity-trials.html","id":"fairfield-smiths-variance-law","chapter":"15 Uniformity trials","heading":"15.5 Fairfield Smith‚Äôs Variance Law","text":"Smith (1938) gave empirical relations variance plot size. developed empirical model representing relationship plot size variance mean per plot. model given equation\\[V_{x} = \\frac{V_{1}}{x^{b}}\\]\\[\\log V_{x} = \\ \\log V_{1} - b\\log x\\]\\(x\\) number basic units plot, \\(V_{x}\\) variance mean per plot \\(x\\) units, \\(V_{1}\\) variance mean per plot one unit, \\(b\\) regression coefficient. values \\(b\\) \ndetermined principle least squares. \\(b\\) called Smith‚Äôs index soil heterogeneity. index gives single value quantitative measure heterogeneity area.","code":""},{"path":"uniformity-trials.html","id":"smith","chapter":"15 Uniformity trials","heading":"15.5.1 Smith‚Äôs index of soil heterogeneity","text":"Step-1 Combine basic units simulate plots different sizes shapes. Use combinations fit exactly whole area,.e. product simulated plots number basic units per plot must equal total number basic units.Step-2 simulated plots constructed Step-1, compute yield total T sum basic units construct plot compute plot variance \\(V_{(x)}\\)\\[V_{(x)} = \\sum_{= 1}^{w}\\frac{{T_{}}^{2}}{x} - \\frac{\\left( G \\right)^{2}}{\\text{rc}}\\]\\(w = \\frac{\\text{rc}}{x}\\ \\)total number simulated plots size\\(\\text{\\ x}\\) basic units. \\(r\\) number rows \\(c\\) number columns. \\(G\\) total basic units.Step-3 plot size shape, compute variance per unit area\\[V_{x} = \\frac{V_{(x)}}{rc - 1}\\]Step-4 plot size one shape, test homogeneity -plot variances\n\\(\\mathbf{V}_{\\left( \\mathbf{x} \\right)}\\)determine significance plot orientation (plot-shape) effect, using F test chi-square test. found homogeneous, average \\(\\mathbf{V}_{\\mathbf{(x)}}\\) values plot shapes given size computed, otherwise \\(\\mathbf{V}_{\\mathbf{(x)}}\\)plot shapes \ngiven size used separately calculation.example, two plot shapes size 2m2i.e. 2 √ó 1m 1 √ó 2m. plot \\(\\mathbf{V}_{\\mathbf{(x)}}\\)\ncalculated. Homogeneity tested using F test. non-significant\naverage \\(\\mathbf{V}_{\\mathbf{(x)}}\\) values plot shapes 2\n√ó 1m 1 √ó 2m calculated.Step-5 Using values variance per unit area\n\\(\\mathbf{V}_{\\mathbf{x}}\\) computed steps 3 4, estimate \nregression coefficient \\(\\mathbf{V}_{\\mathbf{x}}\\) plot size\n\\(\\mathbf{x}\\)Using Fairfield Smith‚Äôs Variance Law, can written\ntaking logarithm base e :\\[\\log V_{x} - \\log V_{1} = \\  - b\\log x\\]consider\\[{Y = \\log}V_{x} - \\log V_{1}\\]equation \\(\\log V_{x} - \\log V_{1} = \\  - b\\log x\\) can written form\\(\\mathbf{Y = \\ cX}\\) \\(\\mathbf{c = \\  - b;\\ X =}\\mathbf{\\log}\\mathbf{x}\\)\\(\\mathbf{b}\\) estimated fitting regression \\(\\mathbf{Y}\\) \n\\(\\mathbf{X}\\).Step-6 Obtain adjusted \\(\\mathbf{b}\\) computed \\(\\mathbf{b}\\) value using range \\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\),\n\\(\\mathbf{\\ }\\mathbf{x}_{\\mathbf{1}}\\), size basic unit \n\\(\\mathbf{n}\\) whole area size. Column 2 3 table 2.1 range \\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\). get computed \\(b\\) value \\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\). can calculate corresponding adjusted \\(b\\) using table 2.1. using interpolation follows.\nFind L1 L2 , calculated b lies ;\nL1 ‚â§ bcal ‚â§ L2, L1 L2 values \ncomputed b column table 2.1. Let y1 , y2 value\ncorresponding L1 L2 range \\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\) table 2.1. using formulaAdjusted b value =\n\\(y_{1} + (b_{\\text{cal}} - L_{1})\\frac{\\left( y_{2} - y_{1} \\right)}{\\left( L_{2} - L_{1} \\right)}\\)\nTable 15.1: Adjusted b table. Obtain value b using range values computed b value\nrelatively low value calculated adjusted Smith's index soil\nheterogeneity (\\(\\mathbf{b}\\)) indicates relatively high degree correlation among\nadjacent plots study area, indicates change \nlevel soil fertility tends gradual rather patches. Even\nthough \\(\\mathbf{b}\\) regression coefficient value lies \n0 1.","code":""},{"path":"uniformity-trials.html","id":"mcm","chapter":"15 Uniformity trials","heading":"15.6 Maximum Curvature Method","text":"method used find optimal plot size experiment. method basic units uniformity trials combined form new units. new units formed combining columns, rows . Combination columns rows done way columns rows left . set units, coefficient variation\n(CV) computed. curve plotted taking plot size (terms basic units) X-axis CV values Y-axis graph sheet. point curve takes turn, .e., point maximum curvature located inspection. value corresponding point maximum curvature optimum plot size.","code":""},{"path":"uniformity-trials.html","id":"uniformity-trial-explained","chapter":"15 Uniformity trials","heading":"15.7 Uniformity trial explained","text":"Let us discuss detail procedures explained using example. Consider rice crop field 12m √ó 17m uniformly managed throughout growing season without applying fertilizer. time harvest border 1m removed sides field. resultant effective area now 10m √ó 15m, entire field divided basic units size 1m √ó 1m, yield (gms) noted basic unit. 150 basic units.\nFigure 15.1: Yield observed plots 1x1m size\nNote: r=15;c=10 total number basic units n=r√óc=150","code":""},{"path":"uniformity-trials.html","id":"fertility-contour-map-1","chapter":"15 Uniformity trials","heading":"15.7.1 Fertility contour map","text":"Also known soil productivity contour map. construction fertility contour map explained using example.Step 1: Calculate moving averages 3m√ó3m basic units. .e including three basic units rows 3 basic units columns.\nFigure 15.2: Calculation moving averages 3m√ó3m basic units\nStep 2: Moving averages labeled shown . calculation moving averages now 8 values row 13 values column. Now dimension plot figure can considered 1.25m √ó 1.154m (10/8 = 1.25 15/13 = 1.514)\nFigure 15.3: Moving averages recorded 3m√ó3m basic units\nStep 3: Similar areas given colours get fertility contour map.\nFigure 15.4: Colouring scheme based range moving averages. can decided experimenter\n\nFigure 15.5: Coloured plot labelled moving averages\nFinal fertility contour map obtained shown figure 2.7. Now can get idea fertility gradient field plan create blocks field.\nFigure 15.6: Final fertility contour map\nFertility contour map gives vague idea fertilizer gradient. procedures may give better idea.","code":""},{"path":"uniformity-trials.html","id":"serial-correlation-1","chapter":"15 Uniformity trials","heading":"15.7.2 Serial Correlation","text":"Pair wise calculation vertical values experiment observations shown figure 15.1\nFigure 15.7: Pair wise calculation vertical values\nUsing\\(r_{s} = \\frac{\\sum_{= 1}^{n}{X_{}X_{+ 1} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}}{\\sum_{= 1}^{n}{X_{}}^{2} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}\\)Vertical\n\\(r_{s} = \\frac{100859156 - \\frac{\\left( 122777 \\right)^{2}}{150}}{101325715 - \\frac{\\left( 122777 \\right)^{2}}{150}}\\ \\)=\n0.438627Pair wise calculation Horizontal values shown figure 15.1\nFigure 15.8: Pair wise calculation horizontal values\nUsing\\[r_{s} = \\frac{\\sum_{= 1}^{n}{X_{}X_{+ 1} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}}{\\sum_{= 1}^{n}{X_{}}^{2} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}\\]Horizontal\n\\(r_{s} = \\frac{100946042 - \\frac{\\left( 122777 \\right)^{2}}{150}}{101325715 - \\frac{\\left( 122777 \\right)^{2}}{150}}\\ \\)=\n0.54317Both coefficients low indicates presence fertile areas\nspots. However, horizontal serial correlation coefficient \nlittle high vertical implying fertility gradient \nhorizontal direction vertical. relative magnitude two\nserial correlations , however, used indicate \nrelative degree gradients two directions.","code":""},{"path":"uniformity-trials.html","id":"mean-square-between-strips-1","chapter":"15 Uniformity trials","heading":"15.7.3 Mean square between strips","text":"\nFigure 15.9: Mean square strips uniformity trial data\n\\(G =\\) 122777, \\(n =\\) 150\\(\\text{Sum}\\ \\text{}\\ \\text{square}\\left( \\text{vertical} \\right)\\frac{\\sum_{= 1}^{c}V_{}^{2}}{r} - \\frac{G^{2}}{n} = \\frac{12686^{2} + 12816^{2} + \\ldots 12253^{2}}{15} - \\frac{122777^{2}}{150}\\)\n= 63971.47\\(\\text{Sum}\\ \\text{}\\ \\text{square}\\left( \\text{Horizontal} \\right)\\frac{\\sum_{= 1}^{c}H_{}^{2}}{c} - \\frac{G^{2}}{n} = \\frac{8874^{2} + 8877^{2} + \\ldots 7692^{2}}{10} - \\frac{7725^{2}}{150}\\ \\)=\n341185.7733\\(\\text{mean}\\ \\text{square}\\left( \\text{vertical} \\right) = \\frac{63971.47}{9}\\ \\)=\n7107.941481\\(\\text{mean}\\ \\text{square}\\left( \\text{Horizontal} \\right) = \\frac{341185.773}{14}\\ \\)=\n24370.41238Results show horizontal-strip MS almost 3 times higher \nvertical-strip MS, indicating trend soil fertility \npronounced along length along width field.","code":""},{"path":"uniformity-trials.html","id":"smiths-index-of-soil-heterogeneity","chapter":"15 Uniformity trials","heading":"15.7.4 Smith‚Äôs index of soil heterogeneity","text":"Optimal plot size can identified using method. possible\nplot sizes optimum plot size can found using method explained \nsection 15.5.1. Table shows 9 different plot sizes created\nuniformity trial data figure 15.1.Size plotNo: plotsIt recommended reader first read section\n15.5.1 read example. , illustrate \nvariance per unit area plot size 25m2 (denoted V25) \ncalculated. width length plot 5m √ó 5m. entire plot\narea figure 15.1 divided six plots size 25m2\nshown .\nFigure 15.10: Uniformity trail plot figure 2.1 divided six plots\nNow consider equation plot variance\n\\(V_{(x)} = \\sum_{= 1}^{w}\\frac{{T_{}}^{2}}{x} - \\frac{\\left( G \\right)^{2}}{\\text{rc}}\\).\n\\(w = \\frac{\\text{rc}}{x}\\ \\)total number simulated plots\nsize\\(\\text{\\ x}\\) basic units. \\(r\\) number rows \\(c\\) \nnumber columns. \\(G\\) total basic units.\nFigure 15.11: Totals six plots\n\\(V_{(25)} = \\frac{21366^{2} + 21433^{2}\\ldots + 18811^{2}}{25} - \\frac{\\left( 122777 \\right)^{2}}{150} = \\ \\)218767.7133Variance per unit area given \\(V_{x} = \\frac{V_{(x)}}{rc - 1}\\)\\(V_{25} = \\frac{218767.7133}{149}\\  = \\ \\)1468.24Coefficient variation calculated \n\\(\\frac{\\text{Standard\\ deviation}}{\\text{mean}} \\times 100\\), \nexample plot size 25. Standard deviation= \\(\\sqrt{1468.24}\\) =\n38.31762. Mean entire data set = 818.5133. Therefore C.V =\n\\(\\frac{38.31762}{818.5133} = 0.047\\). Similarly, can calculated \nplot sizes. Vx¬†C.V plot sizes \nsummarized .Size plotNo: plotsFor plot size one shape (plot size 5 \none shape), test homogeneity -plot variances\nV(x), determine significance plot orientation\n(plot-shape) effect, using F test chi-square test. \nplot size whose plot-shape effect non-significant, compute \naverage Vx, values plot shapes proceed \nestimation Smith's index soil heterogeneity.V(x) calculated plot sizesDegrees freedom F test \\((w_{1} - 1,\\ w_{2} - 1)\\), \n\\(w_{1}\\) \\(w_{2}\\) number plots particular shapes . ,\ndegrees freedom = (30-1, 30-1)=(29,29). F calculated ratio\nV(x) plot shapes. Fcal= \\(\\frac{456895.9}{329859.5}\\)\n=1.38. Calculated value F (Fcal) compared table value F.\nTable value F (29,29) degrees freedom, Ftable= 1.860. Since\nFcal < Ftable; Calculated F non-significant, take average \nVx plot shapes proceed. example proceeded\nwithout taking average. included example F test \nbetter understanding theory written.Now Smith's index soil heterogeneity estimated followsSize plotFit linear regression Y = cX, estimate value c; c= -bEstimated value c -0.3952. Therefore, calculated value b =\n0.3952. case\n\\(\\text{}\\frac{x_{1}}{n} = \\frac{1}{150} = 0.0067\\). Now need find adjusted value explained \nstep-6 section 15.5.1 using table .value \n\\(\\frac{x_{1}}{n} = \\frac{1}{150} = 0.0067\\) 0.001 0.01.\n\\(b_{\\text{cal}}\\)= 0.3952\\(y_{1}\\) = 0.443\\(y_{2}\\) = 0.528\\(L_{1}\\)= 0.40\\(L_{2}\\) = 0.50Adjusted b value =\n\\(0.443 + (0.3952 - 0.40)\\frac{\\left( 0.528 - 0.443 \\right)}{\\left( 0.50 - 0.40 \\right)}\\)\n= 0.43892.ComputedIf \\(b\\) value low indicates relatively high degree correlation among adjacent plots study area indicating gradual change soil fertility. \\(b\\) 0.43892, moderate, indication gradient slight indication patches.","code":""},{"path":"uniformity-trials.html","id":"maximum-curvature-method","chapter":"15 Uniformity trials","heading":"15.7.5 Maximum Curvature Method","text":"method (see section 15.6) used find optimum plot size. curve plotted taking plot size (terms basic units) X-axis CV values Y-axis.\nFigure 15.12: Relationship CV plot size\nFigure 15.12indicates plot size increases, coefficients variation decreases decrease maximum square shape plot 5m√ó5m. took small data set illustrative purpose, clearly seen 5m√ó5m plot lowest CV value also lowest variance per basic unit area.\nFigure 15.13: Relationship variance per basic unit area plot size\nFigure 15.13 shows relationship variance per basic unit area Vx plot size (x)","code":""},{"path":"analysis-of-variance-anova.html","id":"analysis-of-variance-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16 Analysis of Variance (ANOVA)","text":"Consider example three chemical fertilizers , B, C tested\npotted plants. Experimenter like identify, among \nfertilizer among , B C gives highest yield. Potted plants \nmaintained way; experimental conditions \nhomogenous. Now collecting yield data plants, can\nobserve variance .e., yield values . , \nvariation caused treatment experimental error. , \ntotal observed variance = variance due treatments + variance due \nerror. , example treatment error can considered \nsource variation data. Basically, experiment perform\nsample based make generalization population.\nNow, based experiment want test whether means \ntreatment , B C significantly different considering \nobserved difference chance taking account experimental\nerror variance.Analysis variance (ANOVA) statistical procedure used analyze\ndifferences among means, observed total variance \npartitioned components attributable different sources \nvariation. logic behind simple, much variation comes\ntreatment, likely mean treatments \ndifferent. variation compared experimental error\nvariance, larger ratio treatment variance error variance, \nlikely groups different means. term \"analysis \nvariance\" originates analysis uses variances determine\nwhether means different. ANOVA works comparing variance\ntreatments (group variance) error variance (within\ngroup variance).short ANOVA statistical hypothesis test determines whether\nmeans least two populations different. ANOVA developed\nstatistician Sir Ronald . Fisher.","code":""},{"path":"analysis-of-variance-anova.html","id":"null-hypothesis-in-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1 Null hypothesis in ANOVA","text":"Null hypothesis ANOVA population means equal, \ndenoted H0: ¬µ1 = ¬µ2 =¬µ3 = ‚Ä¶., = ¬µk . Alternate\nhypothesis atleast pair treatment equal, H1: ¬µi ‚â† ¬µj, ‚â† j; , j = 1,2, ‚Ä¶, k. ¬µ1, ¬µ2, ¬µ3, ‚Ä¶, ¬µk k population means","code":""},{"path":"analysis-of-variance-anova.html","id":"degrees-of-freedom","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.2 Degrees of freedom","text":"proceeding, important understand concept degrees\nfreedom. Degrees freedom can defined number \nindependent observations free vary.Consider simple example, 7 hats want wear \ndifferent hat every day week. first day, can wear \n7 hats. second day, can choose 6 remaining\nhats, day 3 can choose 5 hats, . day 6 rolls\naround, still choice 2 hats haven‚Äôt worn yet\nweek. choose hat day 6, choice\nhat wear Day 7. must wear one remaining hat.\n7-1 = 6 days ‚Äúhat‚Äù freedom‚Äîhat wore \nvary!Degrees freedom can also defined number independent\nvalues, included calculation estimate. estimate\nsingle number expresses property population \nsample. can mean, median, standard deviation, variance \ncalculated sample. independent values (\nobservations) went formula calculation. quantity \nvalues called ‚Äúdegrees freedom‚Äù.Consider three observations 6, x 9. x unknown suppose \nknow mean 6. can say x exactly equal 3, \nmean 6. two values free vary third value depends\ntwo constraint mean 6. .e.\ntwo values changed third value also change.Now consider height 7 students164, 173, 158, 179, 168, 187, 167.Mean: 170.85We can find standard deviation using two formulasSD= \\(\\frac{\\sum_{}^{}\\left( x_{} - \\overline{x} \\right)^{2}}{n}\\), \nn number observationsHere SD = 9SD= \\(\\frac{\\sum_{}^{}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1}\\),\nn-1 degrees freedomHere SD = 9.72It easy notice divide degrees freedom, make\nestimate standard deviation greater diving \nsample size. need make greater? ‚Äôve already\ncalculated mean, don‚Äôt use data order \nestimate standard deviation. depend piece \ninformation, last observation contribute \nstandard deviation. , don‚Äôt delete redundant data, \nunderestimate standard deviation population sample data. using degrees freedom denominator provides unbiased estimate population standard deviation.Degrees freedom also define probability distributions \ntest statistics various hypothesis tests. example, hypothesis\ntests use t-distribution, F-distribution, chi-square\ndistribution determine statistical significance. \nprobability distributions family distributions DF\ndefine shape. Hypothesis tests use distributions make\ndecisions null hypothesis.","code":""},{"path":"analysis-of-variance-anova.html","id":"mean-squares","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.3 Mean squares","text":"sum squares sum square difference \nobservation mean. Sum squares\n=\\(\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}\\). , \n=1, 2, ‚Ä¶, n, mean \\(\\overline{x} = \\frac{\\sum_{= 1}^{n}x_{}}{n}\\).\nSum squares generates measure variability. One-way ANOVA \ncalculate sum squares observation get measure total\nvariability along within group group sum squares,\ngives idea (error) group (\ntreatments) variability respectively. Mean sum squares mean\nsquares obtained dividing sum squares corresponding degrees\nfreedom. Mean square provides unbiassed estimate variance. \nexample, ANOVA error mean square (MSE) given \n\\(\\frac{\\text{group sum squares}}{\\text{error degrees freedom}}\\)\nprovides unbiased estimate error variance","code":""},{"path":"analysis-of-variance-anova.html","id":"test-statistic-f","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.4 Test Statistic F","text":"function sample values known statistic. example,\nsample mean, sample variance, sum sample values statistic,\nfunctions sample values. statistic \nused test hypothesis, known test statistic. Examples\ntest statistic t, F, œá2 etc. test statistic used ANOVA\nF.F-statistic ratio two variances named Sir\nRonald . Fisher. proved null hypothesis H0: ¬µ1 =\n¬µ2 =¬µ3 = ‚Ä¶., = ¬µk true ratio mean square treatment \nmean square error follows F distribution.F distribution right-skewed distribution. general, calculated\nF value ANOVA larger F critical value, can reject\nnull hypothesis.\nFigure 16.1: Distribution F different degrees freedom\n","code":""},{"path":"analysis-of-variance-anova.html","id":"assumptions-of-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.5 Assumptions of ANOVA","text":"Observations treatment group drawn randomly normally distributed populationObservations treatment group drawn randomly normally distributed populationAll populations observations drawn common variance :- Homogeneity varianceAll populations observations drawn common variance :- Homogeneity varianceAll samples drawn independently otherAll samples drawn independently otherTreatment environmental effects additiveTreatment environmental effects additiveAs result assumptions. experimental errors independent identically distributed (iid) Normal distribution mean 0 variance œÉ2. word can say \\(e_{}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"types-of-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6 Types of ANOVA","text":"Based model used sources variation studied ANOVA can classified following typesOne-way ANOVATwo-way ANOVAm-way ANOVA","code":""},{"path":"analysis-of-variance-anova.html","id":"one-way-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1 One-way ANOVA","text":"simplest type analysis variance known one way analysis variance, one source variation factor interest controlled effect elementary units observed. See example section 16.7.1 example one-way ANOVA can employed. one source variation assumed causing variance error field.","code":""},{"path":"analysis-of-variance-anova.html","id":"one--way-anova-model-oneway","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.1 One -way ANOVA model {oneway}","text":"\\[Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}}\\]\n= 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. \\(Y_{\\text{ij}}\\) observed value ith\ntreatment jth replication, \\(\\tau_{}\\) effect ith treatment. \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"one-way-classification","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.2 One-way classification","text":"\nFigure 16.2: One way classification Data\n","code":""},{"path":"analysis-of-variance-anova.html","id":"total-sum-of-squares-partitioning","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.3 Total sum of squares partitioning","text":"ANOVA consists partitioning total variation Yij values\nvariation due treatments variation caused uncontrolled\nfactors (error variation). Therefore, can write,Variance Yij =\n\\(\\frac{1}{n}\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - \\bar{Y} \\right)^{2}\\)n =\nv x r, overall mean \\(\\bar{Y} = \\frac{G}{n}\\)Total Sum Squares = \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - \\frac{G^{2}}{n}\\); \n\\(\\frac{G^{2}}{n}\\) known correction factor (C.F.).","code":""},{"path":"analysis-of-variance-anova.html","id":"proof","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.3.1 Proof","text":"= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - \\bar{Y} \\right)^{2}\\)\n= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}}^{2} - 2Y_{\\text{ij}}\\bar{Y} + {\\bar{Y}}^{2} \\right)\\)\n= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}}^{2} - 2n{\\bar{Y}}^{2} + n{\\bar{Y}}^{2} \\right)\\)= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}}^{2} - n{\\bar{Y}}^{2} \\right)\\)= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - n\\left( \\frac{\\sum_{= 1}^{v}\\sum_{j = 1} Y_{\\text{ij}}}{n} \\right)^{2},\\)\ndenote \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}} = G\\)\nTotal Sum Squares = \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - \\frac{G^{2}}{n}\\)Total Sum Squares partitioned Sum Squares due Treatments Sum Squares due Error.","code":""},{"path":"analysis-of-variance-anova.html","id":"proof-1","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.3.2 Proof","text":"\\[\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - \\overline{Y} \\right)^{2}} = \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} + {\\overline{Y}}_{} - \\overline{Y} \\right)^{2}}\\]\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( \\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)^{2} + \\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2} + 2\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)\\left( {\\overline{Y}}_{} - \\overline{Y} \\right) \\right)}\\]\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)^{2} +}}\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2} + 2\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)}}}}\\]Let\n\\(\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)^{2} = e_{\\text{ij}}^{2}\\),\n\\(\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}e_{\\text{ij}}} = 0\\)\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{{e_{\\text{ij}}}^{2} +}}\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2} + 2\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{e_{\\text{ij}}\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)}}}}\\]\\(\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{e_{\\text{ij}}\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)}} = 0\\),\nsince sum weighted residuals zero, equation\n\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{{e_{\\text{ij}}}^{2} +}}\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2}}\\]Total Sum Square = group sum square + group sum \nsquareTotal Sum Square = Error sum square + Treatment Sum Square","code":""},{"path":"analysis-of-variance-anova.html","id":"treatment-sum-of-square","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.4 Treatment sum of square","text":"Let us denote treatment totals T1, T2,¬†‚Ä¶.,TV .e \\({T}_{}{\\ =\\ }\\sum_{j = 1}^{r} Y_{\\text{ij}}\\) \n\\(T_{1}{\\ +\\ }{T}_{2}{\\ +\\ }\\text{T}_{3}{\\ +\\ }{T}_{n}\\text{ }\\text{‚Ä¶‚Ä¶}{+\\ }{T}_{v}{\\ =\\ }\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}} = G\\)Treatment Sum Squares, TSS =\n\\(\\frac{\\sum_{= 1}^{v}\\text{T}\\text{}^{2}}{r} - \\frac{G^{2}}{n}\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"oneway","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.5 Calculations for One-way ANOVA","text":"Total Sum Square (Total SS) = \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - \\text{C.F.}\\)\n(square observations, sum correction factor subtracted get total sum squares)Treatment Sum Squares (TSS) = \\(\\frac{\\sum_{= 1}^{v}\\text{T}\\text{}^{2}}{r} - \\text{C.F.}\\).\n(square treatment totals, sum correction factor subtracted get treatment sum squares )Error Sum Squares (ESS)= Total SS ‚Äì TSSCorrection factor (C.F.) = \\(\\frac{G^{2}}{n}\\), \\(G\\) = Grand Total observations.Mean sum squares sum squares divided corresponding\ndegrees freedoms (d.f.).Error degrees freedom = Total degrees freedom ‚Äì treatment degrees freedom.Error d.f.= \\(vr - 1 - \\left( v - 1 \\right) = vr - 1 - v + 1 = vr - v = v(r - 1)\\)\nFigure 16.3: One-way ANOVA table\nF ~ F (v-1),¬†v(r-1)MSE estimate error variance œÉ2 \n\\(\\sqrt{\\frac{\\text{MSE}}{r}}\\) standard error treatment mean.calculated F value ANOVA larger F critical value corresponding degrees freedoms critical value table, can reject null hypothesis. see section 19","code":""},{"path":"analysis-of-variance-anova.html","id":"example-of-one-way-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.1.6 Example of One-way ANOVA","text":"data tiller count transplanting recorded 4 hill sampling units fertilizer trial involving 6 treatments. Test significant difference treatments. treatments labeled T1, T2, T3, T4, T5 T6.\nData arranged shown :\nFigure 16.4: One-way classified data tiller count\nCalculation:\nequal number observations taken treatments, r1 = r2 = ‚Ä¶ = r6 = 4\nFigure 16.5: One-way ANOVA calculation\n\nFigure 16.6: One-way ANOVA table\ncan see calculated value greater table value F. null hypothesis rejected conclude least pair treatments significantly different 5% level significance.","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.2 Two-way ANOVA","text":"one way ANOVA explained previous section, treatments constitute different levels single factor controlled experiment. , however, many situations response variable interest may affected one factor. example,Milk yield cow may affected differences treatments .e., feeds fed well differences breed cows.Milk yield cow may affected differences treatments .e., feeds fed well differences breed cows.Moisture contents butter prepared churning cream may affected different levels fat churning speed etc.Moisture contents butter prepared churning cream may affected different levels fat churning speed etc.two independent factors might effect response variable interest. two factors considered source variation data addition experimental error. compare means situation use two-way ANOVA.two-way classification, data classified according two different criteria factors. procedure analysis variance somewhat different one followed earlier. One can also study interaction two factors, interaction effect included two-way ANOVA model.","code":""},{"path":"analysis-of-variance-anova.html","id":"twoway1","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.2.1 Two-way ANOVA model","text":"\\[Y_{\\text{ij}} = \\mu + \\tau_{} +  \\gamma_{j}+ e_{\\text{ij}}\\]\n= 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. \\(Y_{\\text{ij}}\\) observed value response ith level \nfactor jth level factor B, \\(\\tau_{}\\) effect ith factor,\\(\\gamma_{j}\\) effect jth factor . \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-classification","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.2.2 Two-way classification","text":"Consider experiment v levels factor 1 r levels factor 2. Observations recorded shown . considered situation one observation per cell.\nFigure 16.7: Two way classification Data\n","code":""},{"path":"analysis-of-variance-anova.html","id":"twoway","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.2.3 Calculations for Two-way ANOVA","text":"SSA = Take sum squares totals level factor divide number levels factor B ‚Äì Correction FactorSSA = Take sum squares totals level factor divide number levels factor B ‚Äì Correction FactorSSB = Take sum squares totals level factor B divide number levels factor ‚Äì Correction Factor.SSB = Take sum squares totals level factor B divide number levels factor ‚Äì Correction Factor.Total SS = Take sum squares observations ‚Äì Correction FactorTotal SS = Take sum squares observations ‚Äì Correction FactorError SS = Total SS ‚Äì SSA ‚Äì SSBError SS = Total SS ‚Äì SSA ‚Äì SSBError Degrees Freedom = Total D.F. ‚Äì Factor (D.F.) ‚Äì Factor B (D.F.)Error Degrees Freedom = Total D.F. ‚Äì Factor (D.F.) ‚Äì Factor B (D.F.)Error D.F.=\\(vr - 1 - \\left( v - 1 \\right) - \\left( r - 1 \\right) = vr - 1 - v + 1 - r + 1 = vr - v - r + 1 = (v - 1)(r - 1)\\)\nFigure 16.8: Two way ANOVA table\ntwo F values one factor one Factor B. FA ~ F (v-1),¬†(v-1)(r-1) FB ~ F ~(r-1),¬†(v-1)(r-1)Two-way ANOVA two null hypotheses one factor factor B. example Factor .\nH0: \\(\\mu\\)1 = \\(\\mu\\)2 =\\(\\mu\\)3 = ‚Ä¶., = \\(\\mu\\)v; \\(\\mu\\)population mean ith level factor .\nH1: atleast pair treatment means equal.\nFactor B\nH0: \\(\\beta\\)1 = \\(\\beta\\)2 =\\(\\beta\\)3 = ‚Ä¶., = \\(\\beta\\)r; \\(\\beta\\)j population mean jth level factor B.\nH1: atleast pair treatment means equal.Decisions null hypothesis made comparing corresponding F value table value explained section 19","code":""},{"path":"analysis-of-variance-anova.html","id":"example-of-two-way-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6.2.4 Example of Two-way ANOVA","text":"Four chemicals S1, S2, S3, S4 administered 5 cow breeds (, B, C, D, E). observations taken peak lactation period. Average yield period taken. significant difference chemicals? significant difference milk yield breeds? Considering chemicals breeds independent.\nFigure 16.9: Two way classification observations chemicals breeds\n\\(\\ n = 20,\\ v = 5\\ \\ r = 4\\), factor = cow breeds; factor B =\nchemicals, Grand total (G)=874Correction Factor, \\(CF = \\frac{G^{2}}{n} = \\frac{\\left( 874 \\right)^{2}}{20} = 38193.8\\)Total Sum squares,\\(\\text{TSS} = \\text{sum squares observations} - CF = \\ \\text{38288} - \\text{CF} = \\text{94.2}\\)Sum Squares Breeds (SSA)\\(= \\sum_{=1}^{v}\\frac{T_{}^{2}}{r} - \\text{CF}\\)\n\\(= \\frac{1}{4}\\left\\lbrack \\left( \\text{179} \\right)^{2} + \\text{...} + \\left( \\text{172} \\right)^{2} \\right\\rbrack - \\text{CF} = \\text{34.7}\\)Sum Squares \nchemicals (SSB)\n\\(= \\sum_{j=1}^{r}\\frac{B_{j}^{2}}{v} - \\text{CF} = \\frac{1}{5}\\left\\lbrack \\left( \\text{217} \\right)^{2} + \\text{...} + \\left( \\text{212} \\right)^{2} \\right\\rbrack - \\text{CF} = \\text{32.2}\\)Sum squares Error\\(\\text{SSE} = \\text{TSS} - (\\text{SST} + \\text{SSB}) = \\text{27.3}\\)\nFigure 16.10: ANOVA table chemicals breeds\nchemicals breeds significance difference 5% level. F values calculated greater table values.","code":""},{"path":"analysis-of-variance-anova.html","id":"models-under-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.7 Models under ANOVA","text":"Analysis variance based linear statistical model. linear model may eitherFixed effects modelFixed effects modelRandom effects modelRandom effects modelMixed effects modelMixed effects model","code":""},{"path":"analysis-of-variance-anova.html","id":"fix","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.7.1 Fixed effects model","text":"Fixed effects model statistical model model parameters\nfixed non-random quantities. using fixed effect model \nagricultural experiments, assume treatment effect unknown\nconstantsFixed effects model can explained using example. Consider \nexperiment experimenter wants know whether three fields \nimpact yield particular strain wheat. Observations \ntaken 12 plots fields (replication). AOV (Analysis \nVariance) model \\(Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}}\\); \\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)= 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. example\nt =3 r =12. \\(Y_{\\text{ij}}\\) observed value ith\nfield jth plot, \\(\\tau_{}\\) effect ith treatment,\nconsidered unknown constant. \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects, independently normally distributed mean 0\nconstant variance œÉ2. model, field ‚Äúfixed effect‚Äù.\nAlong \\(\\mu\\), fixed parameters \\(\\tau_{1}\\),\\(\\tau_{2}\\) \n\\(\\tau_{3}\\) quantities interest. Using model,\nexperimenter can make decisions treatment (field) tested.","code":""},{"path":"analysis-of-variance-anova.html","id":"random-effects-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.7.2 Random effects model","text":"random effects model, also called variance components model, \nstatistical model model parameters random variables.\nConsider example wheat fields, random effect model\nused , fields assumed randomly sampled \npopulation fields area AOV (Analysis Variance)\nmodel \\(Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}}\\);\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\) \\(\\tau_{}\\sim N(0,\\ \\sigma_{\\tau}^{2})\\)= 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. \\(\\tau_{}\\) \nconsidered random variable,\n\\(\\tau_{}\\sim N(0,\\ \\sigma_{\\tau}^{2})\\); \n\\(e_{ij}\\sim N(0,\\ \\sigma^{2})\\). model, field ‚Äúrandom\neffect‚Äù. statistical model describes whole ensemble possible\nrepetitions experiment region fields \nselected. Experimenter make generalisations fields \nparticular region based experiment. One important\nconsequence random effects responses (\\(Y_{\\text{ij}}\\)'s)\nlonger independent. random \\(\\tau_{}\\)'s induce correlations\namong responses. responses jointly multivariate normal\ndistribution.","code":""},{"path":"analysis-of-variance-anova.html","id":"mixed-effects-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.7.3 Mixed effects model","text":"mixed model, mixed-effects model mixed error-component model \nstatistical model containing fixed effects random effects.","code":""},{"path":"multiple-comparison-test.html","id":"multiple-comparison-test","chapter":"17 Multiple comparison test","heading":"17 Multiple comparison test","text":"ANOVA omnibus test. Omnibus test, general name, refers overall global test. Omnibus test means implemented overall hypothesis tends find whether significant difference treatment means (case ANOVA) .ANOVA doesn‚Äôt tell treatment pairs different; clearly rejecting null hypothesis ANOVA, conclude least pair treatment means different, didn‚Äôt give idea treatments different. order identify treatment means significantly different one need employ multiple comparison tests post-hoc tests.Multiple comparison tests also known post-hoc tests. significant omnibus F test ANOVA procedure, advance requirement conducting post-hoc comparison, otherwise comparisons required.Post hoc tests (Multiple Comparison tests) planned tests conducted obtaining significant F test ANOVA. several methods performing post hoc tests, areFisher‚Äôs LSD test (Least Significant Difference test)Fisher‚Äôs LSD test (Least Significant Difference test)DMRT (Duncan‚Äôs Multiple Range Test)DMRT (Duncan‚Äôs Multiple Range Test)Tukey‚Äôs testTukey‚Äôs testBonferroni methodBonferroni methodDunnett methodDunnett methodScheffe‚Äôs testScheffe‚Äôs testNewman-Keuls methodNewman-Keuls method","code":""},{"path":"multiple-comparison-test.html","id":"why-do-we-need-a-post-hoc-test.","chapter":"17 Multiple comparison test","heading":"17.1 Why do we need a post-hoc test.","text":"Type error occurs H0 statistically rejected even though actually true, whereas type II error refers false negative. comparing treatment pairs individually post-hoc test. example, consider situation three treatments , B C compared, may form following three pairs: versus B, \nversus C, B versus C. pair comparison called ‚Äòfamily.‚Äô type error occurs family compared called ‚Äòfamily-wise error‚Äô (FWE).(Lee Lee 2018)example, one performs pairwise test two given groups B 5% Œ± level significance observed significantly different, chance correct decision made (perform type error) 95%. another pairwise tests groups B C simultaneously non-significant result, real probability correct decision made B, B C 0.95 √ó 0.95 = 0.9025, 90.25% , consequently, testing Œ± error 1 ‚àí 0.9025 = 0.0975,\n0.05. time, statistical analysis groups C also non significant result, probability non-significance three pairs (families) 0.95 √ó 0.95 √ó 0.95\n= 0.857 actual testing Œ± error 1 ‚àí 0.857 = 0.143, 14%. now chance type error increased testing several treatment pairs together. Multiple comparison tests devised way correction incorporated control inflation \\(\\alpha\\), actual level significance remain \nprescribed rate (case 0.05).Inflated \\(\\alpha = 1 - \\left( 1 - \\alpha \\right)^{N}\\), \\(N\\) =\nnumber hypotheses testedThe inflation probability type error increases increase\nnumber comparisons.","code":""},{"path":"multiple-comparison-test.html","id":"lsd","chapter":"17 Multiple comparison test","heading":"17.2 Fisher‚Äôs LSD test","text":"first pairwise comparison technique developed Fisher 1935 called least significant difference (LSD) test. technique can used ANOVA F statistic significant. Fisher's least significant difference (LSD) procedure two-step\ntesting procedure pairwise comparisons several treatment groups. first step procedure, ANOVA performed. null hypothesis can rejected pre-specified level significance, ANOVA, second step procedure, pairwise comparisons treatment means performed.Steps Fisher‚Äôs LSD testANOVA performed null hypothesis rejected proceed \nstep 2ANOVA performed null hypothesis rejected proceed \nstep 2An LSD (Least Significant Difference) value calculated. \npair treatments, difference treatment means\ngreater LSD value, conclude treatment means \nsignificantly different.LSD (Least Significant Difference) value calculated. \npair treatments, difference treatment means\ngreater LSD value, conclude treatment means \nsignificantly different.Note among agricultural researchers India use term CD (Critical Difference) instead LSD, terms used interchangeably.","code":""},{"path":"multiple-comparison-test.html","id":"lsd-least-significant-difference","chapter":"17 Multiple comparison test","heading":"17.2.1 LSD (Least Significant Difference)","text":"Consider two treatments \\(T_{}\\) \\(T_{j}\\) replications \\(r_{}\\)\n\\(r_{j}\\). LSD CD given formula given \\[\\text{LSD} = \\ t_{\\frac{\\alpha}{2},edf}.SE(d)\\]\\[SE(d) = \\ \\sqrt{\\text{MSE}\\left( \\frac{1}{r_{}} + \\frac{1}{r_{j}} \\right)}\\], \\(\\text{MSE}\\) Mean Square Error ANOVA.\n\\(t_{\\frac{\\alpha}{2},edf}\\) critical value two tailed student‚Äôs t distribution \\(\\alpha\\) level significance error degrees \nfreedom.replications \\(r_{} = r_{j} = r\\), \n\\(\\text{SE}\\left( d \\right) = \\ \\sqrt{\\text{MSE}\\left( \\frac{1}{r} + \\frac{1}{r} \\right)}\\  = \\ \\sqrt{\\text{MSE}\\left( \\frac{2}{r} \\right)} = \\sqrt{\\frac{2\\ MSE}{r}}\\)\\[LSD = \\ t_{\\frac{\\alpha}{2},edf}.\\sqrt{\\frac{2\\ MSE}{r}}\\]difference means two treatments \\(T_{}\\) \\(T_{j}\\) greater LSD say \\(T_{}\\) \\(T_{j}\\) significantly different specified level significance (\\(\\alpha)\\)","code":""},{"path":"multiple-comparison-test.html","id":"logic-behind-lsd-test","chapter":"17 Multiple comparison test","heading":"17.2.2 Logic behind LSD test","text":"\\(\\overline{T_{}}\\) \\(\\overline{T_{j}}\\) treatment means obtained experiment. \\(\\mu_{}\\) \\(\\mu_{j}\\) corresponding unknown population mean treatments, null hypothesis pairwise comparison isH0: \\(\\mu_{} = \\ \\mu_{j}\\).e. \\(\\mu_{} - \\ \\mu_{j\\ } = 0\\)t-test can used test hypothesis\\[t = \\frac{T_{} - T_{j}}{SE(d)}\\]\\(100\\left( 1 - \\alpha \\right)\\%\\) confidence interval mean difference \\(T_{} - T_{j}\\) given \n\\(\\left( T_{} - T_{j} \\right) \\pm t_{\\frac{\\alpha}{2},edf}SE(d)\\). \ncan written \\(\\left( T_{} - T_{j} \\right) \\pm \\text{LSD}\\). , \n\\(\\left( T_{} - T_{j} \\right) > \\text{LSD}\\) \n\\(100\\left( 1 - \\alpha \\right)\\%\\) confidence interval include 0.\ncan reject null hypothesis, population mean treatments equal \\(100\\left( 1 - \\alpha \\right)\\%\\) confidence.","code":""},{"path":"multiple-comparison-test.html","id":"advantages-and-disadvantages-of-lsd.","chapter":"17 Multiple comparison test","heading":"17.2.3 Advantages and disadvantages of LSD.","text":"LSD test also known protected Fisher's LSD test. Protection means perform calculations described null hypothesis rejected based ANOVA. first step sort controls false positive rate (Type error) entire family \ncomparisons (Hayter 1986).protected Fisher's LSD test first post-hoc test ever developed, longer recommended correct \\(\\alpha\\) inflation multiple comparisons effectively compared post-hoc tests. Still LSD common post-hoc\ntest used agricultural research experiments.Fisher's LSD procedure known preserve experiment wise type error rate nominal level significance, number treatments just three. \\[\\@Meier\\]. , number treatments , recommended conduct DMRT Tukey‚Äôs test.Note multiple comparison tests (Bonferroni, Tukey, etc.) require ANOVA. results multiple comparisons tests valid even null hypothesis rejected based ANOVA","code":""},{"path":"single-factor-experiments.html","id":"single-factor-experiments","chapter":"18 Single factor experiments","heading":"18 Single factor experiments","text":"Experiments single factor varies others kept constant called single-factor experiments. experiments, treatments consist solely different levels single variable factor. factors maintained uniformly experimental units.Example:experiment find best nitrogen level getting high yield, 5 different nitrogen levels tested. nitrogen levels treatments factors like irrigation, light, fertility gradient assumed homogenous experimental units (plots experimental units).experiment find best nitrogen level getting high yield, 5 different nitrogen levels tested. nitrogen levels treatments factors like irrigation, light, fertility gradient assumed homogenous experimental units (plots experimental units).experiment find best feed formulation milk yield cows 4 feed formulas. feed formula factor varies 4 levels. factors like breed cow, age etc kept constant.experiment find best feed formulation milk yield cows 4 feed formulas. feed formula factor varies 4 levels. factors like breed cow, age etc kept constant.","code":""},{"path":"single-factor-experiments.html","id":"crd","chapter":"18 Single factor experiments","heading":"18.1 Completely Randomized Design (CRD)","text":"Completely Randomized Design basic single factor design. \ndesign treatments assigned completely random \nexperimental unit chance receiving one treatment.\nCRD appropriate experiments homogeneous experimental\nunits, laboratory green house experiments, \nenvironmental effects relatively easy control. field\nexperiments, generally large variation among experimental\nplots, CRD rarely used.CRD provides layout conducting experiments, experimental\nunits homogeneous. two basic principles design,\nrandomization 14.8.1 replication14.8.2 followed \nCRD. Local control14.8.3 required experimental\nunits homogeneous. Treatments can unequal replications CRD","code":""},{"path":"single-factor-experiments.html","id":"random1","chapter":"18 Single factor experiments","heading":"18.1.1 Randomization procedure","text":"Consider experiment involving four treatments , B, C, D CRD. treatment replicated 5 times. Randomization procedure explained .Determine total number experimental units (\\(n\\)) product number treatments (\\(t\\)) number replications (\\(r\\)); , \\(n = r\\  \\times t\\). example, \\(n = 5 \\times 4 = 20\\)Determine total number experimental units (\\(n\\)) product number treatments (\\(t\\)) number replications (\\(r\\)); , \\(n = r\\  \\times t\\). example, \\(n = 5 \\times 4 = 20\\)Assign unit number experimental units convenient manner; example, consecutively 1 n.¬†example, unit numbers 1, ... , 20 assigned 20 experimental units shown Figure 18.1Assign unit number experimental units convenient manner; example, consecutively 1 n.¬†example, unit numbers 1, ... , 20 assigned 20 experimental units shown Figure 18.1Assign treatments experimental plots using randomization schemes. explain simple randomization scheme section 18.1.1.1.Assign treatments experimental plots using randomization schemes. explain simple randomization scheme section 18.1.1.1.\nFigure 18.1: Numbered experimental units\n","code":""},{"path":"single-factor-experiments.html","id":"random","chapter":"18 Single factor experiments","heading":"18.1.1.1 Randomization by drawing lots","text":"Prepare \\(n\\) identical pieces paper divide \\(t\\) groups, group \\(r\\) pieces paper. Label piece paper group letter (number) corresponding treatment. Uniformly fold \\(n\\) labeled pieces paper, mix thoroughly, place container. example, 20 pieces paper, five treatments , B, C, D appearing .Prepare \\(n\\) identical pieces paper divide \\(t\\) groups, group \\(r\\) pieces paper. Label piece paper group letter (number) corresponding treatment. Uniformly fold \\(n\\) labeled pieces paper, mix thoroughly, place container. example, 20 pieces paper, five treatments , B, C, D appearing .Draw one piece paper time, without replacement constant shaking container draw mix content. example, label corresponding sequence piece paper drawn. sequence number can considered experimental unit number treatment assigned.Draw one piece paper time, without replacement constant shaking container draw mix content. example, label corresponding sequence piece paper drawn. sequence number can considered experimental unit number treatment assigned.\nFigure 18.2: Sequence labelled lots drawn\n","code":""},{"path":"single-factor-experiments.html","id":"layout-of-crd","chapter":"18 Single factor experiments","heading":"18.1.2 Layout of CRD","text":"allotting treatment according randomization procedure explained section 14.8.1. final layout look like figure 18.3.\nFigure 18.3: Layout Completely Randomized Design\n","code":""},{"path":"single-factor-experiments.html","id":"statistical-model-for-crd","chapter":"18 Single factor experiments","heading":"18.1.3 Statistical model for CRD","text":"Consider CRD v treatments r replications. Statistical model CRD one-way ANOVA (16.6.1.5).\\[Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}}\\]\n= 1,2, ‚Ä¶, v j =1,2, ‚Ä¶, ri. \\(Y_{\\text{ij}}\\) observed value ith\ntreatment jth replication, \\(\\tau_{}\\) effect ith treatment. \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"single-factor-experiments.html","id":"analysis-of-crd","chapter":"18 Single factor experiments","heading":"18.1.4 Analysis of CRD","text":"Analysis CRD explained section 16.6.1.5 one-way ANOVA.Note unequal replication (.e.¬†treatment can different replication) possible CRD. calculating TSS, treatment total square divided corresponding replication sum taken. replication treatments, calculate TSS take sum squares treatment total divide replication.\nFigure 18.4: ANOVA table CRD\n","code":""},{"path":"single-factor-experiments.html","id":"postcrd","chapter":"18 Single factor experiments","heading":"18.1.4.1 Post hoc test (LSD)","text":"null hypothesis rejected ANOVA; proceed post-hoc test explained section 17.2 . Fisher‚Äôs Least significant difference (LSD) test explained section 17.2 commonly employed test agricultural experiments.\nArrange treatment means descending order. Critical difference (CD) calculated treatment pair \\(T_{},\\ T_{j}\\) using following formula.\\[C.D. = t_{\\left( \\frac{\\alpha}{2},\\ \\ \\ error\\ df \\right)}\\sqrt{\\text{MSE}\\left( \\frac{1}{r_{}} + \\frac{1}{r_{j}} \\right)}\\]\\(t_{\\left( \\frac{\\alpha}{2},\\ \\ \\ error\\ df \\right)}\\) denotes critical value Student‚Äôs t two tailed test Œ± level significance error degrees freedom. \\(r_{}\\) \\(r_{j}\\) replications \\(T_{},\\ T_{j}\\) respectively. difference two treatment means greater\ncritical difference, significantly different. difference two treatment means less \\(\\text{C.D}\\). said par. (.e., statistically significant difference). treatments par given \nsymbols letters, treatment means without common letters symbols significantly different corresponding level significance (Œ±).","code":""},{"path":"single-factor-experiments.html","id":"advantages-of-a-crd","chapter":"18 Single factor experiments","heading":"18.1.5 Advantages of a CRD","text":"layout easy.layout easy.complete flexibility design .e. number \ntreatments replications treatment can tried.complete flexibility design .e. number \ntreatments replications treatment can tried.Whole experimental material can utilized design.Whole experimental material can utilized design.design yields maximum degrees freedom experimental\nerror.design yields maximum degrees freedom experimental\nerror.analysis data simplest compared design.analysis data simplest compared design.Even values missing analysis can done.Even values missing analysis can done.","code":""},{"path":"single-factor-experiments.html","id":"disadvantages-of-a-crd","chapter":"18 Single factor experiments","heading":"18.1.6 Disadvantages of a CRD","text":"suitable field experimentsNot suitable field experimentsRelatively low accuracy due lack local control. Completely homogeneous experimental units practically difficult many situations.Relatively low accuracy due lack local control. Completely homogeneous experimental units practically difficult many situations.","code":""},{"path":"single-factor-experiments.html","id":"replical","chapter":"18 Single factor experiments","heading":"18.1.7 Number of replications in CRD","text":"experimenter free choose number replications experiment. precision estimating experimental error variance increases number replications increases. design number replications optimum, since increasing replications increase experiment cost. , \nrule thumb says, error degrees freedom least 12 keep type II error check. One can easily find number replications required design using formula error degrees freedom design.","code":""},{"path":"single-factor-experiments.html","id":"example-10","chapter":"18 Single factor experiments","heading":"Example","text":"5 treatments; planning conduct experiment CRD. Find minimum number replications required.Formula error degrees freedom (edf) CRD \\(n - t\\); \\(n\\) total number experimental units \\(t\\) number treatments. \\(n = rt\\), \\(r\\) number replications.edf \\(\\geq\\) 12edf \\(= n - t = rt - t = t\\left( r - 1 \\right) \\geq 12\\)Since t = 5, edf \\(= 5\\left( r - 1 \\right) \\geq 12\\)\\(= r \\geq 3.4\\), Since \\(r\\) fraction \\(r =\\) 4The number replications required 4Why minimum error degrees freedom required design 12The error degrees freedom major role deciding critical value F. can see table F section 19 error degrees freedom degrees freedom denominator decreases F tends larger values, inflates type II error, .e. able reject null hypothesis considerable difference treatments (higher\nTreatment Sum Square). towards 12 degrees freedom denominator F values stabilize increase degrees freedom.","code":""},{"path":"single-factor-experiments.html","id":"example-11","chapter":"18 Single factor experiments","heading":"18.1.8 Example","text":"order find yielding abilities five varieties sesame experiment conducted green house using CRD four pots per variety. results given following table .\nFigure 18.5: ANOVA table CRD\n","code":""},{"path":"single-factor-experiments.html","id":"solution","chapter":"18 Single factor experiments","heading":"18.1.8.1 Solution","text":"Correction Factor:\\(= \\frac{G^{2}}{n} = \\frac{\\left( 397 \\right)^{2}}{20}\\)=7880.45Total Sum squares:\\(= \\left( 25^{2} + 21^{2} + .... + 11^{2} \\right) - CF\\)\\(= 8307 - CF = 426.55\\)Treatment Sum Squares:\\(\\frac{1}{4}(85^{2} + 102^{2} + ... + 53^{2}) - CF\\)\\(= 8211.75 - CF = 331.30\\)Error Sum Square:\\(426.55 - 331.30 = 95.25\\)\nFigure 18.6: ANOVA table CRD\nSince F value calculated greater table value F, can reject null hypothesis conclude significant difference atleast pair treatments. order find, treatments significantly different, perform Fisher‚Äôs LSD test.\\[\\text{CD} = t_{\\frac{\\alpha}{2}}\\sqrt{\\frac{\\text{2xMSE}}{r}} = \\text{2.131}\\sqrt{\\frac{2\\left( \\text{6.350} \\right)}{4}} = (\\text{2.131})(\\text{1.7819}) = \\text{3.797}\\text{2}\\]\\(t_{0.025\\text{ }15 \\text{ edf}} = 2.131\\) (Just look critical value \nt two tailed test Œ±= 0.05 statistical table. (see\nsection 19. shown \\(t_{\\frac{\\alpha}{2}}\\) , since\ntwo tailed test one side t-distribution curve \nprobability \\(\\frac{\\alpha}{2}\\), combined probability \nŒ±.)Treatment means arranged descending order. treatment pairs\nwhose difference means greater C.D. labelled \ndifferent letters.table can see difference mean V2 V1\n4 greater C.D. value. V2 V1 significantly\ndifferent; different letters assigned . Instead \nletters one can also use symbols shown . treatments \nletters/symbols significantly different., example can conclude V2 best variety ,\nhighest mean significantly different \nothers.Note: cases, one alphabets can appear \ntreatment mean, see hypothetical example .\nFigure 18.7: hypothetical example treatment grouping\nlettering like appears meaning treatments common alphabets significantly different. table V2\nV1 significantly different common\nletter . time can see V5 V4 significantly\ndifferent don‚Äôt common letters.","code":""},{"path":"single-factor-experiments.html","id":"randomized-complete-block-designs-rcbd","chapter":"18 Single factor experiments","heading":"18.2 Randomized Complete Block Designs (RCBD)","text":"field experiments, plots treatments applied may uniform may difference plot--plot fertility. cases CRD \nrecommended. one source constitutes difference plots can recommend randomized block design.randomized complete block (RCB) design one widely used experimental designs agricultural research. design especially suited field experiments number treatments large experimental area predictable fertility gradient.experimental material heterogeneous, experimental material grouped homogeneous sub-groups called blocks. block consists entire set treatments block equivalent replication. example, field known fertility gradient one direction, plots perpendicular gradient considered block. RCBD (Randomized Complete\nBlock Design) blocks equal size contains treatments. Since block contains treatments, term ‚Äòcomplete block design‚Äô used. See figure @ref(fig: field) @ref(fig: field2) see blocks formed field.RCBD number replications, \\(r\\) = number blocks\nNumber plots within block = number treatments, \\(v\\)","code":""},{"path":"single-factor-experiments.html","id":"blocking-technique","chapter":"18 Single factor experiments","heading":"18.2.1 Blocking technique","text":"Blocking local control, one basic principles design effectively implemented RCBD. primary purpose blocking reduce experimental error eliminating contribution known sources variation among experimental units. achieved grouping homogeneous units together, variability blocks minimized, blocks maximized.Two important factors considered blockingThe selection source variability used basis blocking, field experiments usually fertility gradient productivity gradient used identified source variability.selection source variability used basis blocking, field experiments usually fertility gradient productivity gradient used identified source variability.Proper selection block shape orientation. example, gradient unidirectional use long narrow blocks. Use square blocks strong fertility gradient directions. experimenter use common sense identify shape orientation block examining field conditions, suc plots block homogeneous.Proper selection block shape orientation. example, gradient unidirectional use long narrow blocks. Use square blocks strong fertility gradient directions. experimenter use common sense identify shape orientation block examining field conditions, suc plots block homogeneous.","code":""},{"path":"single-factor-experiments.html","id":"rcbdrandom","chapter":"18 Single factor experiments","heading":"18.2.2 Randomization in RCBD","text":"Consider experiment involving four treatments , B, C, D RCBD. treatment replicated 5 times. Randomization procedure explained . randomization process RCB design applied separately independently blocks.STEP 1. Divide experimental area \\(r\\) equal blocks, \\(r\\) number replications. example \\(r =\\) 5.\nFigure 18.8: Division experimental field five blocks, consisting four plots, RCBD four treatments five replications. Blocking done blocks rectangular perpendicular direction unidirectional fertility gradient\nSTEP 2. Subdivide block \\(v\\) experimental plots, \\(v\\) number treatments. Number \\(v\\) plots consecutively 1 \\(v\\), assign \\(v\\) treatments random \\(v\\) plots following randomization scheme CRD described Section 18.1.1.1. example 5 blocks divided 4 plots. Plots numbered 1 4 block. Now block, paper lots 1 4 prepared. Treatment names written order plot number lot recorded order shown .STEP 3. treatments allotted randomly selected plots. Step 2 repeated blocks.\nFigure 18.9: Allotment treatments blocks. RCBD block treatments treatment appear block exactly .\n","code":""},{"path":"single-factor-experiments.html","id":"layout-of-rcbd","chapter":"18 Single factor experiments","heading":"18.2.3 Layout of RCBD","text":"RCBD block treatments treatment appear block exactly . allotting treatment according randomization procedure explained section 18.2.2. final layout look like figure 18.10.Note:\nRBD every treatment number replications. (treatments equally replicated say, \\(r\\) times). need total \\(v\\) \\(\\times\\) \\(r\\) plots conducting experiment. \\(r\\) blocks containing \\(v\\) plots. design, number blocks = number replication treatments = \\(r\\)\nnumber plots within block = number treatments = \\(v\\)\nFigure 18.10: Layout RCBD four treatments five replications\n","code":""},{"path":"single-factor-experiments.html","id":"statistical-model-for-rcbd","chapter":"18 Single factor experiments","heading":"18.2.4 Statistical Model for RCBD","text":"Consider RCBD ¬†\\(v\\) treatments ¬†\\(r\\)¬†replications (Blocks). Statistical model RCBD two-way ANOVA¬†described section 16.6.2.1.\\[Y_{\\text{ij}} = \\mu + \\tau_{} +  \\gamma_{j}+ e_{\\text{ij}}\\]\n= 1,2, ‚Ä¶, v j =1,2, ‚Ä¶, r. \\(Y_{\\text{ij}}\\) observed value response ith treatment jth block, \\(\\tau_{}\\) effect ith factor,\\(\\gamma_{j}\\) effect jth block . \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"single-factor-experiments.html","id":"analysis-of-rcbd","chapter":"18 Single factor experiments","heading":"18.2.5 Analysis of RCBD","text":"Analysis RCBD explained section 16.6.2.3 two-way ANOVA.Observation \\(v\\) treatments \\(r\\) blocks arranged shown .\nFigure 18.11: Two-way arrangement observations RCBD\n\nFigure 18.12: ANOVA table RCBD\nF1 ~ F (\\(v\\)‚Äì1),¬†(\\(v\\)‚Äì1)(\\(r\\)‚Äì1) F2 ~ F ~(\\(r\\)‚Äì1),¬†(\\(v\\)‚Äì1)(\\(r\\)‚Äì1) two F values one Treatment one blocks respectively. Decisions null hypothesis made comparing corresponding F value table value explained section 19.F2 significant\nF2 significant, indicates significant difference blocks, means blocking effective. Blocking considered effective reducing experimental error F2 significant.","code":""},{"path":"single-factor-experiments.html","id":"number-of-replications-in-rcbd","chapter":"18 Single factor experiments","heading":"18.2.6 Number of replications in RCBD","text":"explained section 18.1.7 number replications RCBD found using formula error degrees freedom (edf).","code":""},{"path":"single-factor-experiments.html","id":"example-12","chapter":"18 Single factor experiments","heading":"Example","text":"4 treatments; planning conduct experiment RCBD. Find minimum number replications required.Formula error degrees freedom (edf) RCBD \\((v - 1) (r-1)\\); \\(v\\) number treatments \\(r\\) number blocks.edf \\(\\geq\\) 12edf \\(=(v - 1) (r-1) \\geq 12\\)Since \\(v\\) = 4, edf \\(= 3\\left( r - 1 \\right) \\geq 12\\)\\(\\implies r \\geq 5\\), minimum replication required 4The number replications required 4","code":""},{"path":"single-factor-experiments.html","id":"post-hoc-test-in-rcbd","chapter":"18 Single factor experiments","heading":"18.2.7 Post-hoc test in RCBD","text":"null hypothesis treatments rejected ANOVA; proceed post-hoc test explained section 17.2. Arrange treatment means descending order. Critical difference (CD) LSD (Least Significant Difference) calculated treatment pair \\(T_{},\\ T_{j}\\) using following formula.\\[CD = \\ t_{\\frac{\\alpha}{2},edf}.\\sqrt{\\frac{2\\ MSE}{r}}\\]\n, \\(\\text{MSE}\\) Mean Square Error ANOVA.\n\\(t_{\\frac{\\alpha}{2},edf}\\) critical value two tailed student‚Äôs t distribution \\(\\alpha\\) level significance error degrees \nfreedom.\\(r\\) number blocks/replication.\nProcedure grouping using symbols CRD explained section 18.1.4.1.","code":""},{"path":"single-factor-experiments.html","id":"advantages-of-rcbd","chapter":"18 Single factor experiments","heading":"18.2.8 Advantages of RCBD","text":"Accuracy: RCB design shown efficient accurate C.R.D. types experimental work.Accuracy: RCB design shown efficient accurate C.R.D. types experimental work.Flexibility: RCBD restrictions placed number treatments \nnumber replicates.Flexibility: RCBD restrictions placed number treatments \nnumber replicates.Even values missing, still analysis can done using missing plot\ntechniqueEven values missing, still analysis can done using missing plot\ntechniqueEase analysis: Statistical analysis simple, rapid straight forward.Ease analysis: Statistical analysis simple, rapid straight forward.","code":""},{"path":"single-factor-experiments.html","id":"disadvantages-of-rcbd","chapter":"18 Single factor experiments","heading":"18.2.9 Disadvantages of RCBD","text":"RCBD may give misleading results blocks homogeneousRCBD may give misleading results blocks homogeneousRCBD suitable large number treatments case block size increase may possible keep large blocks\nhomogeneous.RCBD suitable large number treatments case block size increase may possible keep large blocks\nhomogeneous.data two plots missing statistical analysis becomes\ntedious complicated.data two plots missing statistical analysis becomes\ntedious complicated.","code":""},{"path":"ftable.html","id":"ftable","chapter":"19 How to use F table","heading":"19 How to use F table","text":"table gives model critical values table F (also known statistical table F) \\(\\alpha\\) = 0.05 level significance. Critical values F can generated now days using softwares (Microsoft Excel, R)Obtain F-ratio ANOVA using formula.(x,y) degrees freedom associated . example, ANOVA treatment degrees freedom (x) error degrees freedom (y)Go along x columns, y rows. point intersection critical F-ratio.obtained value F equal larger critical F-value, can reject null hypothesis stating population means equal 100(1-\\(\\alpha\\))%, 100(1-0.05) % = 95% confidence.example: obtain F ratio 3.26 (2, 24) degrees freedom. Go along 2 columns 24 rows. critical value F 3.40. obtained F-ratio less , conclude enough evidence reject null hypothesis.\nFigure 19.1: model critical value table F alpha=0.05\nClick download full statistical table","code":""},{"path":"ftable.html","id":"generate-critical-value-of-f-using-excel","chapter":"19 How to use F table","heading":"19.1 Generate critical value of F using excel","text":"find F critical value Excel, can use F.INV.RT() function, uses following syntax:= F.INV.RT(probability, deg_freedom1, deg_freedom2).example F(2, 24) \\(\\alpha\\)=0.05, Excel function \n> = F.INV.RT(0.05, 2, 24).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
