[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"\n\n\nbook collection lecture notes covering syllabus statistics course STATISTICAL METHODS & APPLICATIONS (STAT 3202) B.Sc.(Hons.) Agriculture Kerala Agricultural University\n","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\n\nNote: book published MeLoN (Module e-Learning & Online Notes) . online version book free read .\n\nfeedback, please feel free contact Dr.Pratheesh P. Gopinath. E-mail: pratheesh.pg@kau.Thank !\n","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"chapter introduction, definition statistics, collection classification data, formation frequency distribution.(Goon Dasgupta 1983) (Gupta Kapoor 1997)","code":""},{"path":"introduction.html","id":"origin-of-the-word-statistics","chapter":"1 Introduction","heading":"1.1 Origin of the word ‚ÄúStatistics‚Äù","text":"Term statistics derived Neo-Latin word\nstatisticum collegium meaning ‚Äúcouncil state‚Äù Italian word\nstatista meaning ‚Äústatesman‚Äù ‚Äúpolitician‚Äù.German word Statistik, got meaning ‚Äúcollection \nclassification data‚Äù generally early 19th century. word\nfirst introduced Gottfried Achenwall (1749). Statistik \noriginally designated term analysis data state\n(data used government administrative bodies). term\nStatistik introduced English 1791 Sir John Sinclair\npublished first 21 volumes titled ‚ÄúStatistical Account \nScotland‚Äù (Ball 2004). first book ‚ÄòStatistics‚Äô title \n‚ÄúContributions Vital Statistics‚Äù (1845) Francis GP Neison,\nactuary1 Medical Invalid General Life Office.\nFigure 1.1: Statistical Account Scotland Sir John Sinclair (1791)\n","code":""},{"path":"introduction.html","id":"statistics-and-mathematics","chapter":"1 Introduction","heading":"1.2 Statistics and Mathematics","text":"Mathematics follows rigid theorem proof. Mathematical theories\ninvolve well-defined proven facts minimal scope \nchange. However, Statistics discipline real-life data \nhandled. factor makes field study abstract, \nindividuals develop newer solutions problems new \nobserved . Statistics applied science; mathematics\ngoal prove theorems. statistics, main goal \ndevelop good methods understanding data making decisions.\nStatisticians often use mathematical theorems justify methods,\ntheorems main focus. Statistics now considered \nindependent field uses mathematics solve real life problems.","code":""},{"path":"introduction.html","id":"definition-of-statistics","chapter":"1 Introduction","heading":"1.3 Definition of Statistics","text":"Statistics science deals theCollection dataCollection dataOrganization data Classification dataOrganization data Classification dataPresentation dataPresentation dataAnalysis dataAnalysis dataInterpretation dataInterpretation dataTwo main branches statistics :Descriptive statistics, deals summarizing data \nsample using indexes mean standard deviation etc.Inferential statistics, use random sample data taken \npopulation describe make inferences population\nparameters.","code":""},{"path":"introduction.html","id":"data","chapter":"1 Introduction","heading":"1.4 Data","text":"Data can defined individual pieces factual information recorded\nused purpose analysis. raw information \ninferences drawn using science ‚ÄúSTATISTICS‚Äù.Example dataNo.¬†farmers village..¬†farmers village.rainfall period time.rainfall period time.Area paddy crop state.Area paddy crop state.","code":""},{"path":"introduction.html","id":"use-and-limitations-of-statistics","chapter":"1 Introduction","heading":"1.5 Use and limitations of statistics","text":"Functions statistics: Statistics simplifies complexity, presents\nfacts definite form, helps formulation suitable policies,\nfacilitates comparison helps forecasting. Valid results \nconclusion obtained research experiments using proper statistical\ntools.Uses statistics: Statistics pervaded almost spheres \nhuman activities. Statistics useful administration, industry,\nbusiness, economics, research workers, banking,insurance companies etc.Limitations StatisticsStatistical theories can applied variability\nexperimental material.Statistical theories can applied variability\nexperimental material.Statistics deals aggregates groups \nindividual objects.Statistics deals aggregates groups \nindividual objects.Statistical results exact.Statistical results exact.Statistics often misused.Statistics often misused.","code":""},{"path":"introduction.html","id":"population-and-sample","chapter":"1 Introduction","heading":"1.6 Population and Sample","text":"Consider following example. Suppose wish study height \nstudents college. take us long time measure \nheight students college may select 20 \nstudents measure height (cm). Suppose obtain \nmeasurements like this149 156 148 161 159 143 158 152 164 171 157 152 163 158 151 147 157 146\n153 159In study, interested height students \ncollege. set height students college called \npopulation study. set 20 height, H = {149, 156,148,\n‚Ä¶, 153, 159}, sample population.","code":""},{"path":"introduction.html","id":"population","chapter":"1 Introduction","heading":"1.6.1 Population","text":"population set objects wish study","code":""},{"path":"introduction.html","id":"sample","chapter":"1 Introduction","heading":"1.6.2 Sample","text":"sample part population study learn \npopulation.","code":""},{"path":"introduction.html","id":"variables-and-constants","chapter":"1 Introduction","heading":"1.7 Variables and constants","text":"","code":""},{"path":"introduction.html","id":"variables","chapter":"1 Introduction","heading":"1.7.1 Variables","text":"type observation can take different values different\npeople, different values different times, places, called \nvariable. following examples variables:number fruits branch, number plots field, number \nschools country, etc.number fruits branch, number plots field, number \nschools country, etc.plant height, yield, panicle length, temperature,etc.plant height, yield, panicle length, temperature,etc.Broadly speaking, two types variables ‚Äì quantitative\nqualitative (categorical) variables.","code":""},{"path":"introduction.html","id":"constants","chapter":"1 Introduction","heading":"1.7.2 Constants","text":"Constants characteristics values change.\nExamples constants : pi (ùùÖ) = ratio circumference \ncircle diameter (ùùÖ = 3.14159...) e, base \nnatural (Napierian) logarithms (e=2.71828).","code":""},{"path":"introduction.html","id":"types-of-variables","chapter":"1 Introduction","heading":"1.8 Types of variables","text":"","code":""},{"path":"introduction.html","id":"quantitative-variables","chapter":"1 Introduction","heading":"1.8.1 Quantitative variables","text":"quantitative variable one can take numerical values. \nvariables like number fruits branch, number plots field,\nnumber schools country, plant height, yield, panicle length,\ntemperature, etc. examples quantitative variables. Quantitative\nvariables may characterized whether discrete\ncontinuous","code":""},{"path":"introduction.html","id":"discrete-variables","chapter":"1 Introduction","heading":"1.8.1.1 Discrete variables","text":"variables like number fruits branch, number plots \nfield, number schools country, etc. can counted. \nexamples discrete variables. Variables can take finite\nnumber values called ‚Äúdiscrete variables.‚Äù variable phrased\n‚Äúnumber ‚Ä¶‚Äù, discrete, possible list \npossible values {0,1, ‚Ä¶}. variable finite number \npossible values discrete. following example illustrates \npoint. number daily admissions hospital discrete\nvariable since can represented whole number, 0, 1, 2\n3. number daily admissions given day number\n1.8, 3.96 5.33.","code":""},{"path":"introduction.html","id":"continuous-variables","chapter":"1 Introduction","heading":"1.8.1.2 Continuous variables","text":"variables like plant height, yield, panicle length, temperature,\netc. can measured. examples continuous variables. \ncontinuous variable possess gaps interruptions\ncharacteristic discrete variable. continuous variable can assume\nvalue within specific relevant interval values assumed \nvariable. Notice age continuous since individual age\ndiscrete jumps. Panicle length can measured 5.5, 5.8 cm etc ,\ncontinuous variable.","code":""},{"path":"introduction.html","id":"categorical-variables","chapter":"1 Introduction","heading":"1.8.2 Categorical variables","text":"variable called categorical measurement scale set \ncategories. example, marital status, categories\n(single,married, widowed), categorical. Whether employed (yes, ),\nreligious affiliation (Protestant, Catholic, Jewish, Muslim, others,\nnone), colours etc. Categorical variables often called qualitative.\ncan seen categorical variables can neither measured \ncounted.","code":""},{"path":"introduction.html","id":"measurement-scales","chapter":"1 Introduction","heading":"1.9 Measurement scales","text":"Variables can classified according following four\nlevels measurement: nominal, ordinal, interval ratio.","code":""},{"path":"introduction.html","id":"nominal-scale","chapter":"1 Introduction","heading":"1.9.1 Nominal scale","text":"scale measure applies qualitative variables . \nnominal scale, order required. example,gender nominal,\nblood group nominal, marital status also nominal. \nperform arithmetic operations data measured nominal scale.","code":""},{"path":"introduction.html","id":"ordinal-scale","chapter":"1 Introduction","heading":"1.9.2 Ordinal scale","text":"scale also applies qualitative data. ordinal scale, order\nnecessary. means one category lower next one \nvice versa. example, Grades ordinal, excellent higher \ngood, turn higher good, . \nnoted , ordinal scale, differences category values\nmeaning.","code":""},{"path":"introduction.html","id":"interval-scale","chapter":"1 Introduction","heading":"1.9.3 Interval scale","text":"scale measurement applies quantitative data . \nscale, zero point indicate total absence quantity\nmeasured. example scale temperature Celsius\nFahrenheit scale. Suppose minimum temperatures 3 cities, , B\nC, particular day 00C, 200C 100C, respectively.\nclear can find differences temperatures.\nexample, city B 200C hotter city . However, say\ncity temperature. Moreover, say city B \ntwice hot city C, just city B 200C city C \n100C. reason , interval scale, ratio two\nnumbers meaningful.","code":""},{"path":"introduction.html","id":"ratio-scale","chapter":"1 Introduction","heading":"1.9.4 Ratio scale","text":"scale measurement also applies quantitative data \nproperties interval scale. addition \nproperties, ratio scale meaningful zero starting point \nmeaningful ratio 2 numbers. example variables measured \nratio scale, weight. weighing scale reads 0 kg gives \nindication absolutely weight . zero\nstarting point meaningful. Ram weighs 60 kg Laxman weighs 30\nkg, Ram weighs twice Laxman. Another example variable\nmeasured ratio scale temperature measured Kelvin scale.\ntrue zero point.\nFigure 1.2: Classification variables\n","code":""},{"path":"introduction.html","id":"collection-of-data","chapter":"1 Introduction","heading":"1.10 Collection of Data","text":"first step enquiry (investigation) collection data.\ndata may collected whole population sample .\nmostly collected sample basis. Collecting data \ndifficult job. enumerator investigator well trained\nindividual collects statistical data. respondents \npersons information collected.","code":""},{"path":"introduction.html","id":"types-of-data","chapter":"1 Introduction","heading":"1.10.1 Types of Data","text":"two types (sources) collection data:Primary DataSecondary Data","code":""},{"path":"introduction.html","id":"primary-data","chapter":"1 Introduction","heading":"1.10.1.1 Primary Data","text":"Primary data first hand information collected, compiled\npublished organizations purpose. \noriginal data character undergone sort \nstatistical treatment.Example: Population census reports primary data \ncollected, complied published population census organization.","code":""},{"path":"introduction.html","id":"secondary-data","chapter":"1 Introduction","heading":"1.10.1.2 Secondary Data","text":"secondary data second hand information already\ncollected organization purpose available \npresent study. Secondary data pure character \nundergone treatment least .Example: economic survey England secondary data \ndata collected one organization like Bureau \nStatistics, Board Revenue, banks, etc.","code":""},{"path":"introduction.html","id":"methods-of-collecting-primary-data","chapter":"1 Introduction","heading":"1.11 Methods of Collecting Primary Data","text":"Primary data collected using following methods:","code":""},{"path":"introduction.html","id":"personal-investigation","chapter":"1 Introduction","heading":"1.11.1 Personal Investigation","text":"researcher conducts survey /collects data \n. data collected way usually accurate reliable.\nmethod collecting data applicable case small\nresearch projects.","code":""},{"path":"introduction.html","id":"through-investigation","chapter":"1 Introduction","heading":"1.11.2 Through Investigation","text":"Trained investigators employed collect data. \ninvestigators contact individuals fill questionnaires \nasking required information. organizations utilize \nmethod.","code":""},{"path":"introduction.html","id":"collection-through-questionnaire","chapter":"1 Introduction","heading":"1.11.3 Collection through Questionnaire","text":"Researchers get data local representations agents \nbased upon experience. method quick gives \nrough estimate.","code":""},{"path":"introduction.html","id":"through-the-telephone","chapter":"1 Introduction","heading":"1.11.4 Through the Telephone","text":"Researchers get information individuals telephone. \nmethod quick gives accurate information.","code":""},{"path":"introduction.html","id":"methods-of-collecting-secondary-data","chapter":"1 Introduction","heading":"1.12 Methods of Collecting Secondary Data","text":"Secondary data collected following methods:","code":""},{"path":"introduction.html","id":"official","chapter":"1 Introduction","heading":"1.12.1 Official","text":"Publications Statistical Division, Ministry Finance, \nFederal Bureaus Statistics, Ministries Food, Agriculture,\nIndustry, Labor, etc.","code":""},{"path":"introduction.html","id":"semi-official","chapter":"1 Introduction","heading":"1.12.2 Semi-Official","text":"Publications State Bank, Railway Board, Central Cotton\nCommittee, Boards Economic Enquiry etc.Publication Trade Associations, Chambers Commerce, etc.Technical Trade Journals Newspapers.Research Organizations universities institutions.","code":""},{"path":"introduction.html","id":"difference-between-primary-and-secondary-data","chapter":"1 Introduction","heading":"1.13 Difference Between Primary and Secondary Data","text":"difference primary secondary data change \nhand. Primary data first hand information directly\ncollected form one source. original character \nundergone sort statistical treatment, secondary\ndata obtained sources agencies. pure \ncharacter undergone treatment least .","code":""},{"path":"introduction.html","id":"frequency-distribution","chapter":"1 Introduction","heading":"1.14 Frequency distribution","text":"Table shows number fruits per branch mango\ntree selected particular plot. data, presented form\ncollected, called raw data.\nFigure 1.3: Raw data set . fruits per branch mango tree\ncan seen , minimum maximum numbers fruits per branch 0 5, respectively. Apart numbers, impossible, without careful study, extract exact information data. breaking data form \nFigure 1.4: Frequency distribution table\nNow certain features data become apparent. instance, can easily seen , branches selected four fruits number branches 4 fruits 7. information easily obtained raw data. table called frequency table frequency distribution. called gives frequency number times observation occurs. Thus, \nfinding frequency observation, intelligible picture obtained.","code":""},{"path":"introduction.html","id":"construction-of-frequency-distribution","chapter":"1 Introduction","heading":"1.14.1 Construction of frequency distribution","text":"List values variable ascending order magnitude.List values variable ascending order magnitude.Form tally column, , value data, record \nstroke tally column next value. tally, \nfifth stroke made across first four. makes easy \ncount entries enter frequency observation.Form tally column, , value data, record \nstroke tally column next value. tally, \nfifth stroke made across first four. makes easy \ncount entries enter frequency observation.Check frequencies sum total number observationsCheck frequencies sum total number observations","code":""},{"path":"introduction.html","id":"grouped-frequency-distribution","chapter":"1 Introduction","heading":"1.15 Grouped frequency distribution","text":"Data gives plant height 20 paddy varieties, measured \nnearest centimeters.\nFigure 1.5: Plant height 20 paddy varieties\ncan seen minimum maximum plant height 107 cm\n144 cm, respectively. frequency distribution giving every plant height 107 cm 144 cm long \ninformative. problem overcome grouping data \nclasses.\nchoose classes\n100 ‚Äì 109\n110 ‚Äì 119\n120 ‚Äì 129\n130 ‚Äì 139\n140 ‚Äì 149\nobtain frequency distribution given :\nFigure 1.6: Grouped Frequency distribution table\ntable gives frequency group class; therefore\ncalled grouped frequency table grouped frequency distribution.\nUsing grouped frequency distribution, easier obtain\ninformation data using raw data. instance, can\nseen 14 20 paddy varieties plant height 110\ncm 139 cm (inclusive). information easily \nobtained raw data.\nnoted , even though table concise, \ninformation lost. example, grouped frequency distribution\ngive us exact plant height paddy varieties. Thus \nindividual plant height paddy varieties lost effort \nobtain overall picture.","code":""},{"path":"introduction.html","id":"terms-used-in-grouped-frequency-tables.","chapter":"1 Introduction","heading":"1.16 Terms used in grouped frequency tables.","text":"","code":""},{"path":"introduction.html","id":"class-limits","chapter":"1 Introduction","heading":"1.16.1 Class limits","text":"intervals observations put called class\nintervals. end points class intervals called\nclass limits. example, class interval 100 ‚Äì 109,\nlower class limit 100 upper class limit 109.","code":""},{"path":"introduction.html","id":"class-boundaries","chapter":"1 Introduction","heading":"1.16.2 Class boundaries","text":"raw data example recorded nearest\ncentimeters. Thus, plant height 109.5cm recorded \n110cm, plant height 119.4 cm recorded 119cm,\nplant height 119.5 cm recorded 120 cm. \ncan therefore seen , class interval 110 ‚Äì 119, consists \nmeasurements greater equal 109.5 cm less 119.5 cm.\nnumbers 109.5 119.5 called lower upper boundaries \nclass interval 110 ‚Äì 120. class boundaries class\nintervals given :\nFigure 1.7: Class boundary class limits\nNote:\nNotice lower class boundary ith class interval \nmean lower class limit class interval upper class\nlimit (-1)th class interval (= 2, 3, 4, ‚Ä¶). example,\ntable lower class boundaries second \nfourth class intervals (110 + 119) /2 = 114.5 (130 + 139)/2 =\n134.5 respectively.\ncan also seen upper class boundary ith class\ninterval mean upper class limit class interval \nlower class limit (+1)th class interval (= 1, 2, 3,\n‚Ä¶). Thus, table upper class boundary fourth\nclass interval (130 + 139)/2 = 134.5.","code":""},{"path":"introduction.html","id":"class-mark","chapter":"1 Introduction","heading":"1.16.3 Class mark","text":"mid-point class interval called class mark class\nmid-point class interval. average upper \nlower class limits class interval. also average \nupper lower class boundaries class interval. example, \ntable, class mark third class interval found \nfollows: class mark =(120+129)/2 = (119.5 + 129.5)/2= 124.5.","code":""},{"path":"introduction.html","id":"class-width","chapter":"1 Introduction","heading":"1.16.4 Class width","text":"difference upper lower class boundaries class\ninterval called class width class interval. Class widths\nclass intervals can also found subtracting two consecutive\nlower class limits, subtracting two consecutive upper class\nlimits.Note:width ith class interval numerical difference\nupper class limits ith ( -1)th class\nintervals (= 2, 3, ‚Ä¶). also numerical difference \nlower class limits ith (+1)th class intervals (\n= 1, 2, ‚Ä¶).grouped frequency table width second class interval\n|110-119| = 9. numerical difference lower\nclass limits second third class intervals. width \nthird class interval |120-129|= 9. numerical\ndifference lower class limits third fourth class intervals.","code":""},{"path":"introduction.html","id":"construction-of-frequency-distribution-table","chapter":"1 Introduction","heading":"1.17 Construction of frequency distribution table","text":"Step 1. Decide many classes wish use.Step 2. Determine class widthStep 3. Set individual class limitsStep 4. Tally items classesStep 5. Count number items classConsider example\nagricultural student measured lengths leaves oak tree\n(nearest cm). Measurements 38 leaves follows\n9,16,13,7,8,4,18,10,17,18,9,12,5,9,9,16,1,8,17,1,10,5,9,11,15,6,14,9,1,12,5,16,4,16,8,15,14,17Step 1. Decide many classes wish use.H.. Sturges provides formula determining approximation number\nclasses. \\[\\mathbf{k = 1 + 3.322}\\mathbf{\\log}\\mathbf{N}\\] Number \nclasses greater calculated k\nexample N=38, k= (1+3.322)√ólog(38) = (1+3.322)√ó1.5797 =\n6.24 = approx 7So approximated number classes less 6.24\n.e.\\(\\ k^{'}\\) =7Step 2. Determine class widthGenerally, class width size classes. C=\n| max ‚àí min|/ k. Class width \\(C^{'}\\)greater calculated\nC. example, C = | 18‚àí 1|/6.24 = 2.72, \napproximately class width \\(C^{'} =\\) 3 (Note k used \ncalculated value using Sturges formula approximated).Step 3. set individual class limits, need find \nlower limit \\[L = min - \\frac{C^{'} \\times k^{'} - (max - min)}{2}\\]C k final approximated class width number \nclasses respectively example\n\\(L = 1 - \\frac{(3 \\times 7) - (18 - 1)}{2}\\)=1-2=-1; since \nnegative values data = 0.Even though student measured whole numbers, data \ncontinuous, ‚Äú4 cm‚Äù means actual value anywhere\n3.5 cm 4.5 cm.","code":""},{"path":"introduction.html","id":"cumulative-frequency","chapter":"1 Introduction","heading":"1.18 Cumulative frequency","text":"many situations, interested number observations\ngiven class interval, number observations \nless (greater ) specified value. example, \ntable, can seen 3 leaves length less 3.5 cm 9\nleaves (.e.¬†3 + 6) length less 6.5 cm. frequencies \ncalled cumulative frequencies. table cumulative frequencies \ncalled cumulative frequency table cumulative frequency\ndistribution.Cumulative frequency defined running total frequencies.\nCumulative frequency can also defined sum previous\nfrequencies current point. Notice last cumulative\nfrequency equal sum frequencies. Two types \ncumulative frequencies Less cumulative frequency Greater\ncumulative frequency. Less cumulative frequency (LCF) \nnumber values less specified value. Greater cumulative\nfrequency (GCF) number observations greater specified\nvalue.specified value LCF case grouped frequency\ndistribution upper limits GCF lower limits\nclasses. LCF‚Äôs obtained adding frequencies \nsuccessive classes GCF obtained subtracting successive\nclass frequencies total frequency.","code":""},{"path":"introduction.html","id":"relative-frequency","chapter":"1 Introduction","heading":"1.19 Relative frequency","text":"sometimes useful know proportion, rather number,\nvalues falling within particular class interval. obtain \ninformation dividing frequency particular class interval\ntotal number observations. Relative frequency class\nfrequency class divided total observations. Relative\nfrequencies add 1.[1] ‚ÄúNote: = Less cumulative frequency; B= Greater cumulative frequency, C = Relative frequency‚Äù¬†\n¬†\n¬†","code":""},{"path":"graphical-representation-of-data.html","id":"graphical-representation-of-data","chapter":"2 Graphical representation of data","heading":"2 Graphical representation of data","text":"found information given frequency distribution easier \ninterpret raw data. Information given frequency distribution\ntabular form easier grasp presented graphically. Many\ntypes diagrams used statistics, depending nature \ndata purpose diagram intended.","code":""},{"path":"graphical-representation-of-data.html","id":"histogram","chapter":"2 Graphical representation of data","heading":"2.1 Histogram","text":"histogram consists rectangles :Bases horizontal axis, centres class marks, lengths\nequal class widths.Bases horizontal axis, centres class marks, lengths\nequal class widths.Areas proportional class frequencies.Areas proportional class frequencies.Note:\nclass intervals equal size, heights \nrectangles proportional class frequencies \ncustomary take heights rectangles numerically equal \nclass frequencies. class intervals different widths, \nheights rectangles proportional \n\\(\\frac{\\text{Class Frequency}}{\\text{Class Width}}\\). ratio \ncalled frequency density.Table shows frequency distribution plant height 50 plants. Draw Histogram.\nFigure 2.1: Histogram\n","code":""},{"path":"graphical-representation-of-data.html","id":"cumulative-frequency-curve-ogive","chapter":"2 Graphical representation of data","heading":"2.2 Cumulative frequency curve (Ogive)","text":"graph obtained plotting cumulative frequency class\nboundary joining points smooth curve, called \ncumulative frequency curve. also called Ogive. Two types \nogive , Less Type Cumulative Frequency Curve (Less \nOgive) Greater Type Cumulative Frequency Curve (Greater \nOgive).","code":""},{"path":"graphical-representation-of-data.html","id":"less-than-ogive","chapter":"2 Graphical representation of data","heading":"2.2.1 Less than Ogive","text":"Also known less type cumulative frequency curve. use \nupper limit classes less cumulative frequency \nplot curve. Let us see example plant height 50 plants.\nFigure 2.2: Less ogive\n","code":""},{"path":"graphical-representation-of-data.html","id":"greater-than-ogive","chapter":"2 Graphical representation of data","heading":"2.2.2 Greater than Ogive","text":"Also known greater type cumulative frequency curve use\nlower limit classes greater cumulative frequency\nplot curve.\nFigure 2.3: greater ogive\nNote:\nIntersection ogives gives median.","code":""},{"path":"graphical-representation-of-data.html","id":"frequency-polygon","chapter":"2 Graphical representation of data","heading":"2.2.3 Frequency polygon","text":"grouped frequency table can also represented frequency\npolygon, special kind line graph. construct frequency\npolygon, plot graph class frequencies corresponding\nclass mid-points join successive points straight lines.\nFrequency polygon also obtained joining midpoints \nhistogram shown Fig 2.5.\nFigure 2.4: Frequency polygon\n\nFigure 2.5: Frequency polygon histogram\n","code":""},{"path":"graphical-representation-of-data.html","id":"stem-and-leaf-plot","chapter":"2 Graphical representation of data","heading":"2.3 Stem-and-leaf plot","text":"stem--leaf plot graphical device useful \nrepresenting relatively small set data takes numerical\nvalues. construct stem--leaf plot, partition measurement\ntwo parts. first part called stem, second part\ncalled leaf. numerical value divided two parts:\nleading digits become stem trailing digits become leaf.\nOne advantage stem--leaf display frequency distribution\nretain value observation. Another \ndistribution data within groups clear. stem--leaf\nplot conveys similar information histogram. Turned side, \nshape histogram. fact, since stem--leaf\nplot shows observation,displays information lost \nhistogram. properly constructed stem--leaf plot, like histogram,\nprovides information regarding range data set, shows \nlocation highest concentration measurements, reveals \npresence absence symmetry.Consider example12,16,21,25,29,26,30,31,37,42,45stem leaf plot can drawn shown .\nFigure 2.6: Stem Leaf plot\n","code":""},{"path":"graphical-representation-of-data.html","id":"bar-chart","chapter":"2 Graphical representation of data","heading":"2.4 Bar chart","text":"bar chart bar graph diagram consisting series \nhorizontal vertical bars equal width. bars represent various\ncategories data. three types bar charts, \nsimple bar charts, component bar charts grouped bar charts.","code":""},{"path":"graphical-representation-of-data.html","id":"simple-bar-chart","chapter":"2 Graphical representation of data","heading":"2.4.1 Simple bar chart","text":"simple bar chart, height (length) bar equal \nvalue category y-axis represents. example data\nshows production coconut five districts Kerala \ncertain year.\nFigure 2.7: Barchart\n","code":""},{"path":"graphical-representation-of-data.html","id":"component-bar-chart","chapter":"2 Graphical representation of data","heading":"2.4.2 Component bar chart","text":"component bar chart, bar category subdivided \ncomponent parts; hence name. Component bar charts therefore used\nshow division items components. illustrated \nfollowing example.Example shows distribution sales agricultural produce \nFarm 1995, 1996 1997.\nFigure 2.8: Sales data agricultural produce\n\nFigure 2.9: Component bar chart\ncomponent bar chart shows changes component \nyears well comparison total sales different\nyears.","code":""},{"path":"graphical-representation-of-data.html","id":"grouped-bar-chart","chapter":"2 Graphical representation of data","heading":"2.4.3 Grouped bar chart","text":"grouped bar chart, components grouped together drawn\nside side. illustrate example.\nFigure 2.10: Grouped bar chart\n","code":""},{"path":"graphical-representation-of-data.html","id":"histogram-and-bar-chart","chapter":"2 Graphical representation of data","heading":"2.5 Histogram and Bar chart","text":"","code":""},{"path":"graphical-representation-of-data.html","id":"pie-charts","chapter":"2 Graphical representation of data","heading":"2.6 Pie Charts","text":"pie chart circular graph divided sectors, sector\nrepresenting different value category. angle sector \npie chart proportional value part data \nrepresents. bar chart precise pie chart visual\ncomparison categories similar relative frequencies.","code":""},{"path":"graphical-representation-of-data.html","id":"steps-for-constructing-a-pie-chart","chapter":"2 Graphical representation of data","heading":"2.6.1 Steps for constructing a pie chart","text":"Find sum category values.Calculate angle sector category, using \nfollowing formula.Angle sector category =\n\\(\\frac{\\text{value category }}{\\text{sum category values}} \\times 360\\)Construct circle mark centre.Use protractor divide circle sectors, using angles\nobtained step 2.Label sector clearly.See example.\nproduction different commodities India particular\nyear given follows.\nFigure 2.11: Pie chart\n¬†\n¬†\n¬†","code":""},{"path":"measures-of-central-tendency---i.html","id":"measures-of-central-tendency---i","chapter":"3 Measures of Central Tendency - I","heading":"3 Measures of Central Tendency - I","text":"previous chapter, learnt data can summarised \nform tables presented form graphs important\nfeatures can illustrated easily effectively. \nLecture, consider statistical measures can used describe\ncharacteristics set data.interested single value serves representative\nvalue overall data. ¬†measure central tendency¬†\nsummary statistic represents centre point typical value \ndataset.five averages. Among mean, median mode called\nsimple averages two averages geometric mean \nharmonic mean called special averages. measures reflect\nnumerical values centre set data therefore called\nmeasures central tendency.Requisites Good Measure Central Tendency:rigidly defined.rigidly defined.simple understand & easy calculateIt simple understand & easy calculateIt based upon values given dataIt based upon values given dataIt capable mathematical treatment.capable mathematical treatment.sampling stability.sampling stability.unduly affected extreme valuesIt unduly affected extreme valuesThe main objectives Measure Central Tendency:condense data single value.condense data single value.facilitate comparisons data.facilitate comparisons data.","code":""},{"path":"measures-of-central-tendency---i.html","id":"arithmetic-mean","chapter":"3 Measures of Central Tendency - I","heading":"3.1 Arithmetic Mean","text":"people usually intend say ‚Äúaverage‚Äù. Arithmetic\nmean simply mean variable defined sum \nobservations divided number observations. Mean set \nnumbers \\(x_{1\\ },x_{2},\\ldots,x_{n}\\) denoted \\(\\overline{x}\\). \ngiven formula\\[\\overline{x} = \\frac{x_{1} + x_{2} + \\ldots + x_{n}}{n}\\]\\[= \\frac{1}{n}\\sum_{= 1}^{n}x_{}\\]Example 3.1 Find mean numbers 2, 4, 7, 8, 11, 12\\[\\overline{x} = \\frac{2 + 4 + 7 + 8 + 11 + 12}{6} = \\frac{44}{6} = 7.33\\]","code":""},{"path":"measures-of-central-tendency---i.html","id":"the-mean-of-a-frequency-distribution","chapter":"3 Measures of Central Tendency - I","heading":"3.1.1 The mean of a frequency distribution","text":"","code":""},{"path":"measures-of-central-tendency---i.html","id":"direct-method","chapter":"3 Measures of Central Tendency - I","heading":"3.1.1.1 Direct method","text":"numbers \\(x_{1\\ },x_{2},\\ldots,x_{n}\\) occur frequencies\n\\(f_{1\\ },f_{2},\\ldots,f_{n}\\) respectively \\[\\overline{x} = \\frac{x_{1}f_{1} + x_{2}f_{2\\ \\ } + \\ldots + x_{n}f_{n}}{f_{1} + f_{2} + \\ldots f_{n}}\\]\\[= \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}}\\]Example 3.2 Table shows plant height 50 plants. Find\nmean plant height.Table 3.1:  Plant height 50 plants.Solution 3.2The calculation can arranged shown\\(\\overline{x} = \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}} = \\frac{8054}{50}\\)=\n161.08 cm","code":""},{"path":"measures-of-central-tendency---i.html","id":"assumed-mean-method-indirect-method","chapter":"3 Measures of Central Tendency - I","heading":"3.1.1.2 Assumed mean method (Indirect method)","text":"amount computation involved can reduced using \nfollowing formula:\\[\\overline{x} = + \\frac{\\sum_{= 1}^{n}{f_{}d_{}}}{\\sum_{= 1}^{n}f_{}}\\]\\(\\) assumed mean, can value x.\n\\(d_{} = x_{} - \\), \\(f_{}\\) frequency \\(x_{}\\)Consider Example 3.2let \\(\\) = 161; can number x\\(\\overline{x} = 161 + \\frac{4}{50}\\) = 161.08 cmThe mean plant height 161.08 cm","code":""},{"path":"measures-of-central-tendency---i.html","id":"mean-of-grouped-data","chapter":"3 Measures of Central Tendency - I","heading":"3.1.2 Mean of Grouped Data","text":"","code":""},{"path":"measures-of-central-tendency---i.html","id":"direct-method-1","chapter":"3 Measures of Central Tendency - I","heading":"3.1.2.1 Direct method","text":"mean grouped data obtained following formula:\\[\\overline{x} = \\frac{\\sum_{= 1}^{k}{f_{}x_{}}}{n}\\]\\(x_{}\\) = mid-point ith class (ith class mark);\n\\(f_{}\\)= frequency ith class; \\(n\\) = sum \nfrequencies total frequencies sample. Note =1,2...,\nk, .e. k classes.Example 3.3 Shows distribution marks scored 60\nstudents Maths examination. Find mean mark.Table 3.2:  Distribution marks scored 60 studentsSolution 3.3The solution can arranged shown\\(\\overline{x} = \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}} = \\frac{4365}{60}\\)=\n72.75The mean mark 72.75%","code":""},{"path":"measures-of-central-tendency---i.html","id":"coding-method-indirect-method","chapter":"3 Measures of Central Tendency - I","heading":"3.1.2.2 Coding method (Indirect method)","text":"class intervals grouped frequency distribution \nequal size \\(C\\) (class width); following formula can used\ninstead direct method . formula makes calculations easier.\\[\\overline{x} = + C\\frac{\\sum_{= 1}^{n}{f_{}u_{}}}{\\sum_{= 1}^{n}f_{}}\\]\\(\\) class mark highest frequency,\n\\(u_{} = \\frac{x_{} - }{C}\\), \\(f_{}\\) frequency \\(x_{}\\), C\nclass width.called ‚Äúcoding‚Äù method computing mean. \nshort method always used finding mean grouped\nfrequency distribution equal class widths.Consider Example 3.3 see Table:3.2\\(\\)=72.5, class mark highest frequency; \\(C\\) =5\\(\\overline{x} = 72.5 + 5 \\times \\left( \\frac{3}{60} \\right)\\)= 72.75The mean mark 72.75%","code":""},{"path":"measures-of-central-tendency---i.html","id":"merits-and-demerits-of-arithmetic-mean","chapter":"3 Measures of Central Tendency - I","heading":"3.1.3 Merits and demerits of Arithmetic mean","text":"MeritsIt rigidly defined.rigidly defined.easy understand easy calculate.easy understand easy calculate.number items sufficiently large, accurate\nreliable.number items sufficiently large, accurate\nreliable.calculated value based position \nseries.calculated value based position \nseries.possible calculate even details data\nlacking.possible calculate even details data\nlacking.averages, affected least fluctuations sampling.averages, affected least fluctuations sampling.provides good basis comparison.provides good basis comparison.DemeritsIt obtained inspection located frequency\ngraph.obtained inspection located frequency\ngraph.study qualitative phenomena capable \nnumerical measurement .e. Intelligence, beauty, honesty etc.study qualitative phenomena capable \nnumerical measurement .e. Intelligence, beauty, honesty etc.can ignore single item risk losing \naccuracy.can ignore single item risk losing \naccuracy.affected much extreme values.affected much extreme values.calculated open-end classes.calculated open-end classes.may lead fallacious conclusions, details data\ncomputed given.may lead fallacious conclusions, details data\ncomputed given.","code":""},{"path":"measures-of-central-tendency---i.html","id":"the-median","chapter":"3 Measures of Central Tendency - I","heading":"3.2 The Median","text":"median set data defined middle value \ndata arranged order magnitude. ties, half \nobservations smaller median, half \nobservations larger median. median can \nmiddle item divides group two equal parts, one part\ncomprising values greater, , values less \nitem. positional measure.","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-ungrouped-or-raw-data","chapter":"3 Measures of Central Tendency - I","heading":"3.2.1 Median of ungrouped or raw data","text":"Arrange given n observations \\(x_{1\\ },x_{2},\\ldots,x_{n}\\) \nascending order. number values odd, median middle\nvalue. number values even, median mean middle two\nvalues.Arrange data ascending use following formulaWhen n odd, Median = Md\n=\\(\\left( \\frac{n + 1}{2} \\right)^{\\text{th}}\\)valueWhen n even, Median = Md\n=\\({\\text{Average }\\left( \\frac{n}{2} \\right)^{th}\\text{}\\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}}\\)valueExample 3.4 Find median following sets \nnumbers.\\()\\) 12, 15, 22, 17, 20, 26, 22, 26, 12\\(b)\\) 4, 7, 9, 10, 5, 1, 3, 4, 12, 10Solution 3.4\\()\\) Arranging data increasing order magnitude, obtain\n12, 12, 15, 17, 20, 22, 22, 26, 26. , N = 9 odd, , median\n=\\(\\left( \\frac{9 + 1}{2} \\right)^{\\text{th}}\\)= 5th ordered observation\n= 20.Note: number repeated, still count number times \nappears calculate median.\\(b)\\) Arranging data increasing order magnitude, obtain\n1, 3, 4, 4, 5, 7, 9, 10, 10, 12. , N = 10 even number \nmedian = \\(\\frac{1}{2}\\){5th ordered observation + 6th ordered\nobservation} = \\(\\frac{1}{2}\\left( 5 + 7 \\right) = 6\\).Note: can see case, median divides distribution \ntwo equal parts, 50% observations greater \n50% less .","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-ungrouped-frequency-distribution","chapter":"3 Measures of Central Tendency - I","heading":"3.2.2 Median of ungrouped frequency distribution","text":"median middle number ordered set data. \nfrequency table, observations already arranged ascending\norder. can obtain median looking value middle\nposition.","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-a-frequency-table-when-the-number-of-observations-is-odd","chapter":"3 Measures of Central Tendency - I","heading":"3.2.2.1 Median of a frequency table when the number of observations is odd","text":"number observations (n) odd, median value\n¬†\\(\\left( \\frac{n + 1}{2} \\right)^{\\text{th}}\\) positional value.\nuse less cumulative frequency.Example 3.5: following frequency table score\nobtained mathematics quiz. Find median score.Table 3.3:  Score obtained mathematics quiz.Solution 3.5:Total frequency = 3 + 4 + 7 + 6 + 3 = 23 (odd number). Since number\nscores odd, median \n\\(\\left( \\frac{23 + 1}{2} \\right)^{\\text{th}} =\\) 12th¬†position. find\n12th¬†position, use less cumulative frequencies \nshown:12th¬†position 7th¬†position \n14th¬†position. , median 2.","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-a-frequency-table-when-the-number-of-observations-is-even","chapter":"3 Measures of Central Tendency - I","heading":"3.2.2.2 Median of a frequency table when the number of observations is even","text":"number observations even, median average\n\n\\({\\left( \\frac{n}{2} \\right)^{th}\\text{}\\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}}\\)\nposition values.Example 3.6: table frequency table marks obtained \ncompetition. Find median score.Table 3.4:  Distribution marks obtained competition.Solution 3.6:Total frequency = 11 + 9 + 5 + 10 + 15 = 50 (even number). Since \nnumber scores even, median average ¬†values \n\\({\\left( \\frac{n}{2} \\right)^{th} = 25\\ \\ \\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}} = 26\\)\npositions. find 25th¬†position 26th¬†position, add\nfrequencies shown:mark 25th¬†position 2 mark 26th¬†position\n3. median average scores 25th¬†\n26th¬†positions =¬†\\(\\frac{2 + 3}{2} = 2.5\\)","code":""},{"path":"measures-of-central-tendency---i.html","id":"median-of-grouped-frequency-distribution","chapter":"3 Measures of Central Tendency - I","heading":"3.2.3 Median of grouped frequency distribution","text":"exact value median grouped data obtained\nactual values grouped data known. grouped\nfrequency distribution, median class interval \ncontains \\(\\left( \\frac{N}{2} \\right)^{\\text{th}}\\)ordered\nobservation, \\(N\\) total number observations. class\ninterval called median class. median grouped\nfrequency distribution can estimated either following two\nmethods:","code":""},{"path":"measures-of-central-tendency---i.html","id":"linear-interpolation-method-for-estimating-the-median","chapter":"3 Measures of Central Tendency - I","heading":"3.2.3.1 Linear interpolation method for estimating the median","text":"median grouped frequency distribution can estimated \nlinear interpolation. assume observations evenly spread\nmedian class. median can computed using \nfollowing formula:\\[Median = L + \\left( \\frac{\\frac{1}{2}N - F}{f_{m}} \\right)C\\]\\(N\\) = total number observations, \\(L\\) = lower limit \nmedian class, \\(F\\) = sum frequencies L(cumulative\nfrequency), \\(f_{m}\\) = frequency median class, \\(C\\) = class width\nmedian class.","code":""},{"path":"measures-of-central-tendency---i.html","id":"estimation-of-the-median-from-a-cumulative-frequency-curve","chapter":"3 Measures of Central Tendency - I","heading":"3.2.3.2 Estimation of the median from a cumulative frequency curve","text":"median grouped frequency distribution can estimated \ncumulative frequency curve. horizontal line drawn point\n\\(\\frac{\\text{N}}{2}\\) vertical axis meet cumulative\nfrequency curve. point intersection, vertical line \ndropped horizontal axis. value horizontal axis \nequal median.\nFigure 3.1: median cumulative frequency curve\nExample 3.7 Table gives distribution heights 60\nstudents Senior High school. Find median height studentsTable 3.5: Distribution heights 60 studentsSolution 3.7() Linear interpolation method estimating median\\(N\\) = 60Median class= class interval contains \n\\(\\left( \\frac{N}{2} \\right)^{\\text{th}}\\)ordered observation; \n\\(\\left( \\frac{60}{2} \\right)^{\\text{th}} =\\) 30th observation. \nclass 160-165 3+9+16=28 observations 30th observation\nclass 160-165, therefore median class.\\(L\\) = lower limit median class =160\\(F\\) = sum frequencies 160(cumulative frequency) = 16+9+3=\n28\\(f_{m}\\) = frequency median class=18\\(C\\) = class width median class=5\\(median = 160 + \\left( \\frac{\\frac{1}{2}60 - 28}{18} \\right)5\\) = 160.56(ii) Estimation median cumulative frequency curve\nFigure 3.2: Median cumulative frequency curve Example 3.7\n","code":""},{"path":"measures-of-central-tendency---i.html","id":"merits-and-demerits-of-median","chapter":"3 Measures of Central Tendency - I","heading":"3.2.4 Merits and Demerits of Median","text":"MeritsMedian influenced extreme values \npositional average.Median influenced extreme values \npositional average.Median can calculated case distribution open-end\nintervals.Median can calculated case distribution open-end\nintervals.Median can located even data incomplete.Median can located even data incomplete.DemeritsA slight change series may bring drastic change median\nvalue.slight change series may bring drastic change median\nvalue.case even number items continuous series, median \nestimated value value series.case even number items continuous series, median \nestimated value value series.suitable mathematical treatment except use\ncalculating mean deviation.suitable mathematical treatment except use\ncalculating mean deviation.take account observations.take account observations.","code":""},{"path":"measures-of-central-tendency---i.html","id":"the-mode","chapter":"3 Measures of Central Tendency - I","heading":"3.3 The mode","text":"mode set data value occurs greatest\nfrequency. mode therefore common value. mode \nimportant measure case qualitative data. mode can used \ndescribe quantitative qualitative data.","code":""},{"path":"measures-of-central-tendency---i.html","id":"mode-of-ungrouped-or-raw-data","chapter":"3 Measures of Central Tendency - I","heading":"3.3.1 Mode of ungrouped or raw data","text":"ungrouped data series individual observations, mode often\nfound mere inspection.Example 3.8\\()\\) modes 1, 2, 2, 2, 3 2.\\(b)\\) modes 2, 3, 4, 4, 5, 5 4 5.\\(c)\\) mode exist every observation \nfrequency. example, following sets data modes: () 3,\n6, 8, 9; (ii) 4, 4, 4, 7, 7, 7, 9, 9, 9.Note: can seen mode distribution may exist, \neven exists, may unique. Distributions single\nmode referred unimodal. Distributions two modes \nreferred bimodal. Distributions may several modes, \ncase referred multimodal.Example 3.9 20 patients selected random blood groups\ndetermined. results given table belowTable 3.6:  Blood groups 20 patientsThe blood group highest frequency O. mode data \ntherefore blood group O. can say patients selected\nblood group O. Notice mean median \napplied data. variable ‚Äúblood group‚Äù \ntake numerical values. However, can seen mode can used\ndescribe quantitative qualitative data.","code":""},{"path":"measures-of-central-tendency---i.html","id":"mode-of-grouped-frequency-distribution","chapter":"3 Measures of Central Tendency - I","heading":"3.3.2 Mode of Grouped frequency distribution","text":"\\[mode = L + \\left( \\frac{f_{m}-f_{p}}{2f_{m}-f_{p} - f_{s}} \\right)C\\]Locate highest frequency class corresponding frequency\ncalled modal class.\\(L\\) = lower limit modal class; \\(f_{m}\\) = frequency modal class; \\(f_{p}\\)= frequency \nclass preceding modal class; \\(f_{s}\\)= frequency class\nsucceeding modal class \\(C\\) = class intervalExample 3.10 frequency distribution weights sorghum\near-heads given table . Calculate mode.Table 3.7: frequency distribution weights sorghum ear headsModal class 100-120\\(mode = 100 + \\left( \\frac{45-35}{90-38-35} \\right)20 =\\) 111.76","code":""},{"path":"measures-of-central-tendency---i.html","id":"mode-using-histogram","chapter":"3 Measures of Central Tendency - I","heading":"3.3.3 Mode using Histogram","text":"Consider figure . modal class class interval \ncorresponds rectangle \\(\\text{ABCD}\\). estimate mode \ndistribution abscissa point intersection line\nsegments \\(\\overline{\\text{AE}}\\) \\(\\overline{\\text{BF}}\\) \nfigure.\nFigure 3.3: Median cumulative frequency curve Example 3.10\n","code":""},{"path":"measures-of-central-tendency---i.html","id":"merits-and-demerits-of-mode","chapter":"3 Measures of Central Tendency - I","heading":"3.3.4 Merits and Demerits of Mode","text":"MeritsIt readily comprehensible easy compute. case \ncan computed merely inspection.readily comprehensible easy compute. case \ncan computed merely inspection.affected extreme values. can obtained even \nextreme values known.affected extreme values. can obtained even \nextreme values known.Mode can determined distributions open classes.Mode can determined distributions open classes.Mode can located graph also.Mode can located graph also.Mode can used describe quantitative qualitative data.Mode can used describe quantitative qualitative data.DemeritsThe mode unique. , can one mode \ngiven set data.mode unique. , can one mode \ngiven set data.mode set data may exist.mode set data may exist.based upon observation.based upon observation.¬†\n¬†\n¬†","code":""},{"path":"measures-of-central-tendency--ii.html","id":"measures-of-central-tendency--ii","chapter":"4 Measures of Central Tendency -II","heading":"4 Measures of Central Tendency -II","text":"","code":""},{"path":"measures-of-central-tendency--ii.html","id":"geometric-mean","chapter":"4 Measures of Central Tendency -II","heading":"4.1 Geometric mean","text":"geometric mean type average, usually used growth rates,\nlike population growth interest rates. arithmetic mean adds\nitems, geometric mean multiplies items.geometric mean series containing n observations \nnth root product values. \n\\(x_{1},x_{2},\\ldots,¬†x_{n}\\)observations \\[\\mathbf{\\text{Geometric mean}}\\mathbf{,\\ }\\mathbf{GM =}\\sqrt[\\mathbf{n}]{\\mathbf{x}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots}\\mathbf{x}_{\\mathbf{n}}}\\]\\[\\mathbf{=}\\left( \\mathbf{x}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots}\\mathbf{x}_{\\mathbf{n}} \\right)^{\\frac{\\mathbf{1}}{\\mathbf{n}}}\\]\\[\\mathbf{\\log}\\mathbf{\\text{GM}}\\mathbf{=}\\frac{\\mathbf{1}}{\\mathbf{n}}\\mathbf{\\log}\\left( \\mathbf{x}_{\\mathbf{1}}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots}\\mathbf{x}_{\\mathbf{n}} \\right)\\]\\[\\mathbf{=}\\frac{\\mathbf{1}}{\\mathbf{n}}\\left( \\mathbf{\\log}\\mathbf{x}_{\\mathbf{1}}\\mathbf{+}\\mathbf{\\log}\\mathbf{x}_{\\mathbf{2}}\\mathbf{\\ldots +}\\mathbf{\\log}\\mathbf{x}_{\\mathbf{n}} \\right)\\]\\[\\mathbf{=}\\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{n}}{\\mathbf{\\log}\\mathbf{x}_{\\mathbf{}}}}{\\mathbf{n}}\\]\\[\\mathbf{\\ GM = Antilog}\\left( \\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{n}}{\\mathbf{\\log}\\mathbf{x}_{\\mathbf{}}}}{\\mathbf{n}} \\right)\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"geometric-mean-for-grouped-frequency-table-data","chapter":"4 Measures of Central Tendency -II","heading":"4.1.1 Geometric mean for grouped frequency table data","text":"\\[\\mathbf{GM = \\ Antilog}\\left( \\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{k}}{{\\mathbf{f}_{\\mathbf{}}\\mathbf{\\log}}\\mathbf{x}_{\\mathbf{}}}}{\\mathbf{n}} \\right)\\]\\(x_{}\\) mid-value, \\(f_{}\\) frequency , k \nnumber classesExample 4.1: weight sorghum ear heads 45, 60, 48,100,\n65 gms. Find Geometric mean?Solution 4.1:n =5Geometric mean=\\[\\text{Antilog}\\left( \\frac{\\sum_{= 1}^{n}{\\log x_{}}}{n} \\right) \\]\\[=\\text{Antilog}\\left( \\frac{8.926}{5} \\right) \\]\\[ =\\text{Antilog}(1.785) = 60.95\\] note: \n\\(\\text{Antilog}\\left( x \\right) = 10^{x}\\) .e.\n\\[\\text{Antilog}\\left( 1.785 \\right) = \\ 10^{1.785} = 60.95\\]Example 4.2: Geometric mean Frequency DistributionSolution 4.2: n =32\\[GM = \\ Antilog\\left( \\frac{\\sum_{= 1}^{k}{{f_{}\\log}x_{}}}{n} \\right)\\]\\[{\\sum_{= 1}^{k}{{f_{}\\log}x_{}} = 57.782\n}\\]\\[{\\text{GM} = \\ Antilog\\left( \\frac{57.782}{32} \\right)  }\\]\\[{= Antilog\\left( 1.8056 \\right)= 10^{1.8056} = 63.92}\\] Example\n4.3: Geometric mean Grouped Frequency DistributionSolution 4.4:\nn =32\\[GM = \\ Antilog\\left( \\frac{\\sum_{= 1}^{k}{{f_{}\\log}x_{}}}{n} \\right)\\]\\[{\\sum_{= 1}^{k}{{f_{}\\log}x_{}} = 65.787}\\]\n\\[{\\text{GM} = \\ Antilog\\left( \\frac{65.787}{32} \\right)}\\]\n\\[{= Antilog\\left( 2.0558 \\right) = 10^{2.0558} = 113.71}\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"merits-and-demerits-of-geometric-mean","chapter":"4 Measures of Central Tendency -II","heading":"4.1.2 Merits and Demerits of Geometric mean","text":"MeritsIt rigidly defined.rigidly defined.based observations series.based observations series.suitable measuring relative changes.suitable measuring relative changes.gives weights small values less weight \nlarge values.gives weights small values less weight \nlarge values.used averaging ratios, percentages determining\nrate gradual increase decrease.used averaging ratios, percentages determining\nrate gradual increase decrease.capable algebraic treatment.capable algebraic treatment.DemeritsIt easy understand.easy understand.difficult calculate.difficult calculate.calculated, number negative values odd.calculated, number negative values odd.calculated, value series zero.calculated, value series zero.times gives value may found series \nimpractical.times gives value may found series \nimpractical.","code":""},{"path":"measures-of-central-tendency--ii.html","id":"harmonic-mean","chapter":"4 Measures of Central Tendency -II","heading":"4.2 Harmonic mean","text":"Harmonic means often used averaging things like rates (e.g.¬†\naverage travel speed given duration several trips). Harmonic mean\n(HM) set observations defined reciprocal \narithmetic average reciprocal given value.\\(x_{1},\\ x_{2},\\ldots,\\ x_{n}\\) n observations \\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{n}\\frac{1}{x_{}}}\\]case Frequency distribution\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{k}{f_{}\\frac{1}{x_{}}}}\\]\\(x_{}\\) mid-value, \\(f_{}\\) frequency , k \nnumber classes","code":""},{"path":"measures-of-central-tendency--ii.html","id":"steps-in-calculating-harmonic-mean-h.m","chapter":"4 Measures of Central Tendency -II","heading":"4.2.1 Steps in calculating Harmonic Mean (H.M)","text":"Calculate reciprocal (1/value) every value.Calculate reciprocal (1/value) every value.Find average reciprocals (just add divide \nmany )Find average reciprocals (just add divide \nmany )reciprocal average (=1/average)reciprocal average (=1/average)Example 4.4: given data 5, 10, 17, 24, 30 calculate H.MSolution 4.4:n = 5\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{n}\\frac{1}{x_{}}} = \\frac{5}{0.433824} = 11.525\\]Example 4.5: Number tomatoes per plant given . Calculate\nharmonic mean.Solution 4.5:n =18\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{= 1}^{n}{f_{}\\frac{1}{x_{}}}} = \\frac{18}{0.821898} = 21.90\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"merits-and-demerits-of-harmonic-mean","chapter":"4 Measures of Central Tendency -II","heading":"4.2.2 Merits and Demerits of Harmonic mean","text":"MeritsIt rigidly defined.rigidly defined.defined observations.defined observations.amenable algebraic treatment.amenable algebraic treatment.suitable average desired give greater\nweight smaller less weight larger ones.suitable average desired give greater\nweight smaller less weight larger ones.DemeritsIt easily understood.easily understood.difficult compute.difficult compute.summary figure may actual item \nseries.summary figure may actual item \nseries.gives greater importance small items therefore, useful\nsmall items given greater weightage.gives greater importance small items therefore, useful\nsmall items given greater weightage.rarely used grouped data.rarely used grouped data.","code":""},{"path":"measures-of-central-tendency--ii.html","id":"relation-between-am-gm-and-hm","chapter":"4 Measures of Central Tendency -II","heading":"4.3 Relation between AM, GM and HM","text":"stands Arithmetic Mean, GM stands Geometric Mean HM\nstands Harmonic Mean; \\[\\mathbf{\\text{}}\\mathbf{\\times}\\mathbf{\\text{HM}}\\mathbf{=}\\mathbf{\\text{GM}}^{\\mathbf{2}}\\]also\\[\\mathbf{\\geq GM \\geq HM}\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"when-to-use-am-gm-and-hm","chapter":"4 Measures of Central Tendency -II","heading":"4.4 When to use AM, GM and HM?","text":"practical answer depends numbers \nmeasuring.measuring units add linearly sequence; \nlengths, distances, weights, arithmetic mean give \nmeaningful average. example, arithmetic mean height \nweight students class represents average height weight \nstudents class.Harmonic mean give meaningful average, measuring\nunits add reciprocals sequence; speed \ndistance travelled per unit time, capacitance series, resistance \nparallel. example, harmonic mean capacitors series\nrepresents capacitance single capacitor \none capacitor used instead set capacitors series.‚Äôre measuring units multiply sequence; growth\nrates percentages, geometric mean give meaningful\naverage. example, geometric mean sequence different\nannual interest rates 10 years represents interest rate , \napplied constantly ten years, produce amount growth\nprincipal sequence different annual interest rates ten\nyears .","code":""},{"path":"measures-of-central-tendency--ii.html","id":"positional-averages","chapter":"4 Measures of Central Tendency -II","heading":"4.5 Positional Averages","text":"Positional average series values refers averages \ntaken series represents whole series\nmay positional properties.median, middle value series taken \nrepresentative value. Therefore, median positional average. Mode \nalso positional average modal values frequently\noccurring values directly taken series . \npositional averages include Percentiles, Quartiles \nDecilesNote Arithmetic mean, Harmonic mean Geometric mean termed\nmathematical averages","code":""},{"path":"measures-of-central-tendency--ii.html","id":"quartile","chapter":"4 Measures of Central Tendency -II","heading":"4.5.1 Quartiles","text":"median divides set data two equal parts. can also\ndivide set data two parts. ordered set \ndata divided four equal parts, division points called\nquartiles.first lower quartile (\\(\\mathbf{Q}_{\\mathbf{1}}\\)) \nvalue one fourth, 25% observations value.second quartile (\\(\\mathbf{Q}_{\\mathbf{2}}\\)), one-half,\n50% observations value. second quartile equal\nmedian.third upper quartile, (\\(\\mathbf{Q}_{\\mathbf{3}}\\)), \nvalue three-fourths, 75% observations .\\(\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)item\\(\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)itemCalculations quartiles explained using example . See \nexample procedure followed fraction appear \ncalculation.Example 4.6: Compute quartiles data 25, 18, 30, 8, 15, 5,\n10, 35, 40, 45Solution 4.6:First arrange data ascending order5, 8, 10, 15, 18, 25, 30, 35, 40, 45here n = 10\\(\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\left(\\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)itemi.e. \\(Q_{1} = \\left( \\frac{10 + 1}{4} \\right)^{th}\\) = 2.75th item;\nfraction appears use following procedure\\(Q_{1} =\\)2.75th item = 2nd item + 0.75(3rd item ‚Äì 2nd\nitem)given data \\(Q_{1}\\)= 8+0.75(10‚Äì 8) = 9.5\\[\\mathbf{Q}_{\\mathbf{2}}\\mathbf{= median}\\]\\(Q_{2} =\\)(18+25)/2 = 21.5\\(\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)^{\\mathbf{\\text{th}}}\\)itemi.e. \\(Q_{3} = \\left( 3 \\times \\frac{(10 + 1)}{4} \\right)^{th}\\) =\n8.25th item = 8th item + 0.25(9th item ‚Äì8th item) =\n35+0.25(40-35) =36.25","code":""},{"path":"measures-of-central-tendency--ii.html","id":"quartiles-of-a-discrete-frequency-data","chapter":"4 Measures of Central Tendency -II","heading":"4.5.1.1 Quartiles of a discrete frequency data","text":"Find cumulative frequencies.Find cumulative frequencies.Find \\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\)Find \\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\) , \ncorresponding value \\(x\\) \\(Q_{1}\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\) , \ncorresponding value \\(x\\) \\(Q_{1}\\)Find \\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\)Find \\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\) ,\ncorresponding value \\(x\\) \\(Q_{3}\\)See cumulative frequencies, value just greater \n\\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\) ,\ncorresponding value \\(x\\) \\(Q_{3}\\)Example 4.7: Compute quartiles data given bellowSolution 4.7:n =24\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\) =\n\\(\\left( \\frac{\\mathbf{n + 1}}{\\mathbf{4}} \\right)\\mathbf{\\ }\\)=\n\\(\\left( \\frac{\\mathbf{25}}{\\mathbf{4}} \\right)\\)= 6.25The cumulative frequency value just greater 6.25 7, \\(\\mathbf{x}\\) value corresponding cumulative frequency 7 8. \n\\(\\mathbf{Q}_{\\mathbf{1}}\\)= 8\\(\\left( \\frac{\\mathbf{3(n + 1)}}{\\mathbf{4}} \\right)\\) =\n\\(\\left( \\frac{\\mathbf{3}\\mathbf{\\times}\\mathbf{25}}{\\mathbf{4}} \\right)\\)=\n18.75The cumulative frequency value just greater 18.75 20, \\(\\mathbf{x}\\) value corresponding cumulative frequency 20 24. \n\\(\\mathbf{Q}_{\\mathbf{3}}\\)= 24","code":""},{"path":"measures-of-central-tendency--ii.html","id":"quartiles-of-a-continuous-frequency-data","chapter":"4 Measures of Central Tendency -II","heading":"4.5.1.2 Quartiles of a continuous frequency data","text":"Find cumulative frequenciesFind cumulative frequenciesFind \\(\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)Find \\(\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)See cumulative frequencies, value just greater\n\\(\\ \\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\), \ncorresponding class interval called first quartile class.See cumulative frequencies, value just greater\n\\(\\ \\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\), \ncorresponding class interval called first quartile class.Find \\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)Find \\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\)See cumulative frequencies value just greater \n\\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{\\ }\\)\ncorresponding class interval called 3rd quartile class.\napply respective formulaeSee cumulative frequencies value just greater \n\\(3\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{\\ }\\)\ncorresponding class interval called 3rd quartile class.\napply respective formulae\\[\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\mathbf{l}_{\\mathbf{1}}\\mathbf{+}\\frac{\\frac{\\mathbf{n}}{\\mathbf{4}}\\mathbf{-}\\mathbf{m}_{\\mathbf{1}}}{\\mathbf{f}_{\\mathbf{1}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{1}}\\]\\[\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\mathbf{l}_{\\mathbf{3}}\\mathbf{+}\\frac{\\mathbf{3}\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{-}\\mathbf{m}_{\\mathbf{3}}}{\\mathbf{f}_{\\mathbf{3}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{3}}\\], \\(l_{1}\\) = lower limit first quartile class\\(f_{1}\\) = frequency first quartile class\\(c_{1}\\) = width first quartile class\\(m_{1}\\) = cumulative frequency preceding first quartile class\\(l_{3}\\)= 1ower limit 3rd quartile class\\(f_{3}\\)= frequency 3rd quartile class\\(c_{3}\\)= width 3rd quartile class\\(m_{3}\\) = cumulative frequency preceding 3rd quartile classExample 4.8: Find quartiles grouped frequency data givenSolution 4.8:\\(\\left( \\frac{n}{4} \\right)\\) = \\(\\frac{204}{4}\\) = 51The cumulative frequency value just greater 51 54 class\n20-30 1st quartile class\\[\\mathbf{Q}_{\\mathbf{1}}\\mathbf{=}\\mathbf{l}_{\\mathbf{1}}\\mathbf{+}\\frac{\\frac{\\mathbf{n}}{\\mathbf{4}}\\mathbf{-}\\mathbf{m}_{\\mathbf{1}}}{\\mathbf{f}_{\\mathbf{1}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{1}}\\]\\[\\mathbf{= 20 +}\\frac{\\mathbf{51 - 29}}{\\mathbf{25}}\\mathbf{\\times 10\\  = 28.8}\\]\\(3\\left( \\frac{n}{4} \\right)\\)= \\(3 \\times \\frac{204}{4}\\) = 153The cumulative frequency value just greater 153 167 class\n60-70 3rd quartile class\\[\\mathbf{Q}_{\\mathbf{3}}\\mathbf{=}\\mathbf{l}_{\\mathbf{3}}\\mathbf{+}\\frac{\\mathbf{3}\\left( \\frac{\\mathbf{n}}{\\mathbf{4}} \\right)\\mathbf{-}\\mathbf{m}_{\\mathbf{3}}}{\\mathbf{f}_{\\mathbf{3}}}\\mathbf{\\times}\\mathbf{c}_{\\mathbf{3}}\\]\\[\\mathbf{= 60 +}\\frac{\\mathbf{153 - 145}}{\\mathbf{22}}\\mathbf{\\times 10 = 63.63}\\]","code":""},{"path":"measures-of-central-tendency--ii.html","id":"percentiles","chapter":"4 Measures of Central Tendency -II","heading":"4.5.2 Percentiles","text":"percentile values divide ordered set data 100 equal parts\ncontaining 1 percent observations. xth percentile,\ndenoted \\(P_{x}\\) value x percent values \ndistribution fall. may noted median 50th\npercentile, 25th percentile first quartile \\(Q_{1}\\) 75th\npercentile $_{3}$raw data, first arrange n observations increasing order.\nxth percentile given \\(\\mathbf{P}_{\\mathbf{x}}\\mathbf{=}\\left( \\frac{\\mathbf{x}\\left( \\mathbf{n + 1} \\right)}{\\mathbf{100}} \\right)^{\\mathbf{\\text{th}}}\\)itemFor frequency distribution xth percentile given \nfollowing stepsFind cumulative frequenciesFind cumulative frequenciesFind \\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)Find \\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)\ncorresponding class interval called Percentile class.See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{100} \\right)\\)\ncorresponding class interval called Percentile class.Use following formulaUse following formula\\[\\mathbf{P}_{\\mathbf{x}}\\mathbf{= l +}\\frac{\\left( \\frac{\\mathbf{x \\times n}}{\\mathbf{100}} \\right)\\mathbf{- cf}}{\\mathbf{f}}\\mathbf{\\times c}\\]\\(\\mathbf{l}\\) = lower limit percentile class\\(\\mathbf{\\text{cf}}\\) = cumulative frequency preceding percentile\nclass\\(\\mathbf{f}\\) = frequency percentile class\\(\\mathbf{c}\\) = class interval\\(\\mathbf{n}\\) = total number observationsExample 4.9: Compute \\(\\mathbf{P}_{\\mathbf{25}}\\)\n\\(\\mathbf{P}_{\\mathbf{75}}\\) data 25, 18, 30, 8, 15, 5, 10, 35,\n40, 45Solution 4.9:First arrange data ascending order5, 8, 10, 15, 18, 25, 30, 35, 40, 45Here n =10\\(\\mathbf{P}_{\\mathbf{25}}\\mathbf{=}\\left( \\frac{\\mathbf{25}\\left( \\mathbf{10 + 1} \\right)}{\\mathbf{100}} \\right)^{\\mathbf{\\text{th}}}\\)=\n2.75th item\\(P_{25} =\\)2.75th item = 2nd item + 0.75(3rd item ‚Äì 2nd\nitem)given data \\(P_{25}\\)= 8+0.75(10‚Äì 8) = 9.5\\(\\mathbf{P}_{\\mathbf{75}}\\mathbf{=}\\left( \\frac{\\mathbf{75}\\left( \\mathbf{10 + 1} \\right)}{\\mathbf{100}} \\right)^{\\mathbf{\\text{th}}}\\)=\n8.25th itemi.e. \\(P_{75} = \\left( 75 \\times \\frac{10 + 1}{100} \\right)^{th}\\) =\n8.25th item = 8th item + 0.25(9th item ‚Äì8th item) =\n35+0.25(40-35) =36.25Note: Data example Example 3.6; can seen\n\\(P_{25} = Q_{1}\\) & \\(P_{75} = Q_{3}\\) always","code":""},{"path":"measures-of-central-tendency--ii.html","id":"deciles","chapter":"4 Measures of Central Tendency -II","heading":"4.5.3 Deciles","text":"Deciles similar quartiles. quartiles three points\ndivide ordered set data four quarters, deciles 9\npoints divide ordered set data ten equal parts. \nxth decile denoted \\(\\text{d}_{x}\\). may noted \nmedian 5thdecile.\\(\\mathbf{d}_{\\mathbf{x}}\\mathbf{=}\\left( \\frac{\\mathbf{x}\\left( \\mathbf{n + 1} \\right)}{\\mathbf{10}} \\right)^{\\mathbf{\\text{th}}}\\)itemFor frequency distribution xth decile given following\nstepsFind cumulative frequenciesFind cumulative frequenciesFind \\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)Find \\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)corresponding\nclass interval called decile class.See cumulative frequencies, value just greater\n\\(\\left( \\frac{\\text{x.n}}{10} \\right)\\)corresponding\nclass interval called decile class.Use following formulaUse following formula\\[\\mathbf{d}_{\\mathbf{x}}\\mathbf{= l +}\\frac{\\left( \\frac{\\mathbf{x \\times n}}{\\mathbf{10}} \\right)\\mathbf{- cf}}{\\mathbf{f}}\\mathbf{\\times c}\\]\\(\\mathbf{l}\\) = lower limit decile class\\(\\mathbf{\\text{cf}}\\) = cumulative frequency preceding decile class\\(\\mathbf{f}\\) = frequency decile class\\(\\mathbf{c}\\) = class interval\\(\\mathbf{n}\\) = total number observations¬†\n¬†\n¬†","code":""},{"path":"measures-of-dispersion.html","id":"measures-of-dispersion","chapter":"5 Measures of Dispersion","heading":"5 Measures of Dispersion","text":"discussed previous chapters, set data can summarized\nsingle representative value describes central value \ndata. Consider two sets data & B belowYou can see mean, median mode sets & B 3See dot diagrams data sets B.\nFigure 5.1: Scatter diagram data sets & B\ncan seen , values data set grouped close \nmean, values data set B spread . \nsay values data set B dispersed (scattered) \ndata set AThis example shows mean, mode median, \nenough describing set data. addition using measures,\nneed numerical measure dispersion (variation) set \ndata.Statistical dispersion means extent numerical data \nlikely vary average value.","code":""},{"path":"measures-of-dispersion.html","id":"characteristics-of-a-good-measure-of-dispersion","chapter":"5 Measures of Dispersion","heading":"5.1 Characteristics of a good measure of dispersion","text":"ideal measure dispersion expected possess following\npropertiesIt rigidly defined.rigidly defined.based items.based items.unduly affected extreme items.unduly affected extreme items.lend algebraic manipulation.lend algebraic manipulation.simple understand easy calculate.simple understand easy calculate.important measures dispersion Range, Quartile\ndeviation, Variance, Inter-quartile range, Mean Absolute\nDeviation (MAD)standard deviation.","code":""},{"path":"measures-of-dispersion.html","id":"the-range","chapter":"5 Measures of Dispersion","heading":"5.2 The Range","text":"simplest possible measure dispersion. range set\ndata defined difference largest observation \nsmallest observation set data.Thus,Range = largest observation ‚Äì smallest observation.symbols, Range = L ‚Äì S.L = Largest value; S = Smallest value.individual observations discrete series, L S easily\nidentified. continuous series, following two methods \nfollowed.","code":""},{"path":"measures-of-dispersion.html","id":"method-1","chapter":"5 Measures of Dispersion","heading":"5.2.1 Method 1","text":"L = Upper boundary highest classS = Lower boundary lowest class.\\[Range = L - S\\]Example 5.1: marks obtained 8 students Mathematics \nPhysics examinations follows:Mathematics: 35, 60, 70, 40, 85, 96, 55, 65.Physics: 50, 55, 70, 65, 89, 68, 72, 80.Find ranges two sets data. Physics marks \ndispersed Mathematics marks?SolutionFor Mathematics,Highest mark = 96, lowest mark = 35, range =96 ‚Äì 35 = 61For Physics,Highest mark = 89, lowest mark = 50, range =89 ‚Äì 50 = 39.mathematics marks wider range Physics marks. \nMathematics marks therefore dispersed Physics marks.Example 5.2: Calculate range following distributionSolutionL = Upper boundary highest class = 75S = Lower boundary lowest class = 60Range = L ‚Äì S = 75 ‚Äì 60 = 15","code":""},{"path":"measures-of-dispersion.html","id":"merits-and-demerits-of-range","chapter":"5 Measures of Dispersion","heading":"5.2.2 Merits and Demerits of Range","text":"MeritsIt simple understand.simple understand.easy calculate.easy calculate.certain types problems like quality control, weather\nforecasts, share price analysis, etc.certain types problems like quality control, weather\nforecasts, share price analysis, etc.DemeritsIt much affected extreme items.much affected extreme items.based two extreme observations.based two extreme observations.calculated open-end class intervals.calculated open-end class intervals.suitable mathematical treatment.suitable mathematical treatment.rarely used measure.rarely used measure.","code":""},{"path":"measures-of-dispersion.html","id":"the-inter-quartile-range-iqr-or-midspread","chapter":"5 Measures of Dispersion","heading":"5.3 The Inter-Quartile Range (IQR) or Midspread","text":"range advantage quick easy calculate.\nHowever, since depends maximum minimum values \nset data, show whole data distributed\ntwo values. range therefore good measure \ndispersion one two values differ greatly \nvalues data. overcome problem, sometimes use \ninter-quartile range. robust measure dispersion \ninter-quartile range. inter-quartile range set data \ndifference upper lower quartiles data. Thus,\\[Inter-Quartile Range (IQR) = Q_3 - Q_1\\]inter-quartile range set data therefore affected \nvalues data outside Q1 Q3. inter-quartile range\nsometimes used measure dispersion.Example 5.3: Consider two sets data , find IQRA: 3, 4, 5, 6, 8, 9, 10, 12, 15B: 3, 8, 8, 9, 9, 9, 10, 10, 15For data set , Q1 = 4.5, Q3 = 11; Inter-Quartile Range =11\n‚Äì 4.5 = 6.5For data set B, Q1 = 8, Q3 = 10; Inter-Quartile Range =10 ‚Äì\n8 = 2Since inter-quartile range data set greater \ndata set B, results confirm data set dispersed \ndata set B. can also see Range sets.","code":""},{"path":"measures-of-dispersion.html","id":"mean-absolute-deviation-mad","chapter":"5 Measures of Dispersion","heading":"5.4 Mean Absolute Deviation (MAD)","text":"mean absolute deviation (MAD) measure variability \nindicates average distance observations mean. MAD\nuses original units data, simplifies interpretation.\nLarger values signify data points spread \naverage. Conversely, lower values correspond data points bunching\ncloser . mean absolute deviation also known mean\ndeviation average absolute deviation.calculate mean absolute deviation.Calculate mean.Calculate mean.Calculate difference observation mean take\nabsolute value .e. ignore sign. difference known \nabsolute deviationCalculate difference observation mean take\nabsolute value .e. ignore sign. difference known \nabsolute deviationAdd deviations together.Add deviations together.Divide sum number data points.Divide sum number data points.\\[MAD = \\frac{\\sum_{= 1}^{n}\\left| x_{} - \\overline{x} \\right|}{n}\\]Example 5.4: find mean absolute deviation following 10,\n15, 15, 17, 18, 21Here n = 6 \\(\\sum_{= 1}^{n}\\left| x_{} - \\overline{x} \\right|\\) =\n16 therefore MAD = \\(\\frac{16}{6} = 2.67\\)","code":""},{"path":"measures-of-dispersion.html","id":"merits-and-demerits-of-mad","chapter":"5 Measures of Dispersion","heading":"5.4.1 Merits and Demerits of MAD","text":"MeritsMean deviation simple easy.Mean deviation simple easy.Different items observations can easily compared mean\ndeviation.Different items observations can easily compared mean\ndeviation.Mean deviation better quartile deviation range \nbased observations series.Mean deviation better quartile deviation range \nbased observations series.Mean deviation less affected extreme values series\ncomparing standard deviation.Mean deviation less affected extreme values series\ncomparing standard deviation.Mean deviation rigidly defined. , fixed value.Mean deviation rigidly defined. , fixed value.Mean deviation median least.Mean deviation median least.DemeritsMean deviation becomes difficult compute mean deviation case\nfractions.Mean deviation becomes difficult compute mean deviation case\nfractions.applicable algebraic calculations.applicable algebraic calculations.calculated open-end class intervals.calculated open-end class intervals.Mean deviation good measure ignores negative signs \ndeviations.Mean deviation good measure ignores negative signs \ndeviations.","code":""},{"path":"measures-of-dispersion.html","id":"the-variance-and-standard-deviation","chapter":"5 Measures of Dispersion","heading":"5.5 The variance and standard deviation","text":"important measures variability sample variance \nsample standard deviation. x1, x2‚Ä¶ xn sample\nn observations, sample variance denoted s¬≤ \ndefined equation.\\[{\\mathbf{\\text{sample variance}},\\ \\mathbf{s}}^{\\mathbf{2}}\\mathbf{=}\\frac{\\sum_{\\mathbf{= 1}}^{\\mathbf{n}}\\left( \\mathbf{x}_{\\mathbf{}}\\mathbf{-}\\overline{\\mathbf{x}} \\right)^{\\mathbf{2}}}{\\mathbf{n - 1}}\\]sample standard deviation, s, positive square root \nsample variance.\\[\\mathbf{variance =}\\left( \\mathbf{\\text{standard deviation}} \\right)^{\\mathbf{2}}\\]\\[\\mathbf{standard\\ deviation = \\ }\\sqrt{\\mathbf{\\text{variance}}}\\]Note: sA, standard deviation data set , greater\nsB, standard deviation data set B, data set \ndispersed data set B. noted standard\ndeviation set data non-negative number.Example 5.4: Consider two sets data & B ; find\nstandard deviation?Solution:Mean (\\(\\overline{x}\\)) =\\(\\ \\frac{18}{6} = 3\\)\\[{sample\\ variance,\\ s}_{}^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1} = \\frac{10}{5} = 2\\]\\[\\text{sample standard deviation,}\\ s_{} = \\ \\sqrt{s_{}^{2}} = \\ \\sqrt{2} = 1.414\\]Mean (\\(\\overline{x}\\)) =\\(\\ \\frac{18}{6} = 3\\)\\[{Sample\\ variance,\\ s}_{B}^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1} = \\frac{54}{5} = 10.8\\]\\[Sample\\ standard\\ deviation,\\ s_{B} = \\ \\sqrt{s_{B}^{2}} = \\ \\sqrt{10.8} = 3.29\\]can seen \\(s_{B} > s_{}\\), confirming data set B \ndispersed data set (see dot diagrams)Note: unit measurement sample variance square\nunit measurement data. Sample standard deviation \nunit measurement data. Thus, x measured \ncentimetres (cm), unit measurement sample variance\ncm2 sample standard deviation cm. standard\ndeviation desirable property measuring variability \nunit data.alternative formula computing varianceThe computation s¬≤ requires calculations \\(\\overline{x}\\), n\nsubtractions n squaring adding operations. original\nobservations deviations \\(\\left( x_{} - \\overline{x} \\right)\\) \nintegers, deviations \\(\\left( x_{} - \\overline{x} \\right)\\) may\ndifficult work , several decimals may carried\nensure numerical accuracy. efficient computational formula \ns¬≤ given \\(s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{x_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}x_{} \\right)^{2} \\right\\}\\)Example 5.5: Consider data set ; find standard deviation?Solution:\\(s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{x_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}x_{} \\right)^{2} \\right\\}\\)\n; n = 9\\(s^{2} = \\frac{1}{8}\\left\\{ 700 - {\\frac{1}{9}\\left( 72 \\right)}^{2} \\right\\}\\)\n=15.5\\(s = \\ \\sqrt{15.5} = 3.94\\)","code":""},{"path":"measures-of-dispersion.html","id":"variance-and-standard-deviation-for-grouped-data","chapter":"5 Measures of Dispersion","heading":"5.5.1 Variance and standard deviation for grouped data","text":"","code":""},{"path":"measures-of-dispersion.html","id":"for-discrete-grouped-data","chapter":"5 Measures of Dispersion","heading":"5.5.1.1 For discrete grouped data","text":"\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}x}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}x}_{} \\right)^{2} \\right\\}\\]\\(f_{}\\) frequency ith observationExample 5.6: frequency distributions seed yield 50 sesamum\nplants given . Find standard deviation.Solution:\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}x}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}x}_{} \\right)^{2} \\right\\}\\]\\[{sample\\ variance,\\ s}^{2} = \\frac{1}{50 - 1}\\left\\{ 1537 - \\frac{271^{2}}{50} \\right\\} = 1.3914\\]\\(standard\\ deviation,\\ s = \\sqrt{1.3914}\\) = 1.179","code":""},{"path":"measures-of-dispersion.html","id":"for-continuous-grouped-data","chapter":"5 Measures of Dispersion","heading":"5.5.1.2 For continuous grouped data","text":"\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}d}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}d}_{} \\right)^{2} \\right\\}\\]\\(f_{}\\) frequency ith class, c class\ninterval, \\(d_{} = \\frac{x_{} - }{c}\\), \\(x_{}\\) class mark, \\(\\)\nclass mark highest frequencyExample 5.7: frequency distributions seed yield 50 sesamum\nplants given . Find standard deviationSolution:n =50; c =1A = 5\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}d}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}d}_{} \\right)^{2} \\right\\}\\]\\[{sample\\ variance,\\ s}^{2} = \\frac{1}{49}\\left( 77 - \\frac{\\left( 21 \\right)^{2}}{50} \\right) = \\ 1.3914\\]\\(standard\\ deviation,\\ s = \\sqrt{1.3914}\\) = 1.179","code":""},{"path":"measures-of-dispersion.html","id":"merits-and-demerits-of-standard-deviation","chapter":"5 Measures of Dispersion","heading":"5.5.2 Merits and Demerits of Standard Deviation","text":"MeritsIt rigidly defined value always definite based \nobservations.rigidly defined value always definite based \nobservations.based arithmetic mean, merits \narithmetic mean.based arithmetic mean, merits \narithmetic mean.important widely used measure dispersion.important widely used measure dispersion.possible algebraic treatment.possible algebraic treatment.less affected fluctuations sampling hence\nstable.less affected fluctuations sampling hence\nstable.basis measuring coefficient correlation \nmeasures.basis measuring coefficient correlation \nmeasures.DemeritsIt easy understand difficult calculate.easy understand difficult calculate.gives weight extreme values values \nsquared .gives weight extreme values values \nsquared .absolute measure variability, used \npurpose comparison.absolute measure variability, used \npurpose comparison.","code":""},{"path":"measures-of-dispersion.html","id":"coefficient-of-variation-relative-measure-of-dispersion","chapter":"5 Measures of Dispersion","heading":"5.6 Coefficient of Variation (relative measure of dispersion)","text":"Standard deviation absolute measure dispersion. \nexpressed terms units original figures collected\nstated. standard deviation heights plants \ncompared standard deviation weights grains, \nexpressed different units, .e heights centimetre \nweights kilograms.Therefore standard deviation must converted relative\nmeasure dispersion purpose comparison. relative\nmeasure known coefficient variation. coefficient \nvariation obtained dividing standard deviation mean \nexpressed percentage.\\[\\mathbf{\\text{Coefficient variation}}\\left( \\mathbf{C}\\mathbf{.}\\mathbf{V} \\right)\\mathbf{=}\\frac{\\mathbf{\\text{standard deviation}}}{\\mathbf{\\text{mean}}}\\mathbf{\\times 100}\\]want compare variability two series, can use\nC.V. series groups data C.V. greater indicate\ngroup variable, less stable, less uniform, less\nconsistent less homogeneous. C.V. less, indicates \ngroup less variable stable uniform \nconsistent homogeneous.Example 5.8: Consider measurement yield plant height \npaddy variety. mean standard deviation yield 50 kg \n10 kg respectively. mean standard deviation plant height \n55 cm 5 cm respectively. Compare variability.Solution:measurements yield plant height different units.\nHence variability can compared using coefficient \nvariation.yield, CV=\\(\\ \\frac{10}{50} \\times 100 =\\) 20%plant height, CV= \\(\\frac{5}{55} \\times 100 =\\) 9.1%yield subject variation plant height.¬†\n¬†\n¬†","code":""},{"path":"skewness-and-kurtosis.html","id":"skewness-and-kurtosis","chapter":"6 Skewness and Kurtosis","heading":"6 Skewness and Kurtosis","text":"previous chapter learned numerical measures central tendency dispersion, measures shape?histogram can give general idea shape distribution values data. need numerical measures identify shape distribution. numerical measures deal shape distribution Skewness Kurtosis.","code":""},{"path":"skewness-and-kurtosis.html","id":"skewness","chapter":"6 Skewness and Kurtosis","heading":"6.1 Skewness","text":"Skewness measure symmetry, precisely, lack symmetry. may ask, symmetric distribution looks like. Histogram symmetric distribution showed :\nFigure 6.1: Histogram symmetric distribution\ndistribution, data set, symmetric looks left right centre point. discussion including unimodal cases.symmetric distribution skewness = 0; mean = median = mode\nFigure 6.2: symmetric distribution\nExample data set skewness = 0 (symmetric distribution)\nFigure 6.3: Data set skewness = 0\n","code":""},{"path":"skewness-and-kurtosis.html","id":"left-skewed-or-negatively-skewed","chapter":"6 Skewness and Kurtosis","heading":"6.1.1 Left-skewed or negatively skewed","text":"negatively skewed data set distribution, left tail longer; mass distribution concentrated right figure. distribution said left-skewed, left-tailed, skewed left, considering long tail left side. See figure , can also see Mean < Median < Mode.\nFigure 6.4: Left skewed negatively skewed distribution\nExample data set negative skewness\nFigure 6.5: Negatively skewed data set\n","code":""},{"path":"skewness-and-kurtosis.html","id":"right-skewed-or-positively-skewed","chapter":"6 Skewness and Kurtosis","heading":"6.1.2 Right-skewed or positively skewed","text":"positively skewed data set distribution, right tail longer; mass distribution concentrated left figure. distribution said right-skewed, right-tailed, skewed right, considering long tail right side. See figure , can also see Mean > Median > Mode\nFigure 6.6: Right skewed positively skewed distribution\nExample data set positive skewness\nFigure 6.7: Data set positive skewness (right skewed)\n","code":""},{"path":"skewness-and-kurtosis.html","id":"measures-of-skewness","chapter":"6 Skewness and Kurtosis","heading":"6.2 Measures of Skewness","text":"direction extent skewness can measured various ways. shall discuss four measures.","code":""},{"path":"skewness-and-kurtosis.html","id":"karl-pearsons-coefficient-of-skewness-s_k","chapter":"6 Skewness and Kurtosis","heading":"6.2.1 Karl Pearson‚Äôs coefficient of Skewness (\\(S_{k}\\))","text":"noticed mean, median mode equal skewed distribution. Karl Pearson‚Äôs measure skewness based upon divergence mean mode skewed distribution.\\[S_{k} = \\frac{mean - mode}{\\text{standard deviation}}\\]sign \\(S_{k}\\) gives direction skewness magnitude gives extent skewness. \\(S_{k}\\) > 0, distribution positively skewed, \\(S_{k}\\) < 0 negatively skewed.formula since mode used, problem mode defined distribution find \\(S_{k}\\). empirical relation mean, median mode states , moderately symmetrical distribution \\(\\ mean - mode \\approx 3(mean - median)\\). formula can written \\[S_{k} = \\frac{3(mean - median)}{\\text{standard deviation}}\\]Example 6.1: Compute Karl Pearson‚Äôs coefficient skewness following data:Mean, \\(\\overline{x} = \\frac{\\sum_{= 1}^{n}{f_{}x_{}}}{\\sum_{= 1}^{n}f_{}}\\) = \\(\\frac{11482}{187} = 61.40\\)\\({sample\\ variance,\\ s}^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{= 1}^{n}{{f_{}x}_{}^{2} - \\frac{1}{n}}\\left( \\sum_{= 1}^{n}{f_{}x}_{} \\right)^{2} \\right\\}\\)= \\(\\frac{705588 - \\frac{\\left( 11482 \\right)^{2}}{187}}{186} = 3.0976\\)\\(standard\\ deviation,\\ s = \\sqrt{3.0976} = 1.76\\)Median: See cumulative frequencies, value just greater \\(\\ \\left( \\frac{n + 1}{2} \\right)\\) , corresponding value \\(x\\) \\(Q_{2}\\), median\\(\\left( \\frac{n + 1}{2} \\right) = \\frac{187 + 1}{2}= \\frac{188}{2}\\) = 94\\[S_{k} = \\frac{3(mean - median)}{\\text{standard deviation}}\\]\\[S_{k} = \\frac{3(61.40 - 61)}{1.179} = \\frac{1.2}{1.179} = 0.68\\]Hence, Karl Pearson‚Äôs coefficient skewness \\(S_{k}\\)=\\(1.017\\), Thus distribution positively skewed.","code":""},{"path":"skewness-and-kurtosis.html","id":"bowleys-measure-of-skewness-sq","chapter":"6 Skewness and Kurtosis","heading":"6.2.2 Bowley‚Äôs measure of Skewness (SQ)","text":"Karl Pearson‚Äôs coefficient skewness commonly used skewness measure. However, order use must know mean, mode (median) standard deviation data. Sometimes might information; instead might information quartiles. ‚Äôs case, can use Bowley‚Äôs measure Skewness alternative find asymmetry distribution. ‚Äôs useful extreme data values (outliers) open-ended distribution.\\[{Bowley‚Äôs\\ measure\\ \\ Skewness,\\ S}_{Q} = \\frac{\\left( Q_{3} - Q_{2} \\right) - \\left( Q_{2} - Q_{1} \\right)}{\\left( Q_{3} - Q_{2} \\right) + \\left( Q_{2} - Q_{1} \\right)}\\]\\(Q_{1}\\)= 1st quartile; \\(Q_{2}\\) = median; \\(Q_{3}\\)= 3rd quartileEquation can modified \\[S_{Q} = \\frac{Q_{3} - 2Q_{2} + Q_{1}}{Q_{3} - Q_{1}}\\]\\(S_{Q}\\)= 0 means curve symmetrical.\\(S_{Q}\\)= 0 means curve symmetrical.\\(S_{Q}\\) > 0 means curve positively skewed.\\(S_{Q}\\) > 0 means curve positively skewed.\\(S_{Q}\\)< 0 means curve negatively skewed.\\(S_{Q}\\)< 0 means curve negatively skewed.Example 6.1 given , Bowley‚Äôs measure Skewness can calculated followsCalculation \\(\\text{Q}_{1}\\), \\(Q_{2}\\), \\(Q_{3}\\) given Section 4.5.1\\[{Q}_{1} = 60\\]\\[Q_{2} = 61\\]\\[Q_{3} = 63\\]\\[S_{Q} = \\frac{63 - (2 \\times 61) + 60}{63 - 60} = \\ \\frac{1}{3} = 0.33\\]Since \\(S_{Q}\\) > 0 means curve positively skewed.","code":""},{"path":"skewness-and-kurtosis.html","id":"kellys-measure-of-skewness-sp","chapter":"6 Skewness and Kurtosis","heading":"6.2.3 Kelly‚Äôs Measure of Skewness (Sp)","text":"Bowley‚Äôs measure skewness based middle 50% observations; leaves 25% observations extreme distribution. improvement Bowley‚Äôs measure, Kelly suggested measure based Percentiles, including P10 P90 10% observations extreme ignored.\\[{Kelly's\\ Measure\\ \\ Skewness,\\ S}_{p} = \\frac{\\left( P_{90} - P_{50} \\right) - \\left( P_{50} - P_{10} \\right)}{\\left( P_{90} - P_{50} \\right) + \\left( P_{50} - P_{10} \\right)}\\]","code":""},{"path":"skewness-and-kurtosis.html","id":"measure-based-on-moments","chapter":"6 Skewness and Kurtosis","heading":"6.2.4 Measure based on moments","text":"going measuring skewness using moments, one know moment :","code":""},{"path":"skewness-and-kurtosis.html","id":"moments","chapter":"6 Skewness and Kurtosis","heading":"6.2.4.1 Moments","text":"rthmoment mean distribution, denoted Œºr given \\[\\mu_{r} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{r}}}{N}\\]\\(f_{}\\) frequency ith observation class mark\\(\\ x_{}\\), \\(N = \\sum_{}^{}f_{}\\), number observationsMoment mean also called Central MomentIf r = 0, \\(\\mu_{0} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{0}}}{N}\\) = 1If r = 1, \\(\\mu_{1} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{1}}}{N}\\) = 0 (sum deviation mean zero)r = 2, \\(\\mu_{2} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{2}}}{N}\\) = \\(\\sigma^{2}\\), Population varianceIn short values following moments mean areMoments mean(central moment)Example 6.1 given , calculate third central moment, \\(\\mu_{3}\\)Mean = 61.40\\[\\mu_{3} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{3}}}{N} = \\ \\frac{49.832}{187} = 0.266\\]","code":""},{"path":"skewness-and-kurtosis.html","id":"moment-measure-of-skewness-mathbfbeta_1textandgamma_1mathbf","chapter":"6 Skewness and Kurtosis","heading":"6.2.4.2 Moment measure of skewness \\(\\mathbf{(}\\beta_{1}\\text{and}\\)\\(\\gamma_{1}\\mathbf{)}\\)","text":"moment measure skewness based property , symmetrical distribution, odd ordered central moments equal zero. note \\(\\mu_{1}\\) = 0, every distribution, therefore, lowest order moment can provide absolute measure skewness \\(\\text{Œº}_{3}\\). measures skewness based \\(\\text{Œº}_{3}\\).\\[\\beta_{1} = \\frac{\\mu_{3}^{2}}{\\mu_{2}^{3}}\\]Pronounced ‚Äòbeta one‚Äô\\(\\beta_{1}\\)= 0 means curve symmetrical. greater value \\(\\beta_{1}\\)skewed distribution. One serious limitation \\(\\beta_{1}\\)tell direction skewness, .e., whether positive negative. Since \\(\\text{Œº}_{2}\\) always positive \\(\\mu_{3}^{2}\\) positive, \\(\\beta_{1}\\) positive always. drawback removed calculating\\(\\text{Œ≥}_{1}\\), called Karl Pearson‚Äôs\\(\\text{ Œ≥}_{1}\\), pronounced ‚Äògamma one‚Äô.\\[\\gamma_{1} = \\sqrt{\\beta_{1}} = \\frac{\\mu_{3}}{\\mu_{2}^{3}}\\]\\(\\mu_{3}\\) positive \\(\\gamma_{1}\\) positive, \\(\\mu_{3}\\) negative \\(\\gamma_{1}\\) negative\\(\\gamma_{1}\\)= 0 means curve symmetrical.\\(\\gamma_{1}\\)= 0 means curve symmetrical.\\(\\gamma_{1}\\) > 0 means curve positively skewed.\\(\\gamma_{1}\\) > 0 means curve positively skewed.\\(\\gamma_{1}\\)< 0 means curve negatively skewed.\\(\\gamma_{1}\\)< 0 means curve negatively skewed.Example 6.1 given , skewness can examined \\(\\mu_{3}\\)= 0.226\\(\\mu_{2}\\)= 3.123\\(\\beta_{1} = \\frac{\\mu_{3}^{2}}{\\mu_{2}^{3}}\\) = \\(\\frac{\\left( 0.226 \\right)^{2}}{\\left( 3.123 \\right)^{3}} = \\ \\frac{0.051}{30.46} = 0.0016\\)\\(\\gamma_{1} = \\sqrt{\\beta_{1}} = \\ \\sqrt{0.0016} = + 0.04\\)Since \\(\\mu_{3}\\) positive \\(\\gamma_{1}\\)positive. Since \\(\\gamma_{1}\\)slightly greater 0, distribution slightly skewed right.","code":""},{"path":"skewness-and-kurtosis.html","id":"kurtosis","chapter":"6 Skewness and Kurtosis","heading":"6.3 Kurtosis","text":"Kurtosis another measure shape distribution. Whereas skewness measures lack symmetry frequency curve distribution, kurtosis measure relative peakedness frequency curve. Various frequency curves can divided three categories depending upon shape peak.\nFigure 6.8: Three categories frequency curves depending upon shape peak\nKurtosis refers degree flatness peakedness curve. measured relative peakedness normal curve. normal curve considered mesokurtic. curve peaked normal curve, called leptokurtic. curve flat-topped normal curve, called platykurtic.condition peakedness (leptokurtic) flatness (platykurtic) called kurtosis excess.Measure kurtosis given ‚Äòbeta two‚Äô given Karl Pearson\\(\\beta_{2} = \\frac{\\mu_{4}}{\\mu_{2}^{2}}\\)\\(\\mu_{4}\\) 4th central moment, \\(\\mu_{2}\\) 2nd central moment\\(\\beta_{2}\\)= 3 means curve mesokurtic.\\(\\beta_{2}\\)= 3 means curve mesokurtic.\\(\\beta_{2}\\) > 3 means curve leptokurtic.\\(\\beta_{2}\\) > 3 means curve leptokurtic.\\(\\beta_{2}\\)< 3 means curve platykurtic.\\(\\beta_{2}\\)< 3 means curve platykurtic.Another measure kurtosis gamma two, \\(\\gamma_{2} = \\beta_{2} - 3\\)\\(\\gamma_{2}\\)= 0 means curve mesokurtic.\\(\\gamma_{2}\\)= 0 means curve mesokurtic.\\(\\gamma_{2}\\) > 0 means curve leptokurtic.\\(\\gamma_{2}\\) > 0 means curve leptokurtic.\\(\\gamma_{2}\\)< 0 means curve platykurtic.\\(\\gamma_{2}\\)< 0 means curve platykurtic.Example 6.1 given , kurtosis can examined followsMean, \\(\\overline{x}\\)= 61.40\\(\\mu_{2}\\) = 3.123 (calculation shown previous example)\\(\\mu_{4} = \\frac{\\sum_{= 1}^{N}{f_{}\\left( x_{} - \\overline{x} \\right)^{4}}}{N} = \\frac{4312.747}{187} = 23.062\\)\\(\\beta_{2} = \\frac{\\mu_{4}}{\\mu_{2}^{2}} = \\frac{23.062}{\\left( 3.123 \\right)^{2}} = 2.364\\)\\(\\beta_{2}\\) 2.364, close 3, distribution can considered slightly platykurtic close symmetric.can verify frequency curve Example 6.1 , can seen slightly right tailed (positively skewed)\nFigure 6.9: frequency curve Example 6.1\n¬†\n¬†\n¬†","code":""},{"path":"measures-of-association.html","id":"measures-of-association","chapter":"7 Measures of Association","heading":"7 Measures of Association","text":"","code":""},{"path":"measures-of-association.html","id":"scatter-diagram","chapter":"7 Measures of Association","heading":"7.1 Scatter Diagram","text":"Consider two variables x y, use scatter diagram \ninvestigate whether relation two variables. \nvariables x y plotted along X-axis Y-axis\nrespectively X-Y plane graph sheet resultant diagram \ndots known scatter diagram. scatter diagram can\nsay whether association X Y.Example 7.1: Consider data Sepal length (x) Sepal width\n(y) Iris setosa.\nFigure 7.1: Scatter diagram data Example 7.1\n","code":""},{"path":"measures-of-association.html","id":"correlation","chapter":"7 Measures of Association","heading":"7.2 Correlation","text":"Correlation statistical technique used analyzing behaviour\ntwo variables. correlation measures degree \ncloseness linear relationship two variables numerical\nmagnitude.Correlation measure enable us compare linear relationship\ntwo variables using single number. two \nquantities vary related manner movements one tend \naccompanied movements , said \ncorrelated.","code":""},{"path":"measures-of-association.html","id":"positive-correlation","chapter":"7 Measures of Association","heading":"7.2.1 Positive correlation","text":"Positive correlation relationship two variables \nvariables move direction. positive correlation exists\none variable decreases variable decreases, one\nvariable increases increases.Examples positive correlation: consider two variables x yThe time spend running treadmill, (Running time\n(x)), calories burn, (calories burned (y)).\ncan see x increases y also increases.time spend running treadmill, (Running time\n(x)), calories burn, (calories burned (y)).\ncan see x increases y also increases.Shorter people (Height (x)) smaller shoe sizes (shoe size\n(y)). can see x decreases y also decreases.Shorter people (Height (x)) smaller shoe sizes (shoe size\n(y)). can see x decreases y also decreases.hours spend direct sunlight (Hours sunlight\n(x)), tan (melanin content(y)). can\nsee x increases y also increases.hours spend direct sunlight (Hours sunlight\n(x)), tan (melanin content(y)). can\nsee x increases y also increases.temperature goes (Temperature (x)), ice cream sales\n(sales (y)), also go .temperature goes (Temperature (x)), ice cream sales\n(sales (y)), also go .","code":""},{"path":"measures-of-association.html","id":"negative-correlation","chapter":"7 Measures of Association","heading":"7.2.2 Negative correlation","text":"Negative correlation relationship two variables \none variable increases decreases, vice versa.Examples negative correlation: consider two variables x yA student many absences (.¬†days absent (x)) \ndecrease grades (grades (x)). can see x increases\ny decreases.student many absences (.¬†days absent (x)) \ndecrease grades (grades (x)). can see x increases\ny decreases.weather gets colder (Average monthly temperature (x)), air\nconditioning costs decrease (Price .C (y)).weather gets colder (Average monthly temperature (x)), air\nconditioning costs decrease (Price .C (y)).chicken increases age (chicken age (x)), number \neggs produces (.¬†eggs produced (y)) decreases.chicken increases age (chicken age (x)), number \neggs produces (.¬†eggs produced (y)) decreases.car decreases speed (average car speed(x)), travel time (y)\ndestination increases.car decreases speed (average car speed(x)), travel time (y)\ndestination increases.","code":""},{"path":"measures-of-association.html","id":"other-types-of-correlation","chapter":"7 Measures of Association","heading":"7.3 Other types of correlation","text":"","code":""},{"path":"measures-of-association.html","id":"simple-and-multiple","chapter":"7 Measures of Association","heading":"7.3.1 Simple and Multiple","text":"simple correlation relationship confined two\nvariables . multiple correlation relationship\ntwo variables judged.","code":""},{"path":"measures-of-association.html","id":"linear-and-non-linear-correlation","chapter":"7 Measures of Association","heading":"7.3.2 Linear and Non linear correlation","text":"","code":""},{"path":"measures-of-association.html","id":"partial-and-total","chapter":"7 Measures of Association","heading":"7.3.3 Partial and total","text":"two types correlations multiple correlation analysis.partial correlation relationship two \nvariables examined eliminating linear effect \ncorrelated variables.total correlation based relevant variables.Correlation measures linear relationship variables","code":""},{"path":"measures-of-association.html","id":"linear-relationship","chapter":"7 Measures of Association","heading":"7.4 Linear relationship","text":"linear relationship (linear association) statistical term used\ndescribe straight-line relationship variables.Linear relationships can expressed either graphical format \nvariable plotted X-Y plane gives straight line relation\ntwo variables (consider x y) can expressed \nequation straight line (y = + bx) (clear \ndiscuss regression)Example 7.2: Consider following example ice cream salesThe local ice cream shop keeps track much ice cream sell\nversus temperature day; figures last\n12 days:rest discussion using example .","code":""},{"path":"measures-of-association.html","id":"methods-of-measurement-of-correlation","chapter":"7 Measures of Association","heading":"7.5 Methods of measurement of correlation","text":"","code":""},{"path":"measures-of-association.html","id":"scatter-diagram-or-graphic-method","chapter":"7 Measures of Association","heading":"7.5.1 Scatter diagram or Graphic method","text":"\nFigure 7.2: Scatter plot Example 7.2\nFigure 7.2 can see linear association \ntwo variables .e. temperature ice cream sales. can\nshown using line . clear temperature\nincreases sales increases, indicating positive correlation.\nFigure 7.3: Linear relationship variables\nexample clear scatter diagram gives idea \nlinear association variables, can also used graphical\ntool see whether correlation present .Perfect Correlation: change value one\nvariable, value variable changed fixed\nproportion correlation said perfect\ncorrelation. perfect correlation, points lie \nstraight line. scatter diagram Example 7.2, can see\nperfect linear relationship points \nexactly line, points scattered form \nstill direction (positive).Direction correlation can identified using scatter diagram \nshown Figure 7.4\nFigure 7.4: Scatter plot nature relationship\n","code":""},{"path":"measures-of-association.html","id":"karl-pearsons-coefficient-of-correlation-r","chapter":"7 Measures of Association","heading":"7.5.2 Karl Pearson‚Äôs coefficient of Correlation (r)","text":"important widely used measure correlation. \nmeasure intensity degree linear relationship two\nvariables developed Karl Pearson, British Biometrician - known\nPearson‚Äôs Correlation coefficient denoted \nr expressed ratio covariance \nproduct standard deviations two variables.","code":""},{"path":"measures-of-association.html","id":"covariance","chapter":"7 Measures of Association","heading":"7.5.2.1 Covariance","text":"Covariance measure joint linear variability two\nvariables. Consider two variables x y n observations\n, covariance given formulaCovariance (x,y) =\n\\(\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}\\)covariance = 0 joint variability linear\nrelationship. unit covariance product units \ntwo variables.Covariance two variables x y denoted Cov(x, y).\nCovariance measure used find correlation coefficient.correlation coefficient two variables (x y) \ncalculated \\[r=\\frac{cov(x,y)}{sd(x)sd(y)}\\]sd. standard deviation.\\[r = \\frac{\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}}{\\sqrt{\\frac{1}{n}\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}\\frac{1}{n}\\sum_{= 1}^{n}\\left( y_{} - \\overline{y} \\right)^{2}}}\\]","code":""},{"path":"measures-of-association.html","id":"properties-of-the-correlation-coefficient-r","chapter":"7 Measures of Association","heading":"7.5.2.2 Properties of the correlation coefficient (r)","text":"pure number independent origin scale \nunits observations.pure number independent origin scale \nunits observations.always lies ‚àí1 +1 (absolute value exceed\nunity). ‚àí1 ‚â§ r ‚â§ +1It always lies ‚àí1 +1 (absolute value exceed\nunity). ‚àí1 ‚â§ r ‚â§ +1r = +1, indicates perfect positive correlation. r = ‚àí1,\nindicates perfect negative correlation. r = 0, indicates \ncorrelation.r = +1, indicates perfect positive correlation. r = ‚àí1,\nindicates perfect negative correlation. r = 0, indicates \ncorrelation.correlation zero linear relationship\nvariables.correlation zero linear relationship\nvariables.meaningful relation variables value\ncorrelation obtained also meaningless. (example \nfertilizer price increases, Kohili‚Äôs batting average also increases,\nknow practical relationship variables,\nstill may get correlation measure called spurious\ncorrelation)meaningful relation variables value\ncorrelation obtained also meaningless. (example \nfertilizer price increases, Kohili‚Äôs batting average also increases,\nknow practical relationship variables,\nstill may get correlation measure called spurious\ncorrelation)Simplified formula computation correlation coefficient can\nderived modifying formulaA Simplified formula computation correlation coefficient can\nderived modifying formula\\[r = \\frac{n\\left( \\sum_{= 1}^{n}{x_{}y_{}} \\right) - \\sum_{= 1}^{n}{x_{}\\sum_{= 1}^{n}y_{}}}{\\sqrt{\\left\\lbrack n\\sum_{= 1}^{n}{x_{}^{2} - \\left( \\sum_{= 1}^{n}x_{} \\right)^{2}} \\right\\rbrack\\left\\lbrack n\\sum_{= 1}^{n}{y_{}^{2} - \\left( \\sum_{= 1}^{n}y_{} \\right)^{2}} \\right\\rbrack}}\\]Example 7.3: Consider Example 7.2 ice cream sales; find\ncorrelation coefficient (r)n =12\\[mean,\\overline{x} = \\ \\frac{224.1}{12} = 18.675\\]\\[mean,\\overline{y} = \\ \\frac{4829}{12} = 402.416\\]\nCov (x,y) =\n\\(\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}\\)\\(\\sum_{= 1}^{12}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)} = 5325.03\\)Cov (x,y) = \\(\\frac{5325.03}{12} = 443.752\\)\\[Standard\\ deviation,\\ S.D\\left( x \\right) = \\ \\sqrt{\\frac{1}{n}\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}} = \\sqrt{\\frac{176.983}{12}} = 3.840\\]\\[Standard\\ deviation,\\ S.D\\left( y \\right) = \\ \\sqrt{\\frac{1}{n}\\sum_{= 1}^{n}\\left( y_{} - \\overline{y} \\right)^{2}} = \\sqrt{\\frac{174754.9}{12}} = 120.676\\]\\(r = \\frac{443.752}{3.840\\  \\times 120.676} = 0.95751\\), indicates\nstrong positive correlation","code":""},{"path":"measures-of-association.html","id":"spearmans-rank-order-correlation-coefficient-œÅ","chapter":"7 Measures of Association","heading":"7.5.3 Spearman‚Äôs Rank order correlation coefficient (œÅ)","text":"Spearman correlation evaluates monotonic relationship \ntwo continuous ordinal variables.Note: monotonic relation?monotonic relationship, variables tend move \nrelative direction, necessarily constant rate. linear\nrelationship, variables move direction constant\nrate.Linear relationship monotonic monotonic relations \nlinear. can see plots better understanding.\nFigure 7.5: Linear Monotonic relationship\nSpearman correlation coefficient based ranked values \nvariable rather raw data. spearman correlation\nmeasures monotonic relationship variables, pearsons\ncorrelation coefficient measures linear relationship . use\nSpearman‚Äôs correlation coefficient data must ordinal,\ninterval ratio scale.two cases calculating œÅ. One case tied rank\ntied rank","code":""},{"path":"measures-of-association.html","id":"no-tied-rank-case","chapter":"7 Measures of Association","heading":"7.5.3.1 No tied rank case","text":"two distinct observations value, thus \ngiven rank, said tiedThe formula Spearman rank correlation coefficient \ntied ranks :\\[\\rho = 1 - \\frac{6\\sum_{= 1}^{n}d_{}^{2}}{n\\left( n^{2} - 1 \\right)}\\]\\(d_{}\\) difference ranks ith pair \nobservationExample 7.4: Calculation Spearman‚Äôs rank correlation \ntied rank explained step step using example belowThe scores nine students physics math follows:Physics: 35, 23, 47, 17, 10, 43, 9, 6, 28Mathematics: 30, 33, 45, 23, 8, 49, 12, 4, 31Compute student‚Äôs ranks two subjects compute Spearman\nrank correlation.Step 1: Find ranks individual subject. Rank scores\ngreatest smallest; assign rank 1 highest score, 2 \nnext highest :Step 2: Add column d, data. d difference\nranks. example, first student‚Äôs physics rank 3 \nmath rank 5, difference -2. next column, square \nd values.Step 4: Sum (add ) d2 values. 4 + 4 + 1 + 0 + 1 +\n1 + 1 + 0 + 0 = 12. ‚Äôll need formula (\n\\(\\sum_{= 1}^{n}d_{}^{2}\\) just ‚Äúsum d2values, n=\n9‚Äù).Step 5: Insert values formula.\\[\\rho = 1 - \\frac{6\\sum_{= 1}^{n}d_{}^{2}}{n\\left( n^{2} - 1 \\right)}\\]\\[\\rho = 1 - \\frac{6 \\times 12}{9\\left( 81 - 1 \\right)} = 0.90\\]Spearman‚Äôs Rank Correlation set data 0.9.Spearman‚Äôs Rank Correlation also lies ‚àí1 +1 always. ‚àí1 ‚â§ œÅ\n‚â§+1","code":""},{"path":"measures-of-association.html","id":"tied-rank-case","chapter":"7 Measures of Association","heading":"7.5.3.2 Tied rank case","text":"Calculation Spearman‚Äôs rank correlation tied rank \nexplained step step using example belowExample 7.5: scores nine students physics mathematics\nfollows:Step 1: Consider marks Physics, ranked usualYou can see value 23 repeated, may equal ranks, \naverage two ranks 5 6 given ;\n\\(\\left( \\frac{5 + 6}{2}\\right)\\)= 5.5Similarly marks mathematics can see 33 repeated thrice.can see value 33 repeated thrice, average three\nranks 3, 4 5 given \\(\\left( \\frac{3 + 4 + 5}{3} \\right)\\)= 4Step 2: Change formula\\[\\rho = 1 - \\frac{6\\left( \\sum_{= 1}^{n}d_{}^{2} + T_{x} + T_{y} \\right)}{n\\left( n^{2} - 1 \\right)}\\]m individuals tied (rank), s sets\nranks X- series ,\n\\(T_{x} = \\ \\frac{1}{12}\\sum_{= 1}^{s}{m_{}\\left( m_{}^{2} - 1 \\right)}\\)example marks Physics (x) two 23 values tied\ntherefore m = 2; since one set s =1\\(T_{x} = \\ \\frac{1}{12}\\left( 2 \\times (2^{2} - 1 \\right)\\) = 0.5If w individuals tied (rank), s‚Äô sets\nranks Y- series ,\n\\(T_{y} = \\ \\frac{1}{12}\\sum_{= 1}^{s'}{w_{}\\left( w_{}^{2} - 1 \\right)}\\)example marks Mathematics (y) three 33 values tied\ntherefore w = 3; since one set s =1\\(T_{y} = \\ \\frac{1}{12}\\left( 3 \\times (3^{2} - 1 \\right)\\) = 2Step 2: Calculate d use formula\\(\\rho = 1 - \\frac{6\\left( \\sum_{= 1}^{n}d_{}^{2} + T_{x} + T_{y} \\right)}{n\\left( n^{2} - 1 \\right)} = 1 - \\frac{6 \\times \\left( 44.5 + 0.5 + 2 \\right)}{9\\left( 9^{2} - 1 \\right)} = \\ 1 - \\frac{282}{720}\\)\n= 0.60834","code":""},{"path":"measures-of-association.html","id":"kendalls-rank-correlation-coefficient-œÑ","chapter":"7 Measures of Association","heading":"7.5.4 Kendall‚Äôs Rank Correlation Coefficient (œÑ)","text":"Kendall‚Äôs rank correlation coefficient also known Kendall‚Äôs Tau \ncoefficient concordance. lies 0 1, 0 ‚â§ œÑ ‚â§ 1. \nseveral sets ranks , can used test association.k sets rankings may determine association among\nusing Kendall‚Äôs coefficient Concordance (œÑ). \nmeasure useful study reliability scorings made \nnumber Judges.Arrange data table row representing ranks\nassigned (judge), say, n number objects. Let \nk number sets rankings object given k judges. \nKendall‚Äôs coefficient concordance œÑ computed \\[\\tau = \\frac{12\\left\\lbrack \\sum_{= 1}^{n}{R_{}^{2} - \\frac{\\left( \\sum_{= 1}^{n}R_{} \\right)^{2}}{n}} \\right\\rbrack}{k^{2}n\\left( n^{2} - 1 \\right)}\\]Example 7.6: crop production competition, 10 entries farmers\nranked agricultural scientists (judges). Find degree \nagreement among scientist competition result given .Solution:k = number judges = 4n = number farmers =10\\(\\left( \\sum_{= 1}^{10}R_{} \\right)^{2}\\)= (220)2 = 48400\\(\\sum_{= 1}^{10}R_{}^{2}\\) = 5900\\[\\tau = \\frac{12\\left\\lbrack \\sum_{= 1}^{n}{R_{}^{2} - \\frac{\\left( \\sum_{= 1}^{n}R_{} \\right)^{2}}{n}} \\right\\rbrack}{k^{2}n\\left( n^{2} - 1 \\right)}\\]\\[\\tau = \\frac{12\\left\\lbrack 5900 - \\frac{48400}{10} \\right\\rbrack}{16 \\times 10\\left( 100 - 1 \\right)} = 0.803\\]Since \\(\\tau\\) nearly equal 1, ranks given judges almost\n.¬†\n¬†\n¬†","code":""},{"path":"regression-analysis.html","id":"regression-analysis","chapter":"8 Regression Analysis","heading":"8 Regression Analysis","text":"Regression analysis method using observations (data records)\nquantify relationship target variable also referred \ndependent variable, set independent variables.","code":""},{"path":"regression-analysis.html","id":"definition","chapter":"8 Regression Analysis","heading":"8.1 Definition","text":"Regression analysis mathematical measure average\nrelationship two variables terms original\nunits data","code":""},{"path":"regression-analysis.html","id":"simple-regression","chapter":"8 Regression Analysis","heading":"8.2 Simple regression","text":"two variables involved; Regression can defined \nfunctional relationship two variables, one may represent\ncause may represent effect. variable representing\ncause known independent variable denoted ‚ÄòX‚Äô. \nvariable ‚ÄòX‚Äô also known predictor variable, regressor \nexplanatory variable. variable representing effect known \ndependent variable denoted Y. example consider yield \nfertilizer dose, yield can considered dependent variable (Y)\nfertilizer dose can considered independent variable (X).","code":""},{"path":"regression-analysis.html","id":"two-types-of-variables","chapter":"8 Regression Analysis","heading":"8.3 Two types of variables","text":"regression analysis variable whose values need predicted\n(Y) called dependent variable variable used \nprediction (X) called independent variable.two independent variables present regression\ncalled Multiple Regression. two variables present\ncalled simple regression.","code":""},{"path":"regression-analysis.html","id":"detailed-explanation","chapter":"8 Regression Analysis","heading":"8.4 Detailed explanation","text":"Correlation statistical measure determines degree \nassociation two variables. Regression hand side describes\nindependent variable numerically related dependent\nvariable.Regression can simply defined technique fitting best line \nline best fit estimate value one variable basis \nanother variable. Now best line? line best fit?explaining , consider example Ice cream sales:Example 8.1: local ice cream shop keeps track much ice\ncream sell versus temperature day; \nfigures last 12 days:can use regression analysis answer following questionsWhat Ice cream sales temperature 20o Celsius?functional form relationship Temperature Ice\ncream sales?\nFigure 8.1: Scatter diagram data Example 8.1\ncan draw line denote functional relationship \ntemperature sales\nFigure 8.2: lines drawn show functional relationship\ncan see shown can draw number lines, \nbest fit line?can say best fit line line passes \npoints distance point line minimum. Using\nregression technique easily draw line. proceeding\nknow concept error residuals.","code":""},{"path":"regression-analysis.html","id":"error-and-residual","chapter":"8 Regression Analysis","heading":"8.5 Error and residual","text":"error difference observed value true value\n(true value unobserved population mean population \nsample observations taken). residual difference\nobserved value predicted value (model\nfitted line). Error measured residual can ; \nresidual considered estimate error.\nFigure 8.3: Error depicted best fit line\ndistance observation (ei) fitted line can \nconsidered residual (error). Best fit line can obtained \nminimizing distance. can achieved using mathematical\ntechnique ‚Äúprinciple least squares‚Äù.","code":""},{"path":"regression-analysis.html","id":"straight-lines","chapter":"8 Regression Analysis","heading":"8.6 Straight lines","text":"straight line simplest figure geometry.Mathematical equation straight line Y= + bX.Two important features line slope intercept. \nY-intercept, intercept line y-value point\ncrosses y-axis. b slope line, \nnumber measures ‚Äústeepness‚Äù. change Y \nunit change X along line. regression b called \nregression coefficient.Intercept ()\nFigure 8.4: Intercept line\nSlope (b)\nFigure 8.5: Slope line\nb can considered finger print line; \nvalues can easily identify line.now problem simple, find line best, estimate &\nb, error ei observation minimized. \nuse method least squares.","code":""},{"path":"regression-analysis.html","id":"method-of-least-squares","chapter":"8 Regression Analysis","heading":"8.7 Method of least squares","text":"considering error term ei; equation straight line isyi=+bxi+ei;ei ith error term corresponding yi, \n=1,2,...,nLine best fit can obtained estimating b \nminimizing error sum ‚ÄôŒ£ei‚Äô. theorem Œ£ei =0; \nb estimated minimising Œ£ei 2","code":""},{"path":"regression-analysis.html","id":"principle-of-least-squares","chapter":"8 Regression Analysis","heading":"8.8 Principle of least squares","text":"statistical method used determine line best fit \nminimizing ¬†sum squares error term Œ£ei 2yi=+bxi+ei;ei = yi ‚Äì (+bxi)ei2 = {yi ‚Äì (+bxi)}2Œ£ei2 = Œ£ {yi ‚Äì (+bxi)}2Œ£ei2 called error sum squares. minimizing\nerror sum squares, hence name principle least squares.want minimize, E = Œ£ei2 = Œ£ {yi ‚Äì\n(+bxi)}2i.e. need find b E minimumE can minimized taking derivative respect \nb equating zero. get two equations,\nequations termed normal equations solving \nnormal equations give formula b.discussing calculation part . taking derivatives \nget two equations (Normal equations) :\\[\\sum_{= 1}^{n}{y_{} = n\\mathbf{} + \\mathbf{b}\\sum_{= 1}^{n}x_{}}\\]\\[\\sum_{= 1}^{n}{y_{}x_{} = \\mathbf{}\\sum_{= 1}^{n}x_{} + \\mathbf{b}\\sum_{= 1}^{n}x_{}^{2}}\\]solving equations getRegression coefficient,\n\\[b=\\frac{\\sum_{= 1}^{n}{y_{}x_{} - \\frac{\\sum_{= 1}^{n}{y_{}\\sum_{= 1}^{n}x_{}}}{n}}}{\\sum_{= 1}^{n}x_{}^{2} - \\frac{\\left( \\sum_{= 1}^{n}x_{} \\right)^{2}}{n}}\\]\\[=\\frac{cov(x,y)}{var(x)}\\]\\[\\mathbf{b =}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(x)}}\n\\]\\[\\mathbf{= }\\overline{\\mathbf{y}}\\mathbf{- b}\\overline{\\mathbf{x}}\\]\\(\\overline{y}\\) = mean y; \\(\\overline{x}\\) = mean x","code":""},{"path":"regression-analysis.html","id":"two-lines-of-regression","chapter":"8 Regression Analysis","heading":"8.9 Two lines of regression","text":"two lines regression- y x x y.Regression y xConsider two variables x y, considering y \ndependent variable x independent variable equation\n:y = + bxThis used predict unknown value variable y value \nvariable x known. Usually b denoted byx\\[\\mathbf{b}_{\\mathbf{\\text{yx}}}\\mathbf{=}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(x)}}\\]Consider Example 8.1; considering ice cream sales dependent variable\ntemperature independent variable\nFigure 8.6: Scatter diagram data Example 8.1\nRegression x yConsider two variables x y, considering x \ndependent variable y independent variable equation\n:x= c + ; c intercept m \nslopeThis used predict unknown value variable x value \nvariable y known. Usually b denoted bxy\\[\\mathbf{b}_{\\mathbf{\\text{xy}}}\\mathbf{=}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(y)}}\\]Consider Example 8.1; considering temperature dependent variable \nice cream sales independent variable\nFigure 8.7: Scatter diagram data Example 8.1\ncan see regression different. depends \nexperimenter choose dependent independent variable. \nexample evident considering temperature dependent variable\nmeaningless, .e. usefulness predicting\ntemperature based ice cream sales?. selection dependent \nindependent variable entirely discretion experimenter based \nobjective study.","code":""},{"path":"regression-analysis.html","id":"assumptions-of-regression","chapter":"8 Regression Analysis","heading":"8.10 Assumptions of Regression","text":"y dependent variable x independent variable\nthenThe x‚Äôs non-random fixed constants.x‚Äôs non-random fixed constants.fixed value x corresponding values y \nnormal distribution mean.fixed value x corresponding values y \nnormal distribution mean.given x, variance y .given x, variance y .values y observed different levels x completely\nindependent.values y observed different levels x completely\nindependent.","code":""},{"path":"regression-analysis.html","id":"properties-of-regression-coefficients","chapter":"8 Regression Analysis","heading":"8.11 Properties of Regression coefficients","text":"correlation coefficient x y geometric\nmean two regression coefficients byx bxy\\[r = \\sqrt{b_{\\text{yx}}b_{\\text{xy}}}\\]Regression coefficients independent change origin \nscale.Regression coefficients independent change origin \nscale.one regression coefficient greater unity, \nmust less unity vice versa. .e. \nregression coefficients can less unity \ngreater unity, .e. byx >1 bxy <1\nbxy >1, byx <1.one regression coefficient greater unity, \nmust less unity vice versa. .e. \nregression coefficients can less unity \ngreater unity, .e. byx >1 bxy <1\nbxy >1, byx <1.Also one regression coefficient positive must \npositive (case correlation coefficient positive) \none regression coefficient negative must negative\n(case correlation coefficient negative).Also one regression coefficient positive must \npositive (case correlation coefficient positive) \none regression coefficient negative must negative\n(case correlation coefficient negative).","code":""},{"path":"regression-analysis.html","id":"uses-of-regression","chapter":"8 Regression Analysis","heading":"8.12 Uses of Regression","text":"Prediction: regression analysis useful predicting \nvalue one variable given value another variable. \npredictions useful difficult expensive measure\ndependent variable, Y.Identify strength relationship: regression might used\nidentify strength effect independent variable(s)\ndependent variable. Like strength relationship \ndose effect, sales marketing spending, age income.Forecast effects impact changes: , regression\nanalysis helps us understand much dependent variable changes\nchange one independent variables. typical question\n, ‚Äúmuch additional sales income get additional 1000\nspent marketing.Predicts trends future values: regression analysis can \nused predict trend future values, like ‚Äúprice \ngold 6 months?‚Äù","code":""},{"path":"regression-analysis.html","id":"example-problem","chapter":"8 Regression Analysis","heading":"8.13 Example problem","text":"Now consider example 8.1 answer questionsWhat functional form relationship Temperature \nIce cream sales?functional form relationship Temperature \nIce cream sales?Ice cream sales temperature 20o Celsius?Ice cream sales temperature 20o Celsius?SolutionFit model considering Ice cream sales dependent variable (y)\ntemperature independent variable (x). Fitting model means\nestimating b using equation.Fit model considering Ice cream sales dependent variable (y)\ntemperature independent variable (x). Fitting model means\nestimating b using equation.fitting model put 20 x value get \npredicted y valueAfter fitting model put 20 x value get \npredicted y valueModel: y = +bxn =12\\[mean,\\overline{x} = \\ \\frac{224.1}{12} = 18.675\\]\\[mean,\\overline{y} = \\ \\frac{4829}{12} = 402.416\\]Cov (x,y) =\n\\[\\frac{1}{n}\\sum_{= 1}^{n}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)}\\]\\[\\sum_{= 1}^{12}{\\left( x_{} - \\overline{x} \\right)\\left( y_{} - \\overline{y} \\right)} = 5325.03\\]\\[Cov (x,y) = \\frac{5325.03}{12} = 443.752\\]\\[variance\\ \\ x,\\ var\\left( x \\right) = \\ \\frac{1}{n}\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2} = \\frac{176.983}{12} = 14.7485\\]\\[\\mathbf{b =}\\frac{\\mathbf{cov(x,y)}}{\\mathbf{var(x)}}\\]\\[\\mathbf{b =}\\frac{443.752}{14.7485}\\mathbf{=}30.088\\]\\[\\mathbf{=}\\overline{\\mathbf{y}}\\mathbf{- b}\\overline{\\mathbf{x}}\\]\\[\\mathbf{=}402.416 - 30.088\\left( 18.675 \\right) = \\  - 159.477\\]model \\[y = \\  - 159.477 + 30.088x\\]\\[Ice\\ cream\\ sales = \\  - 159.477 + 30.088(Temperature)\\]Ice cream sales temperature 20o Celsius\\[x = 20\\]\\(y = \\  - 159.477 + 30.088(20)\\) = 442.283So predicted ice cream sales 20o Celsius 442.283.\n¬†\n¬†","code":""},{"path":"probability.html","id":"probability","chapter":"9 Probability","heading":"9 Probability","text":"Many events can‚Äôt predicted total certainty. best can say likely happen, using idea probability.","code":""},{"path":"probability.html","id":"tossing-a-coin","chapter":"9 Probability","heading":"9.1 Tossing a Coin","text":"coin tossed, two possible outcomes:heads (H) orheads (H) ortails (T)tails (T)say probability coin landing¬†H¬†¬†¬Ω probability coin landing¬†T¬†¬†¬Ω","code":""},{"path":"probability.html","id":"throwing-dice","chapter":"9 Probability","heading":"9.2 Throwing Dice","text":"single¬†die¬†thrown, six possible\noutcomes: 1, 2, 3, 4, 5, 6. probability getting one \\(\\frac{1}{6}\\)\nFigure 9.1: Image dice (plural)\n","code":""},{"path":"probability.html","id":"playing-cards","chapter":"9 Probability","heading":"9.3 Playing cards","text":"standard deck playing cards consists 52 cards divided 4 suits (Spades, Hearts, Diamonds, Clubs) 13 cards \nFigure 9.2: Image spade symbol\nSpades : - 13 cards, include 9 numbered cards 2 10 picture cards Ace, King, Queen, Jack\nFigure 9.3: Image hearts symbol\nHearts : - 13 cards\nFigure 9.4: Image diamond symbol\nDiamonds :- 13 cards\nFigure 9.5: Image clubs symbol\nClubs :- 13 cardsRed cards:- 26 cards Black cards:- 26 cards","code":""},{"path":"probability.html","id":"probability-1","chapter":"9 Probability","heading":"9.4 Probability","text":"general:Probability event happening =\n\\(\\frac{\\text{Number ways can happen}}{\\text{Total  number outcomes}}\\)","code":""},{"path":"probability.html","id":"example-1","chapter":"9 Probability","heading":"Example 1","text":"chances rolling \"4\" dieNumber ways can happen: 1¬†(1 face \"4\" )Total number outcomes: 6¬†(6 faces altogether)probability =¬†\\(\\frac{1}{6}\\)","code":""},{"path":"probability.html","id":"example-2","chapter":"9 Probability","heading":"Example 2","text":"5 marbles bag: 4 blue, 1 red. probability blue marble gets picked?Number ways can happen: 4¬†(4 blues)Total number outcomes: 5¬†(5 marbles total)probability =¬†\\(\\frac{4}{5}\\) = 0.8","code":""},{"path":"probability.html","id":"must-learn-definitions","chapter":"9 Probability","heading":"Must learn definitions","text":"Random experiment: random experiment experiment process outcome predicted certainty.Eg: Tossing coin; Tossing coin five times; Choosing card deck\ncards; Tossing die.Sample space: sample space (denoted S) random experiment set possible outcomes.Eg: Throwing coin generates sample space; S={H,T},Throwing die: S= {1,2,3,4,5,6}Sample point: Just one possible outcomes. Eg: heads, 5 Clubs cards. 6 different sample points sample space throwing \ndie.Exercise1: Check yourselfIf die tossedWhat sample space?probability getting 1?probability obtaining even number?probability getting 7?Answers given section 9.9\nFigure 9.6: Sample space throwing die\n","code":""},{"path":"probability.html","id":"event","chapter":"9 Probability","heading":"9.4.1 Event","text":"One outcomes random experimentAn event can just one outcome:Getting Tail tossing coinGetting Tail tossing coinRolling \"5\"Rolling \"5\"event can include one outcome:Choosing \"King\" deck cards (4 Kings)Choosing \"King\" deck cards (4 Kings)Rolling \"even number\" (2, 4 6)Rolling \"even number\" (2, 4 6)","code":""},{"path":"probability.html","id":"example","chapter":"9 Probability","heading":"Example","text":"Ram wants see many times \"double\" (dice number) comes throwing 2 dice.Sample Space possible Outcomes (36 Sample Points):{1,1} {1,2} {1,3} {1,4} ... {6,3} {6,4} {6,5} {6,6}Event Ram looking \"double\", dice number. made 6 Sample Points:{1,1} {2,2} {3,3} {4,4} {5,5} {6,6}","code":""},{"path":"probability.html","id":"types-of-events","chapter":"9 Probability","heading":"9.4.1.1 Types of Events","text":"Independent EventsEvents can \"Independent\", meaning event affected events.Eg: toss coin comes \"Heads\" three times. chance next toss also \"Head\"?chance simply¬†¬Ω¬†(0.5) just like toss coin. Past affect current toss.Dependent EventsBut events can \"dependent\", means can affected previous events. taking one card deck less\ncards available, probabilities change!look chances getting King deck cardsFor 1st card chance drawing King 4 52But 2nd card:1st card King, 2nd card less likely \nKing, 3 51 cards left Kings.1st card King, 2nd card slightly \nlikely King, 4 51 cards left King.NoteReplacement: put card back drawing chances change, events independent.Without Replacement: chances change, events dependent.Mutually Exclusive EventsMutually Exclusive means get events time. \neither one , bothExamples:Turning left right Mutually Exclusive (\ntime)Turning left right Mutually Exclusive (\ntime)Heads Tails Mutually ExclusiveHeads Tails Mutually ExclusiveKings Aces Mutually ExclusiveKings Aces Mutually ExclusiveWhat Mutually Exclusive?Kings Hearts Mutually Exclusive, can King Hearts!Exhaustive EventsA set events called exhaustive events together make \nentire sample space.example,Sample space tossing die \\(S\\)= {1, 2, 3, 4, 5, 6}theEvent : getting even number; {2, 4, 6}Event B: getting odd number; {1, 3, 5}exhaustive events together makes entire sample space.Equally likely eventsEqually likely events events theoretical\nprobability occurring.example: Tossing coinEvent : getting headEvent B: getting tailBoth events equal probability occurring; events termed equally likely events","code":""},{"path":"probability.html","id":"definition-of-probability","chapter":"9 Probability","heading":"9.5 Definition of Probability","text":"","code":""},{"path":"probability.html","id":"mathematical-definition","chapter":"9 Probability","heading":"9.5.1 Mathematical Definition","text":"experiment \\(n\\) exhaustive, mutually exclusive equally likely outcomes, \\(m\\) outcomes favourable happening event \\(E\\), probability \\(p\\) happening E given \\[P\\left( E \\right) = p = \\ \\frac{m}{n}\\]\\(p\\) termed probability success.","code":""},{"path":"probability.html","id":"example-3","chapter":"9 Probability","heading":"Example","text":"coin tossed, two possible outcomes Heads Tails. Outcomes exhaustive, mutually exclusive equally likely.Consider event \\(E\\) : getting headProbability event \\(E\\), \\(p(E)\\) denoted \\(p\\)\\(p\\) given definition :\\(m\\) = number outcomes favourable happening event \\(E\\) = 1\\(n\\) = number outcomes (Head Tail) = 2\\[P\\left( E \\right) = p = \\ \\frac{m}{n}\\]\\[P\\left( E \\right) = p = \\ \\frac{1}{2}\\].e. probability getting Head ¬Ωdefinition following limitationsWhat happen outcomes equally likely. example\ntossing biased die.happen outcomes equally likely. example\ntossing biased die.Probability defined total cases ‚Äòn‚Äô unknown \ntends infinity. example; probability raining\ntomorrow?Probability defined total cases ‚Äòn‚Äô unknown \ntends infinity. example; probability raining\ntomorrow?overcome limitations, definitions given","code":""},{"path":"probability.html","id":"statistical-definition","chapter":"9 Probability","heading":"9.5.2 Statistical Definition","text":"probability 'p' happening E given \\[P\\left( E \\right) = \\lim_{n \\rightarrow \\infty}\\frac{m}{n}\\]\\(n\\) number times process (e.g., tossing die) \nperformed tends infinity, \\(m\\) number times \noutcome ‚Äò\\(E\\)‚Äô happens.definition also limitationsIn cases, experiment never practice carried \n.cases, experiment never practice carried \n.leaves open question large \\(n\\) get\ngood approximation.leaves open question large \\(n\\) get\ngood approximation.overcome limitations, another approach probability \nintroduced Russian mathematician .N. Kolmogorov. approach \nprecise definition given, instead certain axioms postulates \nprobability calculations based.","code":""},{"path":"probability.html","id":"axiomatic-approach","chapter":"9 Probability","heading":"9.5.3 Axiomatic Approach","text":"Whole field probability theory based following three axiomsProbability event, P (\\(E\\)) lies 0 1.\n\\(0 \\leq P(E) \\leq 1\\)Probability event, P (\\(E\\)) lies 0 1.\n\\(0 \\leq P(E) \\leq 1\\)Probability entire sample space 1. \n\\(P\\left( S \\right) = 1\\)Probability entire sample space 1. \n\\(P\\left( S \\right) = 1\\)B mutually exclusive events probability occurrence either B denoted \\(P(\\cup B)\\) shall given \\(P\\left( \\cup B \\right) = P\\left( \\right) + P(B)\\)B mutually exclusive events probability occurrence either B denoted \\(P(\\cup B)\\) shall given \\(P\\left( \\cup B \\right) = P\\left( \\right) + P(B)\\)","code":""},{"path":"probability.html","id":"some-interesting-facts-on-probability","chapter":"9 Probability","heading":"Some interesting facts on probability","text":"Probability \\(p\\) happening event known probability success probability \\(q\\) \nnon-happening event probability failure.Probability \\(p\\) happening event known probability success probability \\(q\\) \nnon-happening event probability failure.\\(p\\) well \\(q\\) non-negative exceed unity. .e., 0 ‚â§ \\(p\\) ‚â§ 1 0 ‚â§ \\(q\\) ‚â§ 1. Thus, probability occurrence event lies 0 1 [including 0 1].\\(p\\) well \\(q\\) non-negative exceed unity. .e., 0 ‚â§ \\(p\\) ‚â§ 1 0 ‚â§ \\(q\\) ‚â§ 1. Thus, probability occurrence event lies 0 1 [including 0 1].Probability impossible event 0 sure event 1. \\(p()\\) = 1, event certainly going happen \\(p()\\) = 0, event certainly going happen.Probability impossible event 0 sure event 1. \\(p()\\) = 1, event certainly going happen \\(p()\\) = 0, event certainly going happen.number (\\(m\\)) favorable outcomes event greater total number outcomes (\\(n\\)).number (\\(m\\)) favorable outcomes event greater total number outcomes (\\(n\\)).\nFigure 9.7: Sample space throwing die\n","code":""},{"path":"probability.html","id":"additional-problems","chapter":"9 Probability","heading":"Additional Problems","text":"simultaneous toss two coins, find probability () getting 2 heads. (ii) exactly 1 head?SolutionHere, possible outcomes HH, HT, TH, TT. .e., Total number possible outcomes = 4.\\()\\) Number outcomes favorable event (2 heads) = 1 (.e.,HH).\\(p\\)(2 heads) = 1/4 .\\(ii)\\) Now event consisting exactly one head two favourable cases, namely HT TH\\(p\\)(exactly one head) = 2 /4 = 1/ 2In single throw two dice, probability sum 9?SolutionThe number possible outcomes 6 √ó 6 = 36.(1,1) (1,2) (1,3) (1,4) (1,5) (1,6)(2,1) (2,2) (2,3) (2,4) (2,5) (2,6)(3,1) (3,2) (3,3) (3,4) (3,5) (3,6)(4,1) (4,2) (4,3) (4,4) (4,5) (4,6)(5,1) (5,2) (5,3) (5,4) (5,5) (5,6)(6,1) (6,2) (6,3) (6,4) (6,5) (6,6)Event : sum 9four outcomes sum 9, (5,4), (6,3), (3,6), (4,5)Probability (sum 9) = 4/36= 1/9From bag containing 10 red, 4 blue 6 black balls, ball drawn random. probability drawing () red ball? (ii) blue ball? (iii) black ball?20 balls . , total number possible outcomes 20\\()\\) Number red balls = 10\\(p\\)(getting red ball ) = 10/20 = 1/2\\(ii)\\) Number blue balls = 4\\(p\\)(getting blue ball ) = 4/20 = 1/5\\(iii)\\) Number balls black = 14\\(p\\)(black ball ) = 14/20 = 7/10","code":""},{"path":"probability.html","id":"event-relations","chapter":"9 Probability","heading":"9.6 Event relations","text":"Consider tossing die. event getting even number, sample points 2, 4 6 favorable event . remaining sample points 1, 3 5 favorable event . Therefore, occur event occur. experiment, outcomes favorable event \ncalled complement denoted '' Ac","code":""},{"path":"probability.html","id":"event-a-or-b","chapter":"9 Probability","heading":"9.6.1 Event A or B","text":"Denoted (\\(\\cup\\) B), spelled union BLet us consider example throwing die. event getting\nmultiple 2 B another event getting multiple 3. \noutcomes 2, 4 6 favourable event outcomes 3 \n6 favourable event B..e.= {2, 4, 6}B= {3, 6}Íì¥ B = { 2, 3, 4,6}, event getting even number B another\nevent getting odd number, thenA = { 2, 4, 6 }B = { 1, 3, 5 }Íì¥ B = {1, 2, 3, 4,5,6}","code":""},{"path":"probability.html","id":"event-a-and-b","chapter":"9 Probability","heading":"9.6.2 Event A and B","text":"Denoted (Íìµ B) spelled intersection B. throwing die event getting multiple 2 B event getting multiple 3. outcomes favorable 2, 4, 6 outcomes favorable B 3, 6. 6 present B Íìµ B = 6\nFigure 9.8: Venn diagram showing intersection\n","code":""},{"path":"probability.html","id":"additive-law-of-probability","chapter":"9 Probability","heading":"9.7 Additive law of Probability","text":"two events B sample space S,\\(p\\)(AÍì¥B) = \\(p\\)()+\\(p\\)(B) ‚àí \\(p\\)(AÍìµB)mutually exclusive case \\(p\\)(AÍìµB)=0; case \\(p\\)(AÍì¥B )= \\(p\\)()+\\(p\\)(B ).","code":""},{"path":"probability.html","id":"example-4","chapter":"9 Probability","heading":"Example","text":"card drawn well-shuffled deck 52 cards. probability either spade king?denotes event drawing 'spade card'. B denotes events drawing 'king' respectively. event consists 13 sample points, whereas event B consists 4 sample points.\\(p\\)()= 13/52\\(p\\)(B)= 4/52\\(p\\)( AÍìµB) = 1/52\\(p\\)(AÍì¥B)= \\(p\\)()+\\(p\\)(B) ‚àí \\(p\\)(AÍìµB) = 13/52+4/52-1/52 = 4/13In single throw two dice, find probability total 9 11.Let events = total 9 B= total 11.Events mutually exclusive Íìµ B = 0Now \\(p\\)() = \\(p [(3, 6), (4, 5), (5, 4), (6, 3)] = 4 /36\\)\\(p\\)(B) = \\(p[(5, 6), (6, 5)] = 2 /36\\)Thus, \\(p(AÍì¥B) = 4/36 +2/36 = 6/36 = 1/6\\)","code":""},{"path":"probability.html","id":"multiplication-law-of-probability","chapter":"9 Probability","heading":"9.8 Multiplication law of probability","text":"B independent events, \\(p\\)(AÍìµB) = \\(p\\)(). \\(p\\)(B)called Multiplication law probabilityA die tossed twice. Find probability number greater 4 throw.Let us denote , event 'number greater 4' first throw. B event 'number greater 4' second throw. Clearly B independent events. first throw, two outcomes, namely, 5 6 favourable event .‚à¥ \\(p\\)() =2/6 = 1/3Similarly second throw, two outcomes, namely, 5 6 favourable event .‚à¥ \\(p\\)(B) = 2/6 = 1/3Hence, \\(p\\)(B) = \\(p\\)(AÍìµB) = \\(p\\)().\\(p\\)(B) = 1 /3 √ó1 /3 = 1/9","code":""},{"path":"probability.html","id":"probability-using-combinations","chapter":"9 Probability","heading":"Probability using combinations","text":"Knowledge combinations can applied find total number possible outcomes.nCr\\(= \\frac{n!}{r!(n - r)!}\\)example\n3C2\\(= \\frac{3!}{2!(3 - 2)!} = \\frac{3 \\times 2 \\times 1}{2 \\times 1(1)} = 3\\)Now let us see example used probabilityA bag contains 3 red, 6 white 7 blue balls. probability two balls drawn white blue?Total number balls = 3 + 6 + 7 = 16Out 16 balls, 2 balls can drawn 16C2 waysi.e. 16C2 = 120.6 white balls, 1 ball can drawn 6C1 ways 7 blue balls, one can drawn 7C1 ways. Since former case associated later case, therefore total number favorable cases 6C1 √ó 7C1Therefore required probability= (6C1 √ó 7C1) / 16C2 = 42/120 = 7/20Find probability getting red balls, bag containing 5 red 4 black balls, two balls drawnTotal number balls = 9Out 9 balls, 2 balls can drawn 9C2 waysNo ways red balls can taken = 5C2Hence \\(p\\)(red balls ) = 5C2 / 9C2 = 5/18","code":""},{"path":"probability.html","id":"answer","chapter":"9 Probability","heading":"9.9 Answers of exercise 1","text":"{1, 2, 3, 4, 5, 6}{1, 2, 3, 4, 5, 6}1/61/63/6 = 1/23/6 = 1/200¬†\n¬†\n¬†","code":""},{"path":"probability-distributions.html","id":"probability-distributions","chapter":"10 Probability Distributions","heading":"10 Probability Distributions","text":"","code":""},{"path":"probability-distributions.html","id":"random-experiment","chapter":"10 Probability Distributions","heading":"10.1 Random experiment","text":"random experiment experiment process outcome\npredicted certainty. sample space (denoted S) \nrandom experiment set possible outcomes.Example: Tossing coin, Throwing die etc","code":""},{"path":"probability-distributions.html","id":"random-variable","chapter":"10 Probability Distributions","heading":"10.2 Random variable","text":"Random Variable set possible values random experiment.\nRandom variable usually denoted ‚Äò\\(X\\)‚Äô\nFigure 10.1: Tossing coin\nrandom variable whole set values take \nvalues, randomly. like Algebra Variable, Algebra \nvariable, like X, unknown value. random variable \ndifferent.Probability random variable can represented \\(p(X = x)\\) \n\\(p(x)\\).Small letter \\(x\\) denotes value taken random variable\n\\(X\\).example throwing die onceHere can define random variable interest. Defining \nrandom variable like:Let \\(X\\) number appearing throwing dice ; \\(x\\) = {1,\n2, 3, 4, 5, 6}. random variable \\(X\\) can take values shown\n. case equally likely, probability anyone\n1/6. , \\(p(X = x)\\) = \\(p(x)\\) =1/6Let \\(X\\) number appearing throwing dice ; \\(x\\) = {1,\n2, 3, 4, 5, 6}. random variable \\(X\\) can take values shown\n. case equally likely, probability anyone\n1/6. , \\(p(X = x)\\) = \\(p(x)\\) =1/6Let \\(X\\) even number appearing throwing dice ; \\(X\\)\n= {2, 4, 6}. random variable \\(X\\) can take values shown\n. \\(p(X = x)\\) = \\(p(x)\\) = 3/6Let \\(X\\) even number appearing throwing dice ; \\(X\\)\n= {2, 4, 6}. random variable \\(X\\) can take values shown\n. \\(p(X = x)\\) = \\(p(x)\\) = 3/6","code":""},{"path":"probability-distributions.html","id":"probability-distributions-1","chapter":"10 Probability Distributions","heading":"10.3 Probability distributions","text":"probability distribution list possible outcomes \nrandom variable along corresponding probability values.Example: - Tossing coin 3 timesDefining random variable:Let X number heads appearing throwing dice case\n0 heads, 1, 2 3 heads, sample space \\(X\\) =\n{0, 1, 2, 3} equally likely.total 8 possible cases tossing coin three times shown \ntable \\(p(X = x)\\) = \\(\\frac{: \\ \\ times\\ X\\ takes\\ value\\ x}{8}\\)\\(p(X = 3)\\) = 1/8; \\(p(X = 2)\\) = 3/8; \\(p(X = 1)\\) = 3/8; \\(p(X = 0)\\) = 1/8A¬†probability distribution list possible outcomes\nrandom variable along corresponding probability values.Table shows probability distribution.example discrete probability distribution \\(X\\) takes\ndiscrete values. \\(x\\) takes continuous values, termed \ncontinuous probability distribution.","code":""},{"path":"probability-distributions.html","id":"probability-mass-function","chapter":"10 Probability Distributions","heading":"10.4 Probability mass function","text":"use probability function describe discrete probability\ndistribution, call probability mass function (commonly\nabbreviated p.m.f). probability mass function, returns \nprobability outcome. Therefore, probability mass function \nwritten : \\(p(x)\\) = \\(p(X = x)\\). variable \\(X\\) discrete \nnature.","code":""},{"path":"probability-distributions.html","id":"probability-density-function","chapter":"10 Probability Distributions","heading":"10.5 Probability density function","text":"use probability function describe continuous probability\ndistribution, call probability density function (commonly\nabbreviated p.d.f).","code":""},{"path":"probability-distributions.html","id":"expected-value-of-a-random-variable","chapter":"10 Probability Distributions","heading":"10.6 Expected value of a random variable","text":"Expected value exactly might think means. return \ncan expect kind action. ¬†expected value¬†¬†random\nvariable long-run average value repetitions ¬†\nexperiment¬†represents.¬†example, expected value rolling \nsix-sided¬†die 3.5, average numbers come\n3.5 number rolls approaches infinity. Expected value \nrandom variable \\(X\\) denoted \\(E(X)\\).\nformula calculating Expected Value random variable \n¬†multiple probabilities isE(\\(X\\)) = \\(\\sum_{= 1}^{\\infty}xp(x)\\) (discrete case)E(\\(X\\)) = \\(\\sum_{= 1}^{\\infty}xp(x)\\) (discrete case)E(\\(X\\)) = \\(\\int_{- \\infty}^{\\infty}{x p(x)\\ \\text{dx }}\\) (continuous\ncase); \\(- \\infty \\leq x \\leq \\infty\\)E(\\(X\\)) = \\(\\int_{- \\infty}^{\\infty}{x p(x)\\ \\text{dx }}\\) (continuous\ncase); \\(- \\infty \\leq x \\leq \\infty\\)random variable \\(X\\) lies \\({-\\infty}\\) \\({+\\infty}\\)","code":""},{"path":"probability-distributions.html","id":"example-5","chapter":"10 Probability Distributions","heading":"Example","text":"Find Expected value \\(X\\) tossing single unfair dieSolution:E(\\(X\\)) = \\(\\sum_{= 1}^{6}xp(x)\\)\n= 0.1+0.2+0.3+0.4+0.5+3 = 4.5Find E(X) following case (Practice question)","code":""},{"path":"probability-distributions.html","id":"discrete-probability-distributions","chapter":"10 Probability Distributions","heading":"10.7 Discrete probability distributions","text":"seen ¬†probability distribution¬†list \npossible outcomes random variable along corresponding\nprobability values. represented table. \nconvenient can expressed equation. value x\ngiven can calculate corresponding probability \nequation. several discrete distributions depending \nsituations. discussion limited following discrete\ndistributions.Bernoulli distributionBernoulli distributionBinomial distributionBinomial distributionPoisson distributionPoisson distribution","code":""},{"path":"probability-distributions.html","id":"bernoulli-distribution","chapter":"10 Probability Distributions","heading":"10.7.1 Bernoulli distribution","text":"Bernoulli distribution ¬†discrete probability distribution. \ndistribution applies random experiment two outcomes\n(usually called ‚ÄúSuccess‚Äù ‚ÄúFailure‚Äù).example tossing coin two outcomes can termed \nsuccess failure experimenterSuccess: getting headFailure: getting tailThe probability mass function distribution \\(p(X = x)\\) = \\(p(x)\\) = \\(p\\)\\(x\\) \\((1-p)\\)\\(1-x\\), \\(X\\) takes two\nvalues = 0,1The¬†expected value¬†random variable, \\(X\\), Bernoulli\ndistribution : E(\\(X\\)) = \\(p\\) ¬†variance¬†Bernoulli random\nvariable : var(\\(X\\)) = \\(p(1 - p)\\).","code":""},{"path":"probability-distributions.html","id":"example-6","chapter":"10 Probability Distributions","heading":"Example","text":"Find probability assuming Bernoulli distribution biased coin\nprobability success (getting head) \\(p\\) = 0.4Let \\(X\\) random variable takes value 0 getting tail\n(failure) takes value 1 getting head (success). using \nequationp(\\(X\\) = \\(x\\)) = p(\\(X\\)) = p\\(X\\) \\((1-p)\\)\\(1-x\\)P(\\(X\\) = 0) = (0.4)0(1-0.4)1 =0.6P(\\(X\\) = 1) = (0.4)1(1-0.4)0 =0.4A¬†Bernoulli trial¬†one simplest experiments can\nconduct probability statistics. ‚Äôs experiment \ncan one two possible outcomes. example, ‚ÄúYes‚Äù ‚Äú‚Äù \n‚ÄúHeads‚Äù ‚ÄúTails.‚Äù","code":""},{"path":"probability-distributions.html","id":"binomial-distribution","chapter":"10 Probability Distributions","heading":"10.7.2 Binomial distribution","text":"Binomial distribution can thought simply probability \nSUCCESS FAILURE outcome experiment survey repeated\nmultiple times; .e. Binomial distribution happens, Bernoulli\ntrial repeated \\(n\\) number times. binomial type \ndistribution two possible outcomes (prefix ‚Äúbi‚Äù means two,\ntwice). example coin toss repeated 5 times, random\nvariable, \\(X\\) = : heads follow binomial distribution \n\\(n\\) = 5.Binomial distributions must also meet following three criteria:number observations trials fixed. (\\(n\\))number observations trials fixed. (\\(n\\))observation trial ¬†independentEach observation trial ¬†independentThe¬†probability success¬†(tails, heads, fail pass) ¬†exactly\n¬†one trial another (\\(p\\))¬†probability success¬†(tails, heads, fail pass) ¬†exactly\n¬†one trial another (\\(p\\))probability mass function distribution \\(p(x)\\) = nCx\\(p^x\\) \\(q^{n-x}\\) =\n\\(\\frac{n!}{\\left( n - x \\right)!x!\\ }\\) \\(p^x\\) \\(q^{n-x}\\)x takes values = 0,1,2,..., \\(n\\)\\(n\\)= number trials\\(x\\)= number success desired\\(p\\)= probability getting success one trial\\(q\\) = \\(1-p\\) = probability getting failure one trialE(\\(X\\)) = \\(np\\)V(\\(X\\)) = \\(npq\\); \\(n\\) \\(p\\) important parameters \nbinomial distribution.Mean binomial distribution \\(np\\) variance \\(npq\\)","code":""},{"path":"probability-distributions.html","id":"example-7","chapter":"10 Probability Distributions","heading":"Example","text":"coin tossed 10 times. probability getting exactly 6\nheads?,\\(n\\) = 10\\(x\\) = 6\\(p\\) = ¬Ω\\(q\\) = \\(1-p\\) = ¬Ωfind \\(p(X=6)\\); using formula \\(p(X=x)\\) \\(=\\) nCx\\(p^x\\) \\(q^{n-x}\\)\\(p(X=6)\\) = 10\\(c\\)6\\(\\left( \\frac{1}{2} \\right)^{6}\\left( \\frac{1}{2} \\right)^{10 - 6}\\)=\n0.2050","code":""},{"path":"probability-distributions.html","id":"poisson-distribution","chapter":"10 Probability Distributions","heading":"10.7.3 Poisson distribution","text":"Discovered French Mathematician Simeon Denis Poisson (1781\n-1840). developed describe number times gambler \nwin rarely won game chance large number tries, .e. Poisson\ndistribution deals rare events.Poisson distribution first applied study number death \nhorse kicking Prussian army.applications examples Poisson distribution usedPest incidencePest incidenceBirth defects genetic mutationsBirth defects genetic mutationsRare diseasesRare diseasesCar accidentsCar accidentsTraffic flow ideal gap distanceTraffic flow ideal gap distanceNumber typing errors pageNumber typing errors pageHairs found McDonald‚Äôs hamburgersHairs found McDonald‚Äôs hamburgersSpread endangered animal AfricaSpread endangered animal AfricaFailure machine one monthFailure machine one monthA random variable \\(X\\) said follow Poisson distribution; \nassumes non-negative values probability mass function given\n:\\[p\\left( X = x \\right) = \\frac{e^{- \\lambda}.\\lambda^{x}}{x!}\\]\\(x\\) = 0,1,2,‚Ä¶ ., ‚àû,\\(e\\) = 2.7183.\\(Œª\\) : Average number successes occurring given time interval \nregion Poisson distributionIt discrete distribution single parameter ŒªThe mean variance Poisson distribution \nequal ¬†\\(Œª\\)","code":""},{"path":"probability-distributions.html","id":"example-8","chapter":"10 Probability Distributions","heading":"Example","text":"average number homes sold Realty company 2 homes per day.\nAssuming Poisson distribution probability exactly 3\nhomes sold tomorrow?Solution:\\(Œª\\) = 2; since 2 homes sold per day, average.\\(x\\) = 3; since want find probability 3 homes sold\ntomorrow.\\(e\\) = 2.71828; since e constant equal approximately 2.71828.\\[p\\left( X = x \\right) = \\frac{e^{- \\lambda}.\\lambda^{x}}{x!}\\]\\(p(X=3)\\) = \\(\\ \\frac{{2.71828}^{- 2}{\\  \\times \\ 2}^{3}}{3!}\\)\\(p(X = 3)\\) = 0.180","code":""},{"path":"probability-distributions.html","id":"questions","chapter":"10 Probability Distributions","heading":"Questions","text":"random variable X follows Poisson distribution mean\n3.4, find \\(p(X = 6)\\)random variable X follows Poisson distribution mean\n3.4, find \\(p(X = 6)\\)number industrial injuries per working week particular\nfactory known follow Poisson distribution mean 0.5.\nFind probability particular week :number industrial injuries per working week particular\nfactory known follow Poisson distribution mean 0.5.\nFind probability particular week :\\()\\) Less 2 accidents \\[Hint: p(X<2) = p(X = 0) + p(X = 1)\\]\\(ii)\\) 2 accidents \\[Hint: p(X>2) = 1-{p(X = 0) + p(X = 1)\n+ p(X =2)}\\]company known past experience 3% bulbs \nproduced defective. Assuming Poisson distribution find \nprobability getting following sample 100 bulbs:\n\\[Hint:  \\ Œª = n √ó p = 100 √ó 0.03\\]\ndefective \\[Hint: let X number defectives; x = 0\\]\n1 defective \\[Hint: x = 1\\]\n2 defectives \\[Hint: x = 2\\]\n3 defectives\\[Hint: x = 3\\]\ndefective \\[Hint: let X number defectives; x = 0\\]defective \\[Hint: let X number defectives; x = 0\\]1 defective \\[Hint: x = 1\\]1 defective \\[Hint: x = 1\\]2 defectives \\[Hint: x = 2\\]2 defectives \\[Hint: x = 2\\]3 defectives\\[Hint: x = 3\\]3 defectives\\[Hint: x = 3\\]Distributions Bernoulli, Binomial Poisson discussed far\ndiscrete distributions.","code":""},{"path":"probability-distributions.html","id":"continuous-probability-distributions","chapter":"10 Probability Distributions","heading":"10.8 Continuous probability distributions","text":"random variable X continuous, corresponding probability\ndistribution termed continuous probability distribution. \nseveral continuous distributions. discussion limited \nNormal distribution.","code":""},{"path":"probability-distributions.html","id":"normal-distribution","chapter":"10 Probability Distributions","heading":"10.8.1 Normal distribution","text":"normal distribution defined following probability density\nfunction (probability density function explained section)\\[f\\left( x \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{- \\frac{{(x - \\mu)}^{2}}{2\\sigma^{2}}}\\], \\(- \\infty < x < + \\infty\\) ; \\(Œº\\) population mean \n\\(œÉ\\)2 variance, e = 2.718.random variable \\(X\\) follows normal distribution, write:\\(X\\)~\\(N\\)(\\(Œº\\), \\(œÉ\\)2)particular, normal distribution \\(Œº\\) = 0 \\(œÉ\\)2 = 1 \ncalled standard normal distribution, denoted \n\\(X\\)~\\(N\\)(0,1).","code":""},{"path":"probability-distributions.html","id":"properties-of-normal-distribution","chapter":"10 Probability Distributions","heading":"Properties of Normal distribution","text":"normal distribution frequently used among \nprobability laws. normal distribution can found many practical problems.plot density \\(f(x)\\) \\(x\\) graph bell shaped\nalways\nFigure 10.2: Normal distribution curve\nMany things closely follow Normal Distribution:Yield cropsYield cropsheights peopleheights peoplesize things produced machinessize things produced machineserrors measurementserrors measurementsblood pressureblood pressuremarks testmarks testThe Normal Distribution :mean = median = modeMean located centre curve. mean = median = mode, \nlocate towards centre\nFigure 10.3: Mean=Median=Mode normal distribution\nNormal distribution symmetric centre, 50% values less\nmean 50% greater mean\nFigure 10.4: Normal distribution symmetric distribution\n","code":""},{"path":"probability-distributions.html","id":"standardisation-of-normal-distribution","chapter":"10 Probability Distributions","heading":"10.8.1.1 Standardisation of Normal distribution","text":"standard normal distribution special case normal\ndistribution mean zero standard deviation 1.distribution also known Z-distribution. value \nstandard normal distribution known standard score Z-score.standard score Z-score represents number standard\ndeviations mean specific observation falls.convert value Standard Score (‚Äúz-score‚Äù):First subtract mean,First subtract mean,divide Standard DeviationThen divide Standard DeviationAnd called ‚ÄúStandardizing‚Äù.\nFigure 10.5: Standardization Normal distribution\nExample: survey daily travel time results (minutes):\\(X\\): 26, 33, 65, 28, 34, 55, 25, 44, 50, 36, 26, 37, 43, 62, 35, 38, 45,\n32, 28, 34Convert standard scores (Z-score).Mean 38.8 minutes, Standard Deviation 11.4First subtract mean observationFirst subtract mean observationThen divide Standard DeviationThen divide Standard Deviation\nFigure 10.6: Z score values X\n¬†z-score formula¬†using :\\[z = \\frac{x - \\mu}{\\sigma}\\]z ‚Äúz-score‚Äù (Standard Score)z ‚Äúz-score‚Äù (Standard Score)\\(x\\) value standardized\\(x\\) value standardized\\(Œº\\) (‚Äômu‚Äù) mean\\(Œº\\) (‚Äômu‚Äù) mean\\(œÉ\\) (‚Äúsigma‚Äù) standard deviation\\(œÉ\\) (‚Äúsigma‚Äù) standard deviation","code":""},{"path":"probability-distributions.html","id":"parameters-of-normal-distribution","chapter":"10 Probability Distributions","heading":"Parameters of Normal distribution","text":"probability distribution, parameters normal\ndistribution define shape probabilities entirely. normal\ndistribution two parameters, mean (\\(Œº\\)) standard deviation (\\(œÉ\\)). normal distribution just one form. Instead, \nshape changes based parameter values.\nFigure 10.7: Shape changes normal distribution based different means\nStandard deviation:standard deviation measure variability. defines width\nnormal distribution. determines far away mean \nvalues tend fall. represents typical distance observations\naverage.\nFigure 10.8: Shape changes normal distribution based different standard deviation\nnormally distributed data, standard deviation can \nused determine proportion values fall within \nspecified number standard deviations mean. example, \nnormal distribution, 68% observation falls within +/- 1 standard\ndeviation mean. property called Area Property.","code":""},{"path":"probability-distributions.html","id":"area-property","chapter":"10 Probability Distributions","heading":"Area Property","text":"\nFigure 10.9: Area property normal distribution\nshort properties Normal distributionNormal distribution curve bell shapedNormal distribution curve bell shapedNormal distribution symmetric, skewed.Normal distribution symmetric, skewed.mean, median, mode equal.mean, median, mode equal.Half population less mean half greater\nmean.Half population less mean half greater\nmean.Area property allows determine proportion values \nfall within certain distances mean.Area property allows determine proportion values \nfall within certain distances mean.","code":""},{"path":"probability-distributions.html","id":"solved-example","chapter":"10 Probability Distributions","heading":"Solved example","text":"1. z-score value 27, given set mean 24, \nstandard deviation 2?SolutionTo find z-score need divide difference value,\n27, mean, 24, standard deviation set, 2.\\[z = \\frac{27 - 24}{2} = \\frac{3}{2} = 1.5\\]indicates 27 +1.5 standard deviations mean.¬†\n¬†\n¬†","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"11 Hypothesis Testing","heading":"11 Hypothesis Testing","text":"Statistical analysis integral part scientific\ninvestigation. statistical analysis can conducted, researcher must generate guess, hypothesis going .process begins working hypothesis. Working hypothesis direct statement research idea. example, plant biologist may think plant height may affected applying different fertilizers. might say: \"Plants different fertilizers grow different heights\".","code":""},{"path":"hypothesis-testing.html","id":"the-falsification-principle","chapter":"11 Hypothesis Testing","heading":"The Falsification Principle","text":"falsification principle proposed Karl Popper, way demarcating science non-science. suggests theory considered scientific must able tested proven false. example, hypothesis \"swans white,\" can falsified observing black swan. According Popper, science attempt disprove theory, rather attempt continually support theoretical hypotheses.","code":""},{"path":"hypothesis-testing.html","id":"null-hypothesis","chapter":"11 Hypothesis Testing","heading":"11.1 Null hypothesis","text":"considering Popperian Principle Falsification, need translate working hypothesis framework consisting two hypotheses. hypotheses termed Null hypothesis Alternative hypothesis. clear example .Null hypothesis hypothesis either rejected accepted based experiment.example biologist may state null hypothesis average height (mean height) plants different fertilizers .alternative hypothesis (biologist hopes show) average height (mean height) plants different fertilizers equal, .e. fertilizer treatments produced plants different mean heights.¬†Now experiment biologist may either reject null hypothesis\naccept . turn result acceptance rejection \nalternative hypothesis accordingly.finally concentrate either rejecting accepting null\nhypothesis.","code":""},{"path":"hypothesis-testing.html","id":"definitions","chapter":"11 Hypothesis Testing","heading":"Definitions","text":"Null hypothesis: statement 'effect' 'difference'.\" often symbolized H0. hypothesis researcher trying disprove.Alternative hypothesis: simply inverse, opposite, null hypothesis. often symbolized H1.","code":""},{"path":"hypothesis-testing.html","id":"example-9","chapter":"11 Hypothesis Testing","heading":"Example","text":"long ago, people believed world flat. research problem whether Earth flat?Null hypothesis, H0: Earth flat.Alternate hypothesis, H1: world round.Several scientists, including Copernicus, set disprove null hypothesis. eventually led rejection null acceptance alternate.","code":""},{"path":"hypothesis-testing.html","id":"state","chapter":"11 Hypothesis Testing","heading":"11.1.1 Stating hypothesis","text":"Problem 1: researcher thinks certain chemical applied twice week flowering fruit tree. Average fruit weight plant 8 kg.Let us see null hypothesis problem formulatedStep 1: Figure hypothesis problem. hypothesis usually hidden word problem, sometimes statement expect happen experiment.hypothesis question ‚Äúresearcher expects average fruit weight per plant 8 kg.‚ÄùŒº (pronounced ‚Äômu') denotes average yield say, null hypothesis can stated asH0: Œº = 8Step 2: alternative hypothesis. State, happen hypothesis doesn‚Äôt come true? average fruit weight equal 8 kg, one assumed possibility weight less 8 kg (assuming possibility chemical \nincrease yield). alternative hypothesis can stated asH1: Œº < 8But researcher doesn‚Äôt idea happen?Problem 2: researcher studying effect chemical plant yield. chemical used pesticide. wants prove chemical effect yield. chemical can dangerous may boost yield.Step 1: Figure hypothesis problem.hypothesis question ‚Äúresearcher expects change average yield application chemical 0‚ÄùŒº denotes ‚Äòchange‚Äô average yield chemical applicationNull hypothesis can stated asH0: Œº = 0Step 2: alternative hypothesis. Researcher idea whether chemical increase decrease yield. just wants prove chemical effect yield. alternative hypothesis beH1: Œº \\(\\mathbf{\\neq}\\) 0","code":""},{"path":"hypothesis-testing.html","id":"hypo","chapter":"11 Hypothesis Testing","heading":"11.2 Hypothesis testing problem","text":"hypothesis testing decision two alternatives, one called null hypothesis alternative hypothesis, must made. make decision, experiment performed. hypothesis testing acceptance rejection null hypothesis can based decision rule.Example: coin . need check whether coin biased unbiased. Unbiased means 50:50 chance landing head tail tossing .First formulate null hypothesis alternative hypothesis.coin unbiased probability obtaining head 0.5; .e. \\(p\\) = 0.5Therefore null hypothesis alternative hypothesis beH0: \\(p\\) = 0.5H1: \\(p\\) \\(\\mathbf{\\neq}\\) 0.5You designed experiment , toss coin 10 times note outcome. conducting experiment got outcome given belowIn hypothesis testing acceptance rejection null hypothesis can based decision rule. calculate test statistic sample (outcome experiment). Decision rule based statistic.Test statistic: Test statistic quantity computed values sample function sample values based decision made null hypothesis.example decision rule, might decide reject null hypothesis accept alternative hypothesis, 8 heads occur 10 tosses coin. , reject null hypothesis \\(p\\) \\(\\geq \\frac{8}{10} = 0.8\\).experiment testNumber heads = 8Number tosses = 10Test statistic calculated, \\(p\\) = 8/10 = 0.8So based decision rule reject null hypothesis, assume coin biased.Now example sake formulated decision rule. frame decision rule simply. lot factors need considered, like confident reject null hypothesis based sample observations taken (number tosses 10). discuss coming sections.","code":""},{"path":"hypothesis-testing.html","id":"errors-in-hypothesis-testing","chapter":"11 Hypothesis Testing","heading":"11.3 Errors in Hypothesis testing","text":"respect hypothesis testing two errors can occur\n:null hypothesis actually true decision based testing process concluded null hypothesis false rejectedThe null hypothesis actually true decision based testing process concluded null hypothesis false rejectedThe null hypothesis actually false testing process concludes true accepted.null hypothesis actually false testing process concludes true accepted.two errors called Type Type II errors.¬†Type error: Reject null hypothesis, actually trueType II error: Accept null hypothesis, actually false¬†","code":""},{"path":"hypothesis-testing.html","id":"level-of-significance-and-power-of-test","chapter":"11 Hypothesis Testing","heading":"11.4 Level of significance and power of test","text":"Level significance (Œ±)significance level, also denoted Œ± (alpha), probability rejecting null hypothesis true. .e. Level significance probability Type error.Power test (1- Œ≤)probability Type II error denoted Œ≤. ‚Äò1-Œ≤‚Äô termed power test. Power test probability rejecting null hypothesis false. Œ± Œ≤ plays role deciding decision rule hypothesis testing.Seriousness type type II errorsWhich error serious? understand interrelationship Type Type II error, determine error severe consequences situation, consider following example.medical researcher wants compare effectiveness two medications.Null hypothesis (H0): Œº1= Œº2The two medications equally effective.Alternative hypothesis (H1): Œº1‚â† Œº2The two medications equally effective.Type error occurs researcher rejects null hypothesis concludes two medications different , fact, .¬†much serious consequence.Type II error occurs researcher concludes medications , fact, different.\nerror potentially life-threatening, less-effective medication sold public instead effective one.Now example, diagnosing cancer.Null hypothesis (H0): Patient cancerAlternative hypothesis (H1): Patient cancerA Type error occurs researcher rejects null hypothesis concludes patient cancer, actually healthy.Type II error occurs researcher concludes patient cancer cancer.errors can serious consequencesDepending situation seriousness Type Type II error may change. cases Type severe, Type II may severe cases.Now let us look critically situation. try reduce Type error Type II error increase. try reduce Type II error Type increase. make decision, must identify error serious. commit Type 1 error, reject \nnull hypothesis true. false positive, like fire alarm rings fire. Type II error happens fail reject null true. false negative - like alarm fails sound fire.Let us return question error, Type Type II, worse.Consider person accused crime waiting judgement hanged.H0: innocentH1: hangedWhat judge makes Type error?innocent hanged!!!!judge makes Type II error?criminal set free!!!course want let guilty person hook, people say sentencing innocent person punishment worse consequence. Hence, many textbooks instructors say Type (false positive) worse Type II (false\nnegative) error. assuming Type worse least worsening situation.practice, fix Type error selecting suitable probability Œ± experiment. reduce Type II error taking adequate sample size. Usually fix Œ± =0.05 0.01Note: exam question pops , error serious? Answer Type . honest answer - ‚Äòdepends‚Äô","code":""},{"path":"hypothesis-testing.html","id":"region-of-acceptance-and-rejection","chapter":"11 Hypothesis Testing","heading":"11.5 Region of acceptance and rejection","text":"test statistic calculate sample probability distribution. example consider coin tossing experiment discussed earlier. test statistic calculated person different person B, may get different outcomes performing experiment. particular value test statistic \nprobability. probability distribution test statistic termed sampling distribution test statistic.reject null hypothesis, test statistic falls particular area sampling distribution, area sampling distribution test statistic called region rejection. region sampling distribution test statistic called\nregion acceptance, test statistic falls area accept null hypothesis.\nFigure 11.1: Acceptance rejection region sampling distribution test statistic\nSize region rejection equal level significance = Œ±Size region acceptance equal = 1¬≠- Œ±The region rejection also known critical region. value test statistic reject null hypothesis called critical value.\nFigure 11.2: Critical value test statistic\n","code":""},{"path":"hypothesis-testing.html","id":"two-tailed-and-single-tailed-test","chapter":"11 Hypothesis Testing","heading":"11.6 Two tailed and single tailed test","text":"statistical test based two competing hypotheses: null hypothesis H0 alternative hypothesis H1. type alternative hypothesis H1 defines test one-tailed two-tailed. based alternative hypothesis type test determined.","code":""},{"path":"hypothesis-testing.html","id":"one-tailed-tests","chapter":"11 Hypothesis Testing","heading":"One-tailed tests","text":"Consider problem 1 section 11.1.1, alternative hypothesis stated asH1: Œº <8For alternative hypothesis reject null hypothesis test statistics falls towards left side sampling distribution, test left tailed test.H1: Œº > 8For alternative hypothesis reject null hypothesis test statistics falls towards right side sampling distribution, test right tailed test.Left tailed test: critical region towards left side sampling distribution test statistic\nFigure 11.3: Left tailed test: Critical region towards left side (Shown sampling distribution student t left tailed 20 degrees freedom)\nRight tailed test: critical region towards right side sampling distribution test statistic\nFigure 11.4: Right tailed test: Critical region towards right side (Shown sampling distribution student t right tailed 20 degrees freedom)\nTwo-tailed testsConsider problem 2 section 11.1.1, alternative hypothesisH1: Œº \\(\\mathbf{\\neq}\\) 0Consider another alternative hypothesisH1: Œº \\(\\mathbf{\\neq}\\) 8In cases critical region lies sides. Size side Œ±/2. Together total size Œ±\nFigure 11.5: Two tailed test: Critical region side\n","code":""},{"path":"hypothesis-testing.html","id":"decision-rule","chapter":"11 Hypothesis Testing","heading":"11.7 Decision rule","text":"calculation test statistic experiment, make decision null hypothesis?Decision rule largely determined level significance Œ±. good test one low probability committing Type error (.e., small Œ±) high power (1-Œ≤, high power).Power determined sample size experiment. Based Œ± select critical value test statistic, calculated value falls critical value reject null hypothesis (right tailed test). calculated value falls critical value \nreject null hypothesis (left tailed test). calculated value falls critical value sides, reject null hypothesis (two tailed test)","code":""},{"path":"hypothesis-testing.html","id":"an-example","chapter":"11 Hypothesis Testing","heading":"11.8 An example","text":"Consider example testing whether coin biased section 11.2Here going identify whether coin biased based just 10 tossing. (10 sample size).Let null hypothesis beH0: \\(p\\) = 0.5H1: \\(p\\) \\(\\neq\\) 0.5Let \\(X\\) number heads, let us see probability null hypothesis \\(X\\) take value 1 10.know binomial theorem:\\(p(X=x)\\) = \\(n\\)\\(c\\)\\(x\\)\\(p\\)\\(x\\)\\(q\\)\\(n-x\\) = \\(\\frac{n!}{\\left( n - x \\right)!x!\\ }\\) \\(p\\)\\(x\\)\\(q\\)\\(n-x\\)experiment n=10, null hypothesis true \\(p\\) = 0.5. probability distribution \\(X\\) , null hypothesis trueAbove distribution \\(X\\) can plotted ,\nFigure 11.6: Sampling distribution test statistic X\nŒ± level significance. experiment selected Œ± = 0.05. two tailed test critical value value \\(X\\), area Œ±/2 = 0.025. using probability distribution table can see area beyond \\(X\\) = 8 \\(X\\) = 2 approximately area 0.025. critical values\n\\(X\\) = 8 \\(X\\) = 2. means number heads 8 2 reject null hypothesis level significance Œ± =0.05.\nFigure 11.7: Probability distribution X critical region\nCommonly used test statistics \\(t\\), \\(F\\), \\(Z\\) œá2 (pronounced chi-square). Critical values test statistics already available tables.","code":""},{"path":"hypothesis-testing.html","id":"confidence-interval","chapter":"11 Hypothesis Testing","heading":"11.9 Confidence Interval","text":"rejecting null hypothesis level significance Œ±. Meaning can attach 100(1-Œ±)% confidence conclusion. rejecting null hypothesis level significance Œ±=0.05, 100(1-0.05)% .e. 95% confident result. Œ±=0.01 100(1-0.01)% .e. 99% confident\nresult. can also interpreted , can attach 95% confidence means, experiment repeated infinite number times, 95% chance get conclusion.","code":""},{"path":"hypothesis-testing.html","id":"steps-in-hypothesis-testing","chapter":"11 Hypothesis Testing","heading":"11.10 Steps in hypothesis testing","text":"Now conclusion following 7 steps hypothesis testingStep 1: State Null HypothesisStep 2: State Alternative HypothesisStep 3: Set level significance (Œ±)Step 4: Collect Data (experiment scientific methods)Step 5: Calculate test statisticStep 6: Identify critical region test statistic specified level significance (Œ±)Step 7: Compare test statistic critical value test statistic.Step 8: calculated value test statistic greater critical value test statistic reject null hypothesis 100(1 ‚Äì Œ± )% confidence. Otherwise state don‚Äôt enough evidence reject null hypothesis.Don‚Äôt panic, things take little time digest. gone much discussing concept detail. exam point view try answer followingWhat hypothesis?hypothesis?Define null alternative hypothesisDefine null alternative hypothesisDefine test statisticDefine test statisticTwo errors hypothesis testing, definition.Two errors hypothesis testing, definition.Define one-tailed two-tailed testDefine one-tailed two-tailed testDefine Critical region Critical valueDefine Critical region Critical valueDefine Power test, Level significanceDefine Power test, Level significanceWrite steps hypothesis testingWrite steps hypothesis testing¬†\n¬†\n¬†","code":""},{"path":"sample-survey.html","id":"sample-survey","chapter":"12 Sample Survey","heading":"12 Sample Survey","text":"Sample surveys widely used means collecting information meet definite need Government, Trade industry, Science Technology various activities society Social, Educational Economic problems.","code":""},{"path":"sample-survey.html","id":"some-specific-situations-in-which-the-sample-surveys-are-employed","chapter":"12 Sample Survey","heading":"Some specific situations in which the sample surveys are employed:","text":"results required maximum accuracy (reliability) fixed budget enumerating minimum number units.units investigation show considerable variation characteristic study.total count population possible, method measurement needs destruction.scope investigation wide population completely known.time money resources limited.","code":""},{"path":"sample-survey.html","id":"definition-of-some-common-terms-used-in-sampling","chapter":"12 Sample Survey","heading":"12.1 Definition of some common terms used in sampling","text":"Population: group individuals (units) defined according survey. collection units kind information collected. cultivated fields specified crop area, population country, households village measuring income etc. examples.Sample: part aggregate (whole population). Inference drawn basis results obtained sample.Census: complete enumeration population.Sampling units: ultimate unit data collected.Sample size: number units included sample.Sampling designs sampling methods: scientific objective procedure selecting units population getting sample expected representative whole population. provides procedures estimation results obtained comparable survey taken units population.Parameters statistics: statistical measures population like mean, variance etc. referred parameters. statistical measures computed sample observations called Statistics.Expectation: arithmetic mean possible values statistic expressed E(statistic) = Expected value expectation statisticUnbiased estimate: statistic t(function sample values) unbiased estimate population parameter say Œ∏ Expectation t = Œ∏ ie., E(Statistic) = Parameter.Sampling error: sample survey can never reproduce exactly various characteristics population (unless population surveyed census conducted). discrepancy sample estimates population values (obtained estimating units) termed sampling error.long sampling errors sufficiently small, sampling method can adopted studying population. Sampling errors absent complete enumeration.primarily due () faulty selection samples (ii) substitution sampling units already included study. (iii) faulty demarcation sampling units. (iv) constant errors due improper choice Statistical methods estimation parameters.Increasing sample size decreases sampling error.Non sampling error: Errors sampling errors arising non response, incompleteness, inaccuracy, etc. called non sampling errors. data obtained census free sampling errors still subjected non sampling errors. (Sample survey data subjected errors.)non sampling errors can occur due () Faulty planning faulty definitions ‚Äì inadequate data specifications, errors locating units, lack trained persons etc. (ii) Response errors- misunderstanding question, bias interviewer respondent (iii) Non response bias ‚Äì full information obtained/collected. (iv) Errors coverage compilation publication errors.","code":""},{"path":"sample-survey.html","id":"sample-suervey-methods","chapter":"12 Sample Survey","heading":"12.2 Sample Suervey-Methods","text":"Principal steps sample survey:Decide objectives survey.Define population surveyed.frame sampling units (population can divided sampling units cover entire population must distinct, unambiguous non-overlapping. list, map material covering population called frame.)Identify data collected.Preparation questionnaire schedule.Decide method collecting information (interview, mailing etc.)Procedures non respondents.Selection proper sampling designs desired degree precision.Organization field work.pre testsSummary analysis data (scrutinizing, editing tabulating etc.)Record information gained future surveys.","code":""},{"path":"sample-survey.html","id":"methods-of-sampling","chapter":"12 Sample Survey","heading":"12.3 Methods of Sampling","text":"various methods sampling can grouped \nFigure 12.1: Sampling methods classification\n","code":""},{"path":"sample-survey.html","id":"probability-sampling","chapter":"12 Sample Survey","heading":"12.3.1 Probability sampling:","text":"Probability sampling scientific method selecting samples according laws chance unit population definite pre assigned probability selected sample like:Units equal chance chosen.Sometimes units selected different probabilities.Probability selection proportional size units.","code":""},{"path":"sample-survey.html","id":"simple-random-sampling","chapter":"12 Sample Survey","heading":"12.3.1.1 Simple Random Sampling","text":"fundamental simple method drawing probability samples. used population units nearly homogeneous respect study variables. units can selected without replacement methods. number (unit) drawn removed population subsequent draws method called Simple random sampling (SRS) without replacement.simple random sample size n replacement, possible samples size n equal chance selected survey. Units numbered 1, 2,‚Ä¶,N n numbers drawn one one assigning equal probability selection units first subsequent draws. samples can drawn random either lottery method using random numbers.Let N population units n sample size selected. Simple random sampling method selecting n units every one distinct samples equal chance drawn.Example: Suppose want select simple random sample 100 students college. , can assign number every student college database 1 300 use random number generator select sample 100 numbers.","code":""},{"path":"sample-survey.html","id":"stratified-random-sampling","chapter":"12 Sample Survey","heading":"12.3.1.2 Stratified random sampling","text":"stratified sampling approach divides entire population smaller groups ¬†complete sampling procedure. small group generated based demographic parameters. dividing population smaller groups, statisticians draw random sample.Example: three boxes (, B C), different balls. Bag 100 balls, bag B 150 balls, bag C 200 balls. choose sample balls bag proportionally. Suppose 1 balls bag , 10 balls bag B 20 balls bag C.","code":""},{"path":"sample-survey.html","id":"systematic-sampling","chapter":"12 Sample Survey","heading":"12.3.1.3 Systematic sampling","text":"systematic sampling method selects items target population selecting random selection point selecting methods predetermined sample interval. determined dividing total population size target population size.Example: Suppose names 300 graduates college sorted reverse alphabetical order. select sample systematic sampling method, choose 15 graduates randomly selecting starting number, say 5. number 5 onwards, select every 15th person sorted list. Finally, can end sample graduates.","code":""},{"path":"sample-survey.html","id":"cluster-sampling","chapter":"12 Sample Survey","heading":"12.3.1.4 Cluster sampling","text":"clustered sampling method creates cluster group objects population set. group shares comparable significatory traits. Also, equal chance included sample. method use basic random sampling cluster population.Example: list agricultural fields village district may easily available list village districts generally available. case, every farm sampling unit every\nvillage district cluster. Draw sample clusters villages collect observations sampling units available selected clusters.","code":""},{"path":"sample-survey.html","id":"non-probability-sampling","chapter":"12 Sample Survey","heading":"12.3.2 Non-probability sampling","text":"non-probability sampling method one researcher chooses sample based subjective judgement rather random selection. approach excludes population members¬†getting involved study.","code":""},{"path":"sample-survey.html","id":"convenience-sampling","chapter":"12 Sample Survey","heading":"12.3.2.1 Convenience sampling","text":"Convenience sampling probably effective type non-probability sampling. Researchers select members based accessibility closeness, making easier collect data.samples easy select, researcher choose sample outlines entire population.Example: Surveying peoples shopping center nearby park considered convenience pattern.","code":""},{"path":"sample-survey.html","id":"consecutive-sampling","chapter":"12 Sample Survey","heading":"12.3.2.2 Consecutive sampling","text":"Consecutive sampling similar convenience sampling minor difference. , researcher chooses sample group people, conducts study time, collects data, moves next sample.Example: companies/ brands stop people crowded areas hand promotional leaflets purchase luxury car.","code":""},{"path":"sample-survey.html","id":"quota-sampling","chapter":"12 Sample Survey","heading":"12.3.2.3 Quota sampling","text":"Quota sampling divides population subgroups strata assigns quota grouping. Researchers collect data individuals within subgroup quota satisfied. allows stratification, guarantee accuracy may result selection bias quotas carefully designed.Example: marketplace studies look new cosmetic product, studies team makes decision accumulate facts customers nearby beauty store. divide capacity contributors classes based totally demographics, age gender. category, set selected quota, like 40 girls elderly 20-30 20 men aged 25-35, . researchers approach shoppers within mall till ‚Äôve filled every quota. Quota sampling lets sure stage stratification contain random choice, contributors decided fill predefined quotas.","code":""},{"path":"sample-survey.html","id":"purposive-sampling","chapter":"12 Sample Survey","heading":"12.3.2.4 Purposive sampling","text":"sampling, researchers select persons believe applicable familiar study topic. strategy widely used qualitative research specialist evaluations necessary. However, can introduce researcher bias may suitable generalisation. also known judgmental sampling.Example: researcher conducting study performance top-performing farmer big village. Instead choosing farmer randomly, researcher chooses review farmer acquired couple awards recognitions terrific work. example judgmental purposive sampling, researcher deliberately selects contributors taken consideration expert precise characteristics relevant research topic.","code":""},{"path":"sample-survey.html","id":"snowball-sampling","chapter":"12 Sample Survey","heading":"12.3.2.5 Snowball sampling","text":"Snowball sampling also known chain-referral sampling method. usually used target population hard reach, include hidden marginalized communities., recognised member population asked locate remaining sampling units. sampling units also target population.Example: Consider study aiming understanding experiences illegal immigrants certain city. Given hidden frequently marginalised nature population, researcher starts identifying interviewing one undocumented immigrant. Following interview, researcher requests initial participant suggest others likely willing participate research. process continues player referring researcher capable individuals.","code":""},{"path":"sample-survey.html","id":"limitations-of-sampling-methods","chapter":"12 Sample Survey","heading":"Limitations of Sampling methods:","text":"Sometimes samples fully cover population consequently results exact.Sampling theory dependable unless trained qualified persons employed field.planning execution done carefully data may provide misleading results.¬†\n¬†\n¬†","code":""},{"path":"large-sample-test.html","id":"large-sample-test","chapter":"13 Large sample test","heading":"13 Large sample test","text":"sample sample size \\(n\\) greater 30 (\\(n\\) ‚â• 30) \nknown large sample. study sampling distribution statistic\nlarge sample known large sample theory .","code":""},{"path":"large-sample-test.html","id":"large-sample-tests","chapter":"13 Large sample test","heading":"13.1 Large Sample Tests","text":"section shall discuss application \\(Z\\)-test. test\nstatistic \\(Z\\) calculated large sample follows standard normal\ndistribution. \\(Z\\) \\(\\sim N(0,1)\\)Test single proportionTest single proportionTest equality two proportionsTest equality two proportionsTest single meanTest single meanTest equality two meansTest equality two meansTest equality two standard deviationsTest equality two standard deviationsTest significance correlation coefficientTest significance correlation coefficient","code":""},{"path":"large-sample-test.html","id":"decision","chapter":"13 Large sample test","heading":"13.1.1 Decision rule for Z test","text":"Let \\(Z\\) calculated value Œ± level significance, \nreject null hypothesis \\(|Z|\\) > \\(Z\\)Œ±/2 ; two tailed test\\(|Z|\\) > \\(Z\\)Œ±/2 ; two tailed test\\(Z\\) > \\(Z\\)Œ± ; right tailed test\\(Z\\) > \\(Z\\)Œ± ; right tailed test\\(Z\\) < - \\(Z\\)Œ± ; left tailed test\\(Z\\) < - \\(Z\\)Œ± ; left tailed testTable values (critical values) Z specified level significance\nshown ","code":""},{"path":"large-sample-test.html","id":"test-for-a-single-population-proportion","chapter":"13 Large sample test","heading":"13.2 Test for a single population proportion","text":"Consider population proportion value, say \\(P\\); \\(P\\) \nunknown, take random sample size \\(n\\) population\ncalculate sample proportion \\(p\\). want test whether \npopulation proportion \\(P\\), unknown equal \\(P\\)0, based \nsample proportion \\(p\\).null hypothesis tested isH0 : \\(P\\) = \\(P\\)0The alternative hypothesis may eitherH1 : \\(P\\) < \\(P\\)0 (called left tailed alternative)OrH1 : \\(P\\) > \\(P\\)0 (called right tailed alternative)OrH1 : \\(P\\) ‚â† \\(P\\)0 (called two tailed alternative)test, calculate test statistic, \\(Z\\) using following\nformula.\\[Z = \\frac{p - P_{0}}{\\sqrt{\\text{pq/n}}}\\]\\(q\\) = 1 - \\(p\\)null hypothesis true \\(Z\\) follow standard normal\ndistribution mean 0 variance 1. \\(Z\\) \\(\\sim N(0,1)\\)calculated \\(Z\\) compared critical value \\(Z\\) \ntable standard normal distribution. calculated value \nhigher table value, reject null hypothesis. \ncalculated value less table value, conclude don‚Äôt\nenough evidence sample reject null hypothesis. See\nsection 13.1.1","code":""},{"path":"large-sample-test.html","id":"example-1-1","chapter":"13 Large sample test","heading":"Example 1","text":"sample 500 apples taken large consignment 60 \nfound bad. Test whether proportion bad apples \nconsignment 0.1 5% level significance.Solution:Sample size (\\(n\\)) = 500Proportion bad apples sample (\\(p\\)) = 60/500 = 0.12Proportion good apples sample (\\(q\\)) = 1 - 0.12 = 0.88We want test whether proportion bad apple population =\n0.1 ; \\(P\\)0 = 0.1The null hypothesis tested isH0 : \\(P\\) = 0.1If want prove proportion bad apples 0.1 \ncan use two tailed test alternative hypothesis belowH1 : \\(P\\) ‚â† 0.1So null hypothesis rejected, accept alternative\nhypothesisLevel significance given 5% , Œ±=0.05Now calculate \\(Z\\)\\(Z = \\frac{p - P_{0}}{\\sqrt{\\text{pq/n}}}\\)\\(Z = \\frac{0.12 - 0.1}{\\sqrt{0.12 \\times 0.88/500}}\\)\\(= 1.428\\)case since two tailed test, look critical value \\(Z\\) Œ±/2. look value \\(Z\\) Œ±/2 = 0.05/2 = 0.025, 2.241 (see section 13.1.1).\ncalculated value Z less table value. , conclude\n, don‚Äôt enough evidence reject null hypothesis. ,\ncan stated proportion bad apples population 0.1.","code":""},{"path":"large-sample-test.html","id":"self-excercise","chapter":"13 Large sample test","heading":"Self excercise","text":"random sample 500 plants taken large experimental\nfield 65 plants found affected yellowing\ndisease. disease rate significant? (Œ± = 0.05)","code":""},{"path":"large-sample-test.html","id":"test-for-equality-of-two-proportions","chapter":"13 Large sample test","heading":"13.3 Test for equality of two proportions","text":"Consider two populations proportion values, say \\(P\\)1\n\\(P\\)2 ; unknown, take random sample sizes\n\\(n\\)1 \\(n\\)2 populations respectively calculate\nsample proportions \\(p\\)1 \\(p\\)2. want test whether \npopulation proportions \\(P\\)1 \\(P\\)2 equal, based sample\nproportions \\(p\\)1 \\(p\\)2.null hypothesis tested isH0 : \\(P\\)1 = \\(P\\)2The alternative hypothesis may eitherH1 : \\(P\\)1 < \\(P\\)2 (called left tailed alternative)OrH1 : \\(P\\)1> \\(P\\)2 (called right tailed alternative)OrH1 : \\(P\\)1‚â† \\(P\\)2 (called two tailed alternative)test, calculate test statistic, \\(Z\\) using following\nformula.\\[Z = \\frac{p_{1} - p_{2}}{\\sqrt{\\widehat{P}\\widehat{Q}\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\]\\(\\widehat{P} = \\frac{n_{1}p_{1} + n_{2}p_{2}}{n_{1} + n_{2}}\\) \n\\(\\widehat{Q} = 1 - \\widehat{P}\\)null hypothesis true \\(Z\\) follow standard normal\ndistribution mean 0 variance 1. \\(Z\\) ~ N(0,1).calculated value \\(z\\) compared critical value \\(Z\\)\ntable standard normal distribution. See section\n13.1.1. calculated value higher table value,\nreject null hypothesis. calculated value less \ntable value, conclude don‚Äôt enough evidence \nsample reject null hypothesis.","code":""},{"path":"large-sample-test.html","id":"example-2-1","chapter":"13 Large sample test","heading":"Example 2","text":"order assess adoption new variety paddy farmers,\nsurvey conducted locality. survey covered 80 farmers\nlarge land holding 250 farmers small land holding. \nobserved 50 big farmers 78 small farmers\nadopted new paddy variety. Test whether significant\ndifference adoption behaviour two groups farmers\n(Take Œ± = 0.01)Solution:Sample size first population, framers large land holding\n(\\(n\\)1) = 80Sample size first population, framers small land holding\n(\\(n\\)2) = 250Proportion framers large land holding adopted paddy variety\n(\\(p\\)1) = 50/80 = 0.625Proportion framers large land holding adopted paddy variety\n(\\(p\\)2) =78/250 = 0.312We want test whether proportion significantly different\npopulations ,H0 : \\(P\\)1= \\(P\\)2Here alternate hypothesis isH1 : \\(P\\)1 ‚â† \\(P\\)2So, two tailed test\\[\\widehat{P} = \\frac{n_{1}p_{1} + n_{1}p_{1}}{n_{1} + n_{2}}\\]\\[\\widehat{P} = \\frac{80 \\times 0.625 + 250 \\times 0.312}{80 + 250}\\]\\[= 0.3879\\]\\[\\widehat{Q} = 1 - 0.3879 = 0.6121\\]null hypothesis rejected, accept alternative\nhypothesisLevel significance given 1% , Œ±=0.01Now calculate \\(Z\\) using formula\\[Z = \\frac{p_{1} - p_{2}}{\\sqrt{\\widehat{P}\\widehat{Q}\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\]\\[= \\frac{0.625 - 0.312}{\\sqrt{0.3879 \\times 0.6121\\left( \\frac{1}{80} + \\frac{1}{250} \\right)}}\\]\\[= \\frac{0.313}{\\sqrt{0.2374 \\times 0.0165}}\\]\\[= 5.008\\]Since two tailed test, look critical value \n\\(Z\\) Œ±/2. case look value \\(Z\\) \nŒ±/2 = 0.01/2 = 0.005, 2.807 (see section 13.1.1).\nSince calculated value (5.008) greater table value\n(2.807) reject null hypothesis conclude \nsignificant difference two population proportions.","code":""},{"path":"large-sample-test.html","id":"test-for-a-single-population-mean","chapter":"13 Large sample test","heading":"13.4 Test for a single population mean","text":"Consider population mean, say \\(Œº\\); \\(Œº\\) unknown,\ntake random sample size n population \ncalculate sample mean, denoted \\(\\overline{x}\\). want test\nwhether population mean \\(Œº\\), unknown equal known\nconstant \\(Œº\\)0, based sample mean \\(\\overline{x}\\).null hypothesis tested isH0 : \\(Œº\\) = \\(Œº\\)0The alternative hypothesis may eitherH1 : \\(Œº\\) < \\(Œº\\)0 (called left tailed alternative)OrH1 : \\(Œº\\) > \\(Œº\\)0 (called right tailed alternative)OrH1 : \\(Œº\\) ‚â† \\(Œº\\)0 (called two tailed alternative)particular test two casesCase population standard deviation œÉ knownCase population standard deviation œÉ knownCase population standard deviation œÉ unknownCase population standard deviation œÉ unknownWe discuss case ","code":""},{"path":"large-sample-test.html","id":"population-standard-deviation-œÉ-is-known","chapter":"13 Large sample test","heading":"Population standard deviation œÉ is known","text":"Population standard deviation œÉ known test, \ncalculate test statistic, \\(Z\\) using following formula.\\[Z = \\frac{\\overline{x} - \\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}}\\]null hypothesis true \\(Z\\) follow standard normal\ndistribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule\ngiven section 13.1.1","code":""},{"path":"large-sample-test.html","id":"population-standard-deviation-œÉ-is-unknown","chapter":"13 Large sample test","heading":"Population standard deviation œÉ is unknown","text":"Population standard deviation œÉ unknown test, ùúé \nreplaced sample estimate s, calculate test statistic,\n\\(Z\\) using following formula.\\[Z = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}}\\]null hypothesis true \\(Z\\) follow standard normal\ndistribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule\ngiven section 13.1.1","code":""},{"path":"large-sample-test.html","id":"example-3-1","chapter":"13 Large sample test","heading":"Example 3","text":"sample 900 items mean 3.4 cm standard deviation 2.61cm.\nTest whether population mean 3.25cm 5% significance level.Solution:Null hypothesis, H0 : \\(Œº\\) = 3.25Alternate hypothesis, H1 : \\(Œº\\) ‚â† 3.25; two tailed testSample size (\\(n\\)) = 900Sample mean, \\(\\overline{x}\\) = 3.4Sample standard deviation (\\(s\\)) = 2.61Population mean (\\(Œº\\)0) = 3.25Level significance, Œ± = 0.05\\(Z = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}}\\)\\(Z = \\frac{3.4 - 3.25}{\\frac{2.61}{\\sqrt{900}}} = 1.724\\)Since two tailed test, look critical value \n\\(Z\\) Œ±/2. case look value Z \nŒ±/2=0.05/2 =0.025, 2.241 (Decision rule see section\n13.1.1). Since calculated value (1.724) less \ntable value (2.241), conclude , don‚Äôt enough evidence \nreject null hypothesis. , can stated mean 3.25 cm.","code":""},{"path":"large-sample-test.html","id":"self-exercise","chapter":"13 Large sample test","heading":"Self exercise","text":"paddy field sample 36 plants selected random.\nplants, panicle lengths observed. mean \nstandard deviation measurements 18.7cm 1.25cm\nrespectively. Test whether mean length panicle paddy \n19cm. (Œ± = 0.05)","code":""},{"path":"large-sample-test.html","id":"test-for-equality-of-two-means","chapter":"13 Large sample test","heading":"13.5 Test for equality of two means","text":"Let two normally distributed populations means \\(¬µ\\)1 \n\\(¬µ\\)2 standard deviations \\(œÉ\\)1 \\(œÉ\\)2 respectively. Let\nsamples sizes \\(n\\)1 \\(n\\)2 taken populations.\nLet sample means \\(\\overline{x}_{1}\\) \\(\\overline{x}_{2}\\)\nrespectively. want test whether population means \nsignificantly different based sample means.null hypothesis tested isH0 : \\(¬µ\\)1 = \\(¬µ\\)2The alternative hypothesis may eitherH1 : \\(¬µ\\)1 < \\(¬µ\\)2 (called left tailed alternative)OrH1 : \\(¬µ\\)1> \\(¬µ\\)2 (called right tailed alternative)OrH1 : \\(¬µ\\)1‚â† \\(¬µ\\)2 (called two tailed alternative)two cases testCase population standard deviations equal \\(œÉ\\)1 ‚â†\n\\(œÉ\\)2Case population standard deviations equal \\(œÉ\\)1 ‚â†\n\\(œÉ\\)2Case population standard deviations equal \\(œÉ\\)1 =\n\\(œÉ\\)2 = \\(œÉ\\)Case population standard deviations equal \\(œÉ\\)1 =\n\\(œÉ\\)2 = \\(œÉ\\)","code":""},{"path":"large-sample-test.html","id":"when-the-population-standard-deviations-are-equal","chapter":"13 Large sample test","heading":"When the population standard deviations are equal","text":"case, calculate test statistic, \\(Z\\) using \nfollowing formula.\\[Z = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sigma\\sqrt{\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\]null hypothesis true \\(Z\\) follow standard normal\ndistribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule\ntests (section 13.1.1).Note: \\(œÉ\\) unknown replaced estimate\n\\(\\sqrt{\\frac{n_{1}s_{1}^{2} + n_{2}s_{2}^{2}}{n_{1} + n_{2}}}\\) \n\\(n\\)1 \\(n\\)2 sample sizes, \\(s\\)1 \\(s\\)2 sample\nstandard deviations.","code":""},{"path":"large-sample-test.html","id":"when-the-population-standard-deviations-are-not-equal","chapter":"13 Large sample test","heading":"13.5.1 When the population standard deviations are not equal","text":"case, calculate test statistic, \\(Z\\) using \nfollowing formula.\\[Z = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sqrt{\\left( \\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{\\sigma_{2}^{2}}{n_{2}} \\right)}}\\]null hypothesis true \\(Z\\) follow standard normal\ndistribution mean 0 variance 1. \\(Z\\) ~ N(0,1). Decision rule\ntests (section 13.1.1).","code":""},{"path":"large-sample-test.html","id":"example-4-1","chapter":"13 Large sample test","heading":"Example 4","text":"means two single large samples 1000 2000 members \n67.5 inches 68 inches respectively. Can samples regarded \ndrawn population standard deviation 2.5 inches. (Test\n5% significance level).Solution:Null hypothesis, \\(H\\)0 : \\(Œº\\)1 = \\(Œº\\)2Alternate hypothesis, \\(H\\)1 : \\(Œº\\)1‚â† \\(Œº\\)2; two tailed testSample size (\\(n\\)1) = 2000Sample size (\\(n\\)2) = 1000Sample mean first group, \\({\\overline{x}}_{1}\\) = 68Sample mean second group, \\({\\overline{x}}_{2}\\) = 67.5Population standard deviation (\\(œÉ\\)) = 2.5Level significance, Œ± = 0.05we calculate test statistic, \\(Z\\) using following formula.\\(Z = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sigma\\sqrt{\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\)\\(Z = \\frac{68 - 67.5}{2.5\\sqrt{\\left( \\frac{1}{2000} + \\frac{1}{1000} \\right)}} = \\frac{0.5}{0.0968} = 5.20\\)Since two tailed test, look critical value \n\\(Z\\) Œ±/2. case look value \\(Z\\) \nŒ±/2=0.05/2 =0.025, 2.241 (see section 13.1.1).\nCalculated value (5.20) greater table value, \nreject null hypothesis. may conclude samples drawn\npopulation.","code":""},{"path":"large-sample-test.html","id":"self-exercise-1","chapter":"13 Large sample test","heading":"Self exercise","text":"Two random samples drawn two populations \nfollowing data obtained. Test whether population means\nequal. \\(n\\)1 = 400, \\(n\\)2 = 400, \\(\\overline{x}_{1}\\) = 250,\n\\(\\overline{x}_{2}\\) = 220, \\(s\\)1 = 40, \\(s\\)2 = 55¬†\n¬†\n¬†","code":""},{"path":"small-sample-tests.html","id":"small-sample-tests","chapter":"14 Small sample tests","heading":"14 Small sample tests","text":"sample size n less 30 (n < 30) known small sample. small samples sampling distributions statistic commonly used œá2 (Chi-square), F t distribution. study sampling distribution statistic small samples known small sample theory.Small Sample Tests (sample size (n) < 30)","code":""},{"path":"small-sample-tests.html","id":"tests-based-on-student-t-distribution-t-tests","chapter":"14 Small sample tests","heading":"14.1 Tests based on Student t distribution (t-tests)","text":"Assumptions t-test:parent population sample drawn normal.parent population sample drawn normal.sample random sample.sample random sample.population standard deviation, œÉ unknown.population standard deviation, œÉ unknown.","code":""},{"path":"small-sample-tests.html","id":"test-for-a-single-population-mean-1","chapter":"14 Small sample tests","heading":"14.1.1 Test for a single population mean","text":"Consider population mean, say Œº; Œº unknown, take random sample size n population calculate sample mean, denoted \\(\\overline{x}\\). want test whether population mean Œº, unknown equal known constant Œº0, based sample mean \\(\\overline{x}\\). sample size less 30.null hypothesis tested isH0 : Œº = Œº0The alternative hypothesis may eitherH1 : Œº < Œº0 (called left tailed alternative)OrH1 : Œº > Œº0 (called right tailed alternative)OrH1 : Œº ‚â† Œº0 (called two tailed alternative)\\[t = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}}\\], \\(s^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1}\\)null hypothesis t follows t distribution n-1 degrees freedom","code":""},{"path":"small-sample-tests.html","id":"decision-rule-for-t-test","chapter":"14 Small sample tests","heading":"14.1.2 Decision rule for t test","text":"Let t calculated value, degrees freedom = n-1, Œ± level significance, reject null hypothesis |t| > tŒ±/2 ; two tailed test|t| > tŒ±/2 ; two tailed testt > tŒ± ; right tailed testt > tŒ± ; right tailed testt < - tŒ± ; left tailed testt < - tŒ± ; left tailed testWhere tŒ± tŒ±/2 can obtained table Student t distribution given degrees freedom, n-1 level significance Œ±. calculated value test statistic less critical values table. may reject null hypothesis. Otherwise, may accept .Example 9:Based field experiments, new variety green gram expected give yield 12 quintals per hectare. variety tested 10 randomly selected farmers‚Äô fields. yields (quintals per hectare) recorded 14.3, 12.6, 13.7, 10.9, 13.7, 12, 11.4, 12, 12.6, 13.1. results confirm expectation?Solution:Null hypothesis, H0 : Œº = 12Alternate hypothesis, H1 : Œº ‚â† 12; two tailed testSample size (n) = 10Sample mean, \\(\\overline{x}\\) =\\(\\frac{\\sum_{= 1}^{n}x_{}}{n} = (14.3+12.6+...+13.1)/10 = 126.3/10=12.63\\)Sample standard deviation (s) = 1.08536Œº0 = 12Level significance, Œ± = 0.05Calculation sample mean sample standard deviation\\[t = \\frac{\\overline{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n - 1}}}\\]\\[t = \\frac{12.63 - 12}{\\frac{1.085306}{\\sqrt{10 - 1}}} = \\frac{0.63}{0.3432} = 1.835\\]Table value t corresponding 5% level significance 9 degrees freedom 2.262 (two tailed test) ‚Äì see t table (Fig: 14.1) end chapter.Since calculated value (1.835) less table value (2.262), conclude , don‚Äôt enough evidence reject null hypothesis. , can stated mean 12 quintals per hectare.Example 10: Try yourselfThe mean weekly sales soap bars departmental stores 146.3 bars per store. advertising campaign mean weekly sales 22 stores typical week 153.7 showed standard deviation 17.2. advertisement campaign successful?","code":""},{"path":"small-sample-tests.html","id":"test-for-equality-of-two-means-1","chapter":"14 Small sample tests","heading":"14.2 Test for equality of two means","text":"Let two normally distributed populations means ¬µ1 ¬µ2. Let population standard deviations equal unknown. Let samples sizes n1 n2 taken populations. Let sample means ùë•ÃÖ1 ùëéùëõùëë ùë•ÃÖ2 respectively. want test whether population means significantly different based sample means.two cases situationPopulation variances equalPopulation variances equalPopulation variances unequalPopulation variances unequalBefore proceeding t-test F test performed test homogeneity population variance (See section 14.6).","code":""},{"path":"small-sample-tests.html","id":"case-when-the-population-variances-are-equal-homogenous","chapter":"14 Small sample tests","heading":"14.2.1 Case when the population variances are equal (homogenous)","text":"null hypothesis tested isH0 : Œº1 = Œº2The alternative hypothesis may eitherH1 : Œº1 < Œº2 (called left tailed alternative)OrH1 : Œº1> Œº2 (called right tailed alternative)OrH1 : Œº1‚â† Œº2 (called two tailed alternative)calculate test statistic, \\(t\\) using following formula.\\[t = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{s\\sqrt{\\left( \\frac{1}{n_{1}} + \\frac{1}{n_{2}} \\right)}}\\], \\(s^{2} = \\frac{(n_{1}-1)s_{1}^2+(n_{2}-1)s_{2}^2}{n_{1} + n_{2} - 2}\\); \\({\\overline{x}}_{1}\\) \\({\\overline{x}}_{2}\\) sample means population 1 & 2, respectively.null hypothesis t follows t distribution \\(n_{1} + n_{2} - 2\\) degrees freedom. Decision rule previous t- test (section 14.1.2).","code":""},{"path":"small-sample-tests.html","id":"case-when-the-population-variances-are-unequal","chapter":"14 Small sample tests","heading":"14.2.2 Case when the population variances are unequal","text":"Welch t-test adaptation Student‚Äôs t-test. used compare means two groups, variances different.null hypothesis tested isH0 : Œº1 = Œº2The alternative hypothesis may eitherH1 : Œº1 < Œº2 (called left tailed alternative)OrH1 : Œº1> Œº2 (called right tailed alternative)OrH1 : Œº1‚â† Œº2 (called two tailed alternative)calculate test statistic, \\(t\\) using following formula.\\[t = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{\\sqrt{\\left( \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} \\right)}}\\]\\(s_{1}\\)\\({s}_{2}\\) sample standard deviations two populations, respectively.degrees freedom Welch t-test calculated follows:\\[\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} \\right)^{2}}{\\frac{s_{1}^{4}}{n_{1}^{2}\\left( n_{1} - 1 \\right)} + \\frac{s_{2}^{4}}{n_{2}^{2}\\left( n_{2} - 1 \\right)}}\\]t value determined, read t table critical value Student‚Äôs t distribution corresponding significance level. Decision rule previous t- test (section 14.1.2).Example 11:order compare effectiveness two sources nitrogen, namely ammonium chloride urea grain yield paddy, experiment conducted. results grain yield paddy (kg/plot) two treatments given .Ammonium chloride: 13.4, 10.9, 11.2, 11.8, 14, 15.3, 14.2, 12.6, 17, 16.2, 16.5, 15.7Urea: 12, 11.7, 10.7, 11.2, 14.8, 14.4, 13.9, 13.7, 16.9, 16, 15.6, 16","code":""},{"path":"small-sample-tests.html","id":"paired-t-test","chapter":"14 Small sample tests","heading":"14.3 Paired t-test","text":"Paired Student‚Äôs t-test used compare means two related samples. two values (pair values) samples. example, 20 cows received treatment 3 months. question test whether treatment impact milk yield cow end 3 months treatment. milk yield 20 cows measured treatment. gives us 20 sets values treatment 20 sets values treatment. case, order test whether significant difference , paired t-test can used; two sets values compared related. pair values cow (one treatment).Suppose two correlated random samples x1, x2, ..., xn y1, y2, ..., yn. want test whether population means significantly different.Welch t-test adaptation Student‚Äôs t-test. used compare means two groups, variances different.null hypothesis tested isH0 : Œº1 = Œº2The alternative hypothesis may eitherH1 : Œº1 < Œº2 (called left tailed alternative)OrH1 : Œº1> Œº2 (called right tailed alternative)OrH1 : Œº1‚â† Œº2 (called two tailed alternative)calculate test statistic, \\(t\\) using following formula.\\[t = \\frac{|d^ÃÖ|}{\\frac{s}{\\sqrt{n}}}\\]\\(\\overline{d} = \\frac{\\sum_{= 1}^{n}d_{}}{n}\\); \\(d_{} = x_{} - y_{}\\), \\(s^{2} = \\frac{\\sum_{= 1}^{n}\\left( d_{} - \\overline{d} \\right)^{2}}{n - 1}\\)null hypothesis t follows t distribution \\(n - 1\\) degrees freedom. Decision rule previous t- test (section 14.1.2).Example 12:experiment plots divided two equal parts. One part received soil treatment second part received soil treatment B plot planted sorghum. sorghum yield (kg/plot) observed shown . Test effectiveness soil treatments sorghum yieldSolution:Null hypothesis, H0 : Œº1 = Œº2, , significant difference effects two soil treatmentsAlternate hypothesis, H1 : : Œº1‚â† Œº2; two tailed test, significant difference effects two soil treatmentsLevel significance, Œ± = 0.05\\[t = \\frac{|d|}{\\frac{s}{\\sqrt{n}}}\\]\\[t = \\frac{| - 2|}{\\frac{1.309}{\\sqrt{8}}}\\]\\[= \\frac{2}{\\frac{1.309}{2.828}}\\]\\[= \\frac{2}{0.4629}\\]\\[= 4.321\\]Table value t 7 degrees freedom 5% level significance 2.365As calculated value (4.321) greater table value (2.365). reject null hypothesis H0. conclude significant difference two soil treatments B. Soil treatment B increases yield sorghum significantly.Example 13: Try yourselfA certain stimulus administered 12 patients resulted following increase Blood pressure: 5, 2, 8, -1, 3, 0, -2, 1, 5, 0, 4, 6. Can concluded stimulus , general accompanied increase blood pressure? (tip: difference \\(d_{}\\) given)","code":""},{"path":"small-sample-tests.html","id":"testing-the-significance-of-correlation-coefficient","chapter":"14 Small sample tests","heading":"14.4 Testing the significance of correlation coefficient","text":"Let two normally distributed populations means ¬µ1 ¬µ2 standard deviations œÉ1 œÉ2 respectively. Let correlation two populations œÅ. want test null hypothesis population correlation coefficient zero (œÅ =0). can use t- test purpose. don‚Äôt enough evidence sample reject null hypothesis, may conclude significant correlation populations (œÅ ‚â† 0).null hypothesis tested isH0 : œÅ = 0The alternative hypothesisH1 : œÅ ‚â† 0 (two tailed alternative)\\[t = \\frac{r\\sqrt{n - 2}}{\\sqrt{1 - r^{2}}}\\]null hypothesis t follows t distribution \\(n - 2\\) degrees freedom. reject null hypothesis, calculated value greater table value t corresponding \\(n - 2\\) degrees freedom level significance (Œ±). case Œ± = 0.05Example 14:coefficient correlation 0.2 derived random sample 625 pairs observations. Test whether population correlation coefficient significant .Solution:Null hypothesis, H0 : œÅ = 0 (Population correlation coefficient zero)Alternative hypothesis, H1 : œÅ ‚â† 0 (Population correlation coefficient zero)Sample correlation coefficient (\\(r\\)) = 0.2Number pairs (n) = 625\\[t = \\frac{r\\sqrt{n - 2}}{\\sqrt{1 - r^{2}}}\\]\\[= \\frac{0.2\\sqrt{625 - 2}}{\\sqrt{1 - 0.04}}\\  = 5.095\\]Sample size large (>30) t distribution can approximated z distribution. Critical value two tailed test 5% level significance 1.96. calculated value 1.96, reject null hypothesis conclude , significant correlation population.","code":""},{"path":"small-sample-tests.html","id":"chi-square-test-œá2","chapter":"14 Small sample tests","heading":"14.5 Chi square test (œá2)","text":"Chi-square tests based sampling distribution called chi-square distribution (œá2 distribution). œá2tests based following assumptionsThe sample observations independent.sample observations independent.total frequency reasonably large, say, greater 50.total frequency reasonably large, say, greater 50.theoretical cell frequencies less 5. theoretical cell frequency less 5, application œá2tests, pooled preceding succeeding frequencies pooled frequency 5 finally adjust degrees freedom lost pooling.theoretical cell frequencies less 5. theoretical cell frequency less 5, application œá2tests, pooled preceding succeeding frequencies pooled frequency 5 finally adjust degrees freedom lost pooling.Constraints cell frequencies linear. (eg., ‚àë ùëÇùëñ = ‚àë ùê∏ùëñ (Oi Ei represents observed expected frequencies)Constraints cell frequencies linear. (eg., ‚àë ùëÇùëñ = ‚àë ùê∏ùëñ (Oi Ei represents observed expected frequencies)Note:œá2 tests make assumptions regarding parent population observations taken. tests involve population parameter. Hence tests known non-parametric tests distribution free tests.Degrees freedom œá2 tests: Degrees freedom œá2 tests refers number independent variates make statistic. degrees freedom general total number observations less number independent constraints imposed observations. example, k number independent constraints set data n observations, degrees freedom = n-k.Three important chi-square tests:Chi-square test goodness fit.Chi-square test goodness fit.Chi-square test independence attributes.Chi-square test independence attributes.Chi-square test variance.Chi-square test variance.","code":""},{"path":"small-sample-tests.html","id":"chi-square-test-œá2-for-goodness-of-fit","chapter":"14 Small sample tests","heading":"14.5.1 Chi square test (œá2) for goodness of fit","text":"powerful test testing significance discrepancy theory experiment given Prof.¬†Karl Pearson 1900 known ‚Äúœá2 tests goodness fit‚Äù.want test null hypothesis, H0: significance theory experimentAgainst alternative hypothesis H1: significance theory experimentIf Oi (=1,2,...,n) set observed frequencies Ei (=1,2,...,n) corresponding set expected (theoretical) frequencies, Karl Pearson‚Äôs chi-square test statistic given \\[\\chi^{2} = \\sum_{= 1}^{n}\\frac{\\left( O_{} - E_{} \\right)^{2}}{E_{}}\\]Oi represents ith observed frequency Ei represents corresponding expected frequency according assumption regarding theory behind data. null hypothesis chi-square follows chi-square distribution n-1 degrees freedom.","code":""},{"path":"small-sample-tests.html","id":"decision-rule-for-goodness-of-fit-test","chapter":"14 Small sample tests","heading":"14.5.1.1 Decision rule for goodness of fit test","text":"Let \\(\\chi_{\\text{cal}}^{2}\\) calculated value, degrees freedom = n-1, Œ± level significance, reject null hypothesis \\(\\chi_{\\text{cal}}^{2}\\) > \\(\\chi_{\\text{tab}}^{2}\\); \\(\\chi_{\\text{tab}}^{2}\\) table value \\(\\chi^{2}\\)n-1 degrees freedom. case \\(\\chi^{2}\\) test one tailed test used.Example 15:plant genetics, interest may test whether observed segregation ratios deviate significantly mendelian ratios. situations want test agreement observed theoretical frequency, test called test goodness fit. cross parents genetic constitution AAbb aaBB, phenotypes sample classified follows:expected occur 9: 3: 3: 1 ratio. data agree theoretical ratio?Solution:\\[\\chi^{2} = \\sum_{= 1}^{n}\\frac{\\left( O_{} - E_{} \\right)^{2}}{E_{}}\\]\\[\\chi^{2} = 0.676\\]\\(\\chi_{\\text{cal}}^{2}\\)= 0.676, table value chi-square 4-1=3 degrees freedom 5% level significance 7.815. won‚Äôt reject null hypothesis, H0: significance theory experiment. Conclude data follows 9:3:3:1 ratio.Example 16: Try yourselfThe number yeast cells counted haemocytometer compared theoretical value given . experimental result support theory.","code":""},{"path":"small-sample-tests.html","id":"chi-square-test-œá2-for-independence-of-attributes","chapter":"14 Small sample tests","heading":"14.5.2 Chi square test (œá2) for independence of attributes","text":"Chi-square test independence checks whether two attributes likely related . example, chemical treatment germination can two attributes. want know whether chemical treatment influence germination, can use chi-square test. purpose, need data arranged form contingency table.","code":""},{"path":"small-sample-tests.html","id":"contingency-table","chapter":"14 Small sample tests","heading":"14.5.2.1 Contingency table","text":"contingency table consists collection cells containing counts. contingency table tabular representation categorical data. contingency table usually shows frequencies particular combinations values two discrete random variables X Y. cell table represents mutually exclusive combination X-Y values.Example 17: Contingency tableIn order determine possible effect chemical treatment rate germination cotton seeds pot culture experiment conducted. results given form contingency table given . (X = Germination, Y = Chemical Treatment). Attribute X two class X1 = Germinated, X2 = germinated. Attribute Y two class Y1 = Treated, Y2 = Untreated.Let us consider two attributes & B, divided r classes A1, A2, ..., Ar B divided s classes B1, B2, ..., Bs. various cell frequencies can expressed form table (called r √ó s contingency table) shown .(AiBj) = number persons (items) possessing attributes Ai (=1,2,..., r) Bj (j =1,2,...,s)(Ai) = number persons (items) possessing attribute Ai ( =1,2,..., r)(Bj) = number persons (items) possessing attribute Bj (j =1,2,..., s)‚àë()ùëñ = ‚àë(B)ùëó = ùëÅ, total frequency.","code":""},{"path":"small-sample-tests.html","id":"expected-frequencies","chapter":"14 Small sample tests","heading":"14.5.2.2 Expected frequencies","text":"expected frequencies corresponding observed frequency (AiBj) calculated formula,\\[E_{\\text{ij}} = \\frac{\\left( A_{} \\right)\\left( B_{j} \\right)}{N}\\]","code":""},{"path":"small-sample-tests.html","id":"degrees-of-freedom","chapter":"14 Small sample tests","heading":"14.5.2.3 Degrees of freedom","text":"Degrees freedom r √ó s contingency table = (r ‚Äì 1)(s ‚Äì 1)Test procedureThe null hypothesis tested H0: two attributes consideration independent.alternative hypothesis H1: two attributes consideration independent.Test statistic used \\[\\chi^{2} = \\sum_{= 1}^{r}{\\sum_{j = 1}^{s}\\frac{\\left( O_{\\text{ij}} - E_{\\text{ij}} \\right)^{2}}{E_{\\text{ij}}}}\\],\\(O_{\\text{ij}}\\) = observed frequencies\\(E\\_{\\text{ij}}\\)= Expected frequenciess = number rowsr = number columnsIt can verified \\(\\sum_{= 1}^{r}{\\sum_{j = 1}^{s}O_{\\text{ij}}} = \\sum_{= 1}^{r}{\\sum_{j = 1}^{s}E_{\\text{ij}}}\\)null hypothesis test statistic follows chi-square distribution (r ‚Äì 1)√ó(s ‚Äì 1) degrees freedom. Decision rule Chi square goodness fit.Example 18:survey, random sample 198 farms classified three classes according tenure status : owned, rented mixed. also classified according level soil fertility : high fertile, moderately fertile low fertile farms. results given . Test whether tenure status depends soil fertilitySolution:Calculation expected values (\\(E_{\\text{ij}})\\) cell multiplying corresponding row total column total divided total frequency table\\(\\chi_{\\text{cal}}^{2}\\)= 26.3, table value chi-square (3-1)(3-1) = 4 degrees freedom 5% level significance 9.488. Since calculated value greater table value, reject null hypothesis, conclude two attributes consideration independent.","code":""},{"path":"small-sample-tests.html","id":"chi-square-test-for-22-contingency-table","chapter":"14 Small sample tests","heading":"14.5.3 Chi-square test for 2√ó2 contingency table","text":"2 x 2 contingency tableWhen number rows number columns equal 2; termed 2 x 2 contingency table. following form shown example 17. General form can represented shown . Consider two attributes B classes A1, A2 B1, B2 respectively. , b, c, d frequencies cellR1, R2 C1, C2 row totals column totals respectively. n total number observations.case 2 x 2 contingency table \\(\\chi^{2}\\) can directly found using short cut formula.null hypothesis tested H0: two attributes consideration independent.alternative hypothesis H1: two attributes consideration independent.\\[\\chi^{2} = \\frac{n\\left( ad - bc \\right)^{2}}{C_{1}C_{2}R_{1}R_{2}}\\]null hypothesis test statistic follows chi-square distribution (2 ‚Äì 1) √ó (2 ‚Äì 1) = 1 degrees freedom.","code":""},{"path":"small-sample-tests.html","id":"yates-correction-for-continuity","chapter":"14 Small sample tests","heading":"14.5.3.1 Yate‚Äôs correction for continuity","text":"2 X 2 contingency table, number degrees freedom (2-1) √ó (2-1) = 1. one cell frequencies less 5, , use pooling method results \\(\\chi^{2}\\) 0 degrees freedom (1 degrees freedom lost due pooling) meaningless. case apply correction due Yates usually known Yates‚Äô correction continuity. Yate‚Äôs correction made adding 0.5 least cell frequency adjusting cell frequencies column row totals remain . formula test statistic equation (15) now modified given .Test statistic used \\[\\chi^{2} = \\frac{{n\\left( \\left| ad - bc \\right| - \\frac{n}{2} \\right)}^{2}}{C_{1}C_{2}R_{1}R_{2}}\\]Solution Example 17H0: treatment improve germination rate cotton seeds. (independent)H1: chemical treatment improves germination rate cotton seeds.\\[\\chi^{2} = \\frac{{300\\left( \\left| 118 \\times 40 - 22 \\times 120 \\right| - \\frac{300}{2} \\right)}^{2}}{238 \\times 62 \\times 140 \\times 160}\\]\\[= 3.927\\]\\(\\chi_{\\text{cal}}^{2}\\)= 3.927, table value chi-square (2-1) √ó (2-1) = 1 degrees freedom 5% level significance 3.841. Since calculated value less table value, don‚Äôt enough evidence reject null hypothesis. chemical treatment improve germination rate cotton seeds significantly.Example 19: Try yourselfIn experiment effect growth regulator fruit setting muskmelon, following results obtained. Test whether fruit setting muskmelon application growth regulator independent 5% level.","code":""},{"path":"small-sample-tests.html","id":"chi-square-test-for-a-population-variance","chapter":"14 Small sample tests","heading":"14.5.4 Chi-square test for a population variance","text":"Consider normal population mean, say Œº variance œÉ2, Œº œÉ2 unknown, take random sample size n population. want test whether population variance œÉ2, unknown equal known constant œÉ20, based sample variance.Null hypothesis H0: œÉ2 = œÉ20Against alternative hypothesis H1: œÉ2 > œÉ20The test statistic \\[\\chi^{2} = \\frac{ns^{2}}{\\sigma_{0}^{2}}\\]\\(s^{2} = \\frac{\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1}\\)sample varianceUnder null hypothesis test statistic follows chi-square distribution n-1 degrees freedom. Decision rule section 3.1.1Example 20: Try yourselfTest null hypothesis œÉ2 = 0.16 alternative hypothesis œÉ2 > 0.16, given \\(s^{2}\\) = 0.01719 random sample size 11 normal population.","code":""},{"path":"small-sample-tests.html","id":"f---test-for-testing-equality-of-two-population-variances","chapter":"14 Small sample tests","heading":"14.6 F - test for testing equality of two population variances","text":"Let two normally distributed populations means ¬µ1 ¬µ2 variances œÉ12 œÉ22 respectively. Let samples sizes n1 n2 taken populations. want test whether population variances significantly different based sample variances.Null hypothesis H0: œÉ21 = œÉ22Against alternative hypothesis H1: œÉ21 > œÉ22Test statistic \\[F = \\frac{s_{1}^{2}}{s_{2}^{2}}\\]null hypothesis test statistic follows F distribution \\(n_{1} - 1\\) \\(n_{2} - 1\\) degrees freedom.","code":""},{"path":"small-sample-tests.html","id":"decision-rule-for-f---test","chapter":"14 Small sample tests","heading":"14.6.1 Decision rule for F - test","text":"calculated value greater table value F specified level significance two degrees freedom (.e. \\(n_{1} - 1\\) \\(n_{2} - 1\\)) reject null hypothesis.Note:\\(s_{2}^{2} >\\) \\(s_{1}^{2}\\) test statistic \\[F = \\frac{s_{2}^{2}}{s_{1}^{2}}\\]null hypothesis test statistic follows F distribution \\(n_{2} - 1\\) \\(n_{1} - 1\\) degrees freedom.Example 20: Try yourselfFor random sample representing one normal population, \\(n_{1}\\) = 11, \\(s_{1}^{2}\\) = 21.87. another random sample representing second normal population, \\(\\ n_{2}\\)= 8 \\(s_{2}^{2}\\) = 15.36. Test equality variances.\nFigure 14.1: t table\n¬†\n¬†\n¬†","code":""},{"path":"design-of-experiments.html","id":"design-of-experiments","chapter":"15 Design of experiments","heading":"15 Design of experiments","text":"","code":""},{"path":"design-of-experiments.html","id":"introduction-1","chapter":"15 Design of experiments","heading":"15.1 Introduction","text":"Design Experiments integral component agricultural research. scientifically designed experiment valuable tool advancement gaining new knowledge technology development. ‚Äúeffective use tools statistical design experiments paved way green revolution‚Äù ‚Äì words father green revolution India, Dr.¬†M.S. Swaminathan, shows important design analysis experiments well statistical science agricultural experiments. carefully designed experiment able answer queries researcher accuracy reliability efficient use available resources experimenters. Thus, successful experimentation, highly desirable scientists researchers scientific disciplines, including agricultural sciences, understand basic principles designing experiment analysis resultant data completed experiment. may emphasized researcher always consult statistician , experimentation, convinced enough using design experiment analysis technique data.scientific investigation involves formulation certain assertions (hypotheses) whose validity examined data generated experiment conducted purpose. term ‚Äòexperiment‚Äô defined systematic procedure carried controlled conditions order discover unknown effect, test establish hypothesis, illustrate known effect.Experiments can designed many different ways collect information. Design experiments (DOE) systematic method determine relationship factors affecting process output process. words, used find cause--effect relationships.DOE structured approach conducting experiments. Mainly aims atValidityValidityReliabilityReliabilityReplicabilityReplicabilityOptimalityOptimality","code":""},{"path":"design-of-experiments.html","id":"a-simple-example","chapter":"15 Design of experiments","heading":"15.2 A simple example","text":"\nFigure 15.1: Poultry manure, cow dung coirpith compost\ndecided conduct experiment. Consider layman knowledge design experiments. , selected 3 potted plants experiment. 3 organic manures applied potted plants.\nFigure 15.2: Treatments given potted plants shown\n\nFigure 15.3: Yield observed plants\n, based experiment, can say poultry manure best?, based experiment, can say poultry manure best?somebody repeats experiment somewhere results different?somebody repeats experiment somewhere results different?variance due experimental error?variance due experimental error?experimenter wants show poultry manure best? allotted healthy plant poultry manure.experimenter wants show poultry manure best? allotted healthy plant poultry manure.Can experiment like validity?Can experiment like validity?Can make conclusion experiment?Can make conclusion experiment?Answer question gives importance proper designing experiments. Nobody scientific fraternity going accept experiment. experiment validity validity proved statistical theories. issues can well taken care proper designing experiments. discussing basic principles design, shown, experiment looks like proper designing.Design experiment means design experiment. sense observations measurements obtained answer query valid, efficient economical way. designing experiment analysis obtained data inseparable. experiment designed properly keeping mind question, data generated valid proper analysis data provides valid statistical inferences. experiment well designed, validity statistical inferences questionable may invalid.","code":""},{"path":"design-of-experiments.html","id":"importance-of-doe","chapter":"15 Design of experiments","heading":"15.3 Importance of DoE","text":"Reduce, control provides estimate experimental errorReduce, control provides estimate experimental errorGives structured approachGives structured approachIt reduce cost experiment considerable reliabilityIt reduce cost experiment considerable reliabilityProduces statistically valid resultsProduces statistically valid resultsAllows accommodate changesAllows accommodate changesReduce complexityReduce complexityImproves accountabilityImproves accountability","code":""},{"path":"design-of-experiments.html","id":"characteristics-of-a-good-design","chapter":"15 Design of experiments","heading":"15.4 Characteristics of a good design","text":"Provides unbiased estimates factor effects associated uncertaintiesProvides unbiased estimates factor effects associated uncertaintiesEnables experimenter detect important differencesEnables experimenter detect important differencesIncludes plan analysis reporting resultsIncludes plan analysis reporting resultsGives results easy interpretGives results easy interpretPermits conclusions wide validityPermits conclusions wide validityMinimal resource usageMinimal resource usageIs simple possibleIs simple possibleStatistical design experiments refers process\nplanning experiment appropriate data\ncollected analyzed statistical methods,\nresulting valid objective conclusions. \nstatistical approach experimental design necessary\nwish draw meaningful conclusions \ndata. problem involves data subject\nexperimental errors, statistical methods \nobjective approach analysis.Creation controlled conditions main\ncharacteristic feature experimentation DOE\nspecifies nature control operations \nexperiments. Proper designing ensures \nassumptions required appropriate interpretations\ndata satisfied thus increasing accuracy \nsensitivity results.two aspects experimental problem:\ndesign experiment statistical\nanalysis data. two subjects closely\nrelated method analysis depends\ndirectly design employed.\nFigure 15.4: DoE Statistical analysis\n","code":""},{"path":"design-of-experiments.html","id":"brief-history","chapter":"15 Design of experiments","heading":"15.5 Brief history","text":"statistical principles underlying design experiments pioneered R. . Fisher 1920s 1930s Rothamsted Experimental Station, agricultural research station around fourty kilometres north London. Fisher shown way draw valid conclusions field experiments nuisance variables temperature, soil conditions, rainfall present. introduced concept analysis variance (ANOVA) partitioning variation present data () due attributable factors, (b) due chance factors. methodologies colleague Frank Yates developed now widely used. methodologies profound impact agricultural sciences research.Though experimental design initially introduced agricultural context, method applied successfully industry since 1940s. George Box co-workers developed experimental design procedures optimizing chemical processes, particularly response surface designs chemical process industries.Recently, experimental designs also used clinical trials. evolved 1960s medical advances previously based unreliable data. example, doctors used examine patients publish papers based data. biases resulting kinds studies became known. led move toward making randomized double-blind clinical trial standard approval new product, medical device, procedure. scientific application valid designing analysis following proper statistical methods became important clinical trials.recently experimental design techniques started gaining popularity area computer-aided design engineering using computer/simulation models including applications manufacturing industries.","code":""},{"path":"design-of-experiments.html","id":"some-terms-involved","chapter":"15 Design of experiments","heading":"15.6 Some terms involved","text":"","code":""},{"path":"design-of-experiments.html","id":"treatments","chapter":"15 Design of experiments","heading":"15.6.1 Treatments","text":"term treatments used denote different objects, methods processes among comparison made. example, experimenter wants identify among objects/methods/process best based experiment; objects/methods/process called treatment. clearly anything compare experiment known treatment.examples treatments different kinds fertilizer agronomic experiments, different irrigation methods levels irrigation, different fungicides pest management experiments , doses different drugs chemicals laboratory experiments, different varieties crops, different pesticides, grazing systems animals, different tree species agro-forestry experiments, different concentrations solute chemical experiments etc.","code":""},{"path":"design-of-experiments.html","id":"control","chapter":"15 Design of experiments","heading":"15.6.2 Control","text":"control treatment standard treatment used baseline basis comparison treatments. control treatment might treatment currently use, might treatment . example, study new pesticides use standard pesticide control treatment, experiment involving fertilizers may one treatment fertilizers . clinical trials, control treatment generally placebo.","code":""},{"path":"design-of-experiments.html","id":"experimental-units","chapter":"15 Design of experiments","heading":"15.6.3 Experimental units","text":"Experimental units subjects objects treatments applied. example, plots land receiving fertilizer, groups animals receiving different feeds, batches chemicals receiving different temperatures, pots glasshouse experiments, Petri dishes tissues culture bacteria micro-organisms laboratory experiments, etc.","code":""},{"path":"design-of-experiments.html","id":"response","chapter":"15 Design of experiments","heading":"15.6.4 Response","text":"Responses measurable outcomes, observed applying treatment experimental unit. Alternatively, response measure find happened\nexperiment. experiment, may one response. examples responses grain yield straw yield, nitrogen content plants biomass plants, quality parameters produce, percentage plants infested disease, weight gain animals, etc.","code":""},{"path":"design-of-experiments.html","id":"factors","chapter":"15 Design of experiments","heading":"15.6.5 Factors","text":"Factors variables whose influence response variable studied experiment. one factor studied experiment experiment called single factor experiment. one factor studied simultaneously experiment, experiment called multi-factor factorial experiment. term factor commonly used case factorial experiments. example, temperature concentration chemicals chemical experiment two factors, Nitrogen, Phosphorus Potassium fertilizers three factors agronomic experiment. Dose time application chemical formulation two factors laboratory experiment.","code":""},{"path":"design-of-experiments.html","id":"factor-levels","chapter":"15 Design of experiments","heading":"15.6.6 Factor levels","text":"term factor levels simply levels used denote values settings factor takes factorial experiment. example, doses nitrogenous fertilizer 0 kg/ha, 30 kg/ ha, 80 kg/ha three levels factor fertilizer. 10%, 20%, 30%, 40% concentration solute solution four levels factor solute laboratory experiment. Presence polythene sheet surface soil absence two levels factor management practice water management study.","code":""},{"path":"design-of-experiments.html","id":"observational-unit","chapter":"15 Design of experiments","heading":"15.6.7 Observational Unit","text":"observational unit unit response variables measured. Observational units often experimental units, may true always. mistake confusing observational unit experimental unit leads pseudo-replication discussed paper (Hurlbert 1984). Consider experiment investigate effects ultraviolet (UV) levels growth smolt. experiment conducted two tanks one tank receives high levels UV light tank receives UV light. Fish placed tank end experiment growths individual fish measured. experiment, tanks experimental units observational units smolts. treatments, presence absence UV light, applied tanks individual fish whole group fish simultaneously exposed UV radiation. tank effect completely confounded treatment effect separated. Another example inorganic fertilizers applied plots field containing plants. time harvest, plants plot harvested. sample plants harvested. case plot experimental unit fertilizers applied observational units plants sampled.","code":""},{"path":"design-of-experiments.html","id":"experimental-error","chapter":"15 Design of experiments","heading":"15.7 Experimental error","text":"explain experimental error consider example given (Gomez Gomez 1984). Consider plant breeder wishes compare yield new rice variety standard variety B known tested properties. lays two plots equal size, side side, sows one variety variety B. Grain yield plot measured variety higher yield judged better. Despite simplicity common-sense appeal procedure just outlined, one important flaw. presumes difference yields two plots caused varieties nothing else. certainly true. Even variety planted plots, yield differ. factors, soil fertility, moisture, damage insects, diseases, birds also affect rice yields. factors affect yields, satisfactory evaluation two varieties must involve procedure can separate varietal difference sources variation. , plant breeder must able design experiment allows decide whether difference observed caused varietal difference factors.logic behind decision simple. Two rice varieties planted two adjacent plots considered different yielding ability observed yield difference larger expected, plots planted variety.Hence, researcher needs know yield difference plots planted different varieties, also yield difference plots planted variety. difference among experimental plots treated alike called experimental error. error primary basis deciding whether observed difference real just due chance. Clearly, every experiment must designed measure experimental error.Response experimental units receiving treatment may even similar conditions. variations responses may due various reasons. factors like heterogeneity soil, climatic factors genetic differences, etc also may cause variations (known extraneous factors).Definition: variations response caused extraneous factors known experimental error.aim designing experiment minimize experimental error.","code":""},{"path":"design-of-experiments.html","id":"basic-principles-of-design","chapter":"15 Design of experiments","heading":"15.8 Basic principles of design","text":"three basic principles designing experiment namely randomization, replication local control (blocking).","code":""},{"path":"design-of-experiments.html","id":"rand","chapter":"15 Design of experiments","heading":"15.8.1 Randomization","text":"Randomization means random assignment conditions study treatments subjects experimental units. principle randomization involves allocation treatment experimental units random avoid bias experiment resulting influence extraneous unknown factor may affect experiment.development analysis variance (ANOVA), assume errors random independent. turn, observations also become random randomization.observations independent identically distributed normal variate important assumption hypothesis testing problems involving test statistics F (Snedecor‚Äôs F) t (Student‚Äôs t). major purpose randomization.Randomization forms basis valid experiment replication also needed validity experiment. randomization process every experimental unit equal chance receiving treatment, called complete randomization.Consider example suppose want randomly allot 3 treatments 3 experimental units. ? easy; just label units 1 3. Make lot equal size labelling 1,2 3. Put labels bowl pick eyes closed. Now 1 comes; first treatment alloted 1st unit. simple technique randomization. Random number tables computer generated random numbers can also used.\nFigure 15.5: Taking lot bowl also procees randomization\n","code":""},{"path":"design-of-experiments.html","id":"rep","chapter":"15 Design of experiments","heading":"15.8.2 Replication","text":"replication principle, treatment repeated number times obtain valid reliable estimate possible one observation . Replication provides efficient way increasing precision experiment. precision increases increase number observations. Replication provides observations treatment used, \nincreases precision.Replication enables experimenter obtain valid estimate \nexperimental error. Estimate experimental error permits statistical\ninference; example, performing tests significance obtaining\nconfidence interval, etc. replication, \nresearcher able estimate experimental error. \nseen later chapters, estimated\nexperimental error null hypotheses tested.\nFigure 15.6: Treatments alloted four plots, replication treatment 2\nresults experiment shown Figure 1.7. yield kg per plot given bracket.\nFigure 15.7: yield kg per plot given bracket\nexperiment, experimental error can estimated \n\\(\\frac{\\left( 6 - 8 \\right)^{2} + {(5 - 4)}^{2}}{2} = \\frac{4 + 1}{2} = 2.5\\);\ndenominator 2 number replications.can also calculated square difference observation\ncorresponding treatment mean, mean \n\\(\\frac{6 + 8}{2} = 7\\); mean B \\(\\frac{5 + 4}{2} = 4.5\\). sum\nsquare difference observation treatment mean \ntaken shown \n\\(\\left( 6 - 7 \\right)^{2} + \\left( 8 - 7 \\right)^{2} + \\ \\left( 5 - 4.5 \\right)^{2} + \\ {(4 - 4.5)}^{2} = 2.5\\)Thus, replication helps estimate experimental error. Increasing \nsize experiment increasing replication also helps \nincrease precision estimating pairwise differences among \ntreatment effects. . Replication provides efficient way increasing\nprecision experiment. precision increases \nincrease number observations. Replication provides \nobservations treatment used, increases precision.","code":""},{"path":"design-of-experiments.html","id":"local","chapter":"15 Design of experiments","heading":"15.8.3 Local control (error control)","text":"good experiment incorporates possible means minimizing \nexperimental error; ability detect experimental error\nincreases size experimental error decreases. putting\nexperimental units similar possible together \ngroup (commonly referred block) assigning treatments\nblock separately independently, variation among blocks can\nmeasured removed experimental error. field experiments\nsubstantial variation within experimental field can \nexpected, significant reduction experimental error usually\nachieved use proper blocking.replication used local control reduce experimental\nerror. example, experimental units divided different\ngroups homogeneous within blocks, \nvariation among blocks eliminated ideally, error\ncomponent contain variation due treatments . \n, turn, increase efficiency.field experiment 4 treatments 5 replications. Consider field fertility gradient left right shown figure 1.8.\nFigure 15.8: field fertility gradient left right\nHomogeneity can achieved dividing field groups shown figure 1.9. Now vertical strips can considered block. Plots formed block, treatment allotted randomly. randomization performed blocks. can see example replication equal number blocks, equal 5. Randomization achieved blocks. Local control achieved grouping treatments homogeneous blocks, fertilizer gradient . typical example Randomized Block Design (RBD), discussed chapter.\nFigure 15.9: Plots grouped blocks\n","code":""},{"path":"design-of-experiments.html","id":"other-methods-of-error-control","chapter":"15 Design of experiments","heading":"15.9 Other methods of error control","text":"","code":""},{"path":"design-of-experiments.html","id":"border-effect","chapter":"15 Design of experiments","heading":"15.9.1 Border effect","text":"Plants outer areas borders plot get influence treatment applied adjacent plot, may alter response character interest plants (example yield plants may higher), phenomenon called border effect. example, plot particular fertilizer applied treatment adjacent one another fertilizer applied, due seepage plants boarder areas influence fertilizer adjacent plot, may affect yield attributes border plants. Usually taking observations, border plants discarded.","code":""},{"path":"design-of-experiments.html","id":"proper-plot-technique","chapter":"15 Design of experiments","heading":"15.9.2 Proper Plot Technique","text":"essential factors, treatments maintained uniformly experimental units. example, field experiments, required factors soil nutrients, solar energy, plant population, pest incidence, almost infinite number environmental factors maintained uniformly plots experiment. requirement impossible satisfy, however ensure variability among experimental plots minimum, important sources variability taken care using good plot technique. field experiments crops, important sources variability considered among plots treated alike soil heterogeneity, competition effects, mechanical errors.","code":""},{"path":"design-of-experiments.html","id":"data-analysis","chapter":"15 Design of experiments","heading":"15.9.3 Data Analysis","text":"Proper choice data analysis helps controlling error, blocking effective. Covariance analysis commonly used purpose. measuring one covariates- characters whose functional relationships character primary interest known, analysis covariance (ANCOVA) can reduce variability among experimental units adjusting values common value covariates.example, animal feeding trial, initial weight animals usually differs. Using initial weight covariate, final weight animals subjected various feeds (.e., treatments) can adjusted values attained experimental animals started body weight. , rice field experiment rats damaged test plots, covariance analysis rat damage covariate can adjust plot yields levels rat damage plot.¬†\n¬†\n¬†","code":""},{"path":"uniformity-trials.html","id":"uniformity-trials","chapter":"16 Uniformity trials","heading":"16 Uniformity trials","text":"Research worker perform experiments identical conditions \nobtain valid results even uniform land can\nselect, still inherent variations soil. , \norder maintain homogeneity, experimenter good idea \nnature extent fertility variation land. can \nobtained results known uniformity trials.Uniformity trials can also planned determine suitable size \nshape plot number plots block. Uniformity trials\nenable us idea fertility variation field.","code":""},{"path":"uniformity-trials.html","id":"how-uniformity-trial-is-performed","chapter":"16 Uniformity trials","heading":"16.1 How uniformity trial is performed","text":"Uniformity trial conducted know nature soil fertility\ngradient. uniformity trial, particular variety crop \nsown entire experiment field uniformly managed throughout \ngrowing season without applying fertilizer. time harvest \nsubstantial border removed sides field. \nremainder field divided small plots termed\nbasic units. size basic unit decided judgment,\ndepending crop. Smaller basic unit accurate study \nheterogeneity possible. produce basic unit \nharvested recorded separately basic unit. yield\ndifferences basic units taken measure soil\nheterogeneity study area.Several types analysis available evaluate pattern soil\nheterogeneity based uniformity trial data. discuss \nprocedures detail.","code":""},{"path":"uniformity-trials.html","id":"fertility-contour-map","chapter":"16 Uniformity trials","heading":"16.2 Fertility Contour Map","text":"approach describe heterogeneity land construct \nfertility contour map. simple informative presentation soil\nheterogeneity. constructed taking moving averages \nyields unit plots demarcating regions fertility \nconsidering areas, yield magnitude. Taking\nmoving average reduce large random variation expected small\nplots.","code":""},{"path":"uniformity-trials.html","id":"serial-correlation","chapter":"16 Uniformity trials","heading":"16.3 Serial Correlation","text":"Serial correlation procedure generally used test randomness \ndata set. However, also useful characterization \ntrend soil fertility using uniformity trial data. Horizontal \nvertical serial correlations calculated. correlations give\nidea whether fertility gradient pronounced horizontally \nvertically. low serial correlation indicates fertile areas occur\nspots high value indicates gradient.\\[r_{s} = \\frac{\\sum_{= 1}^{n}{X_{}X_{+ 1} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}}{\\sum_{= 1}^{n}{X_{}}^{2} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}\\]\\(X_{}\\) value ith basic unit \\(n\\) number \nbasic units.","code":""},{"path":"uniformity-trials.html","id":"mean-square-between-strips","chapter":"16 Uniformity trials","heading":"16.4 Mean square between strips","text":"method simpler compute objective \nserial correlation. Units first combined horizontal vertical\nstrips. Variability strips measured direction \nmean square strips. relative size two mean squares\nindicates whether fertility gradient pronounced horizontally \nvertically.\\[\\text{Sum square}\\left( \\text{Vertical}\\right)=\\frac{\\sum_{= 1}^{c}V_{}^{2}}{r} - \\frac{G^{2}}{n}\\ \\]\\[\\text{Sum square}\\left( \\text{Horizontal}\\right)=\\frac{\\sum_{= 1}^{c}H_{}^{2}}{c} - \\frac{G^{2}}{n}\\ \\]\\[\\text{mean square}\\left( \\text{Vertical} \\right) = \\frac{\\text{Sum square}\\left( \\text{vertical} \\right)}{c - 1}\\]\\[\\text{mean square}\\left( \\text{Horizontal} \\right) = \\frac{\\text{Sum square}\\left( \\text{Horizontal} \\right)}{r - 1}\\]\\(V_{},H_{}\\) sum total basic units vertical \nhorizontal strips respectively; \\(r\\) number rows \\(c\\) \nnumber columns. \\(n\\) total number basic units;\n\\(n = r \\times c\\). \\(G\\) total basic units.","code":""},{"path":"uniformity-trials.html","id":"fairfield-smiths-variance-law","chapter":"16 Uniformity trials","heading":"16.5 Fairfield Smith‚Äôs Variance Law","text":"Smith (1938) gave empirical relations variance plot size.\ndeveloped empirical model representing relationship \nplot size variance mean per plot. model given \nequation\\[V_{x} = \\frac{V_{1}}{x^{b}}\\]\\[\\log V_{x} = \\ \\log V_{1} - b\\log x\\]\\(x\\) number basic units plot, \\(V_{x}\\) variance \nmean per plot \\(x\\) units, \\(V_{1}\\) variance mean per plot \none unit, \\(b\\) regression coefficient. values \\(b\\) \ndetermined principle least squares. \\(b\\) called Smith‚Äôs\nindex soil heterogeneity. index gives single value \nquantitative measure heterogeneity area.","code":""},{"path":"uniformity-trials.html","id":"smith","chapter":"16 Uniformity trials","heading":"16.5.1 Smith‚Äôs index of soil heterogeneity","text":"Step-1 Combine basic units simulate plots different sizes\nshapes. Use combinations fit exactly whole\narea,.e. product simulated plots number basic units\nper plot must equal total number basic units.Step-2 simulated plots constructed Step-1,\ncompute yield total T sum basic units construct \nplot compute plot variance \\(V_{(x)}\\)\\[V_{(x)} = \\sum_{= 1}^{w}\\frac{{T_{}}^{2}}{x} - \\frac{\\left( G \\right)^{2}}{\\text{rc}}\\]\n\\(w = \\frac{\\text{rc}}{x}\\) total number simulated plots size/x\nbasic units. \\(r\\) number rows \\(c\\) number columns.\n\\(G\\) total basic units.Step-3 plot size shape, compute variance per unit\narea\\[V_{x} = \\frac{V_{(x)}}{rc - 1}\\]Step-4 plot size one shape, test \nhomogeneity -plot variances\n\\(\\mathbf{V}_{\\left( \\mathbf{x} \\right)}\\)determine significance \nplot orientation (plot-shape) effect, using F test \nchi-square test. found homogeneous, average \n\\(\\mathbf{V}_{\\mathbf{(x)}}\\) values plot shapes given size\ncomputed, otherwise \\(\\mathbf{V}_{\\mathbf{(x)}}\\)plot shapes \ngiven size used separately calculation.example, two plot shapes size 2m2i.e. 2 √ó 1m\n1 √ó 2m. plot \\(\\mathbf{V}_{\\mathbf{(x)}}\\)\ncalculated. Homogeneity tested using F test. non-significant\naverage \\(\\mathbf{V}_{\\mathbf{(x)}}\\) values plot shapes 2\n√ó 1m 1 √ó 2m calculated.Step-5 Using values variance per unit area\n\\(\\mathbf{V}_{\\mathbf{x}}\\) computed steps 3 4, estimate \nregression coefficient \\(\\mathbf{V}_{\\mathbf{x}}\\) plot size\n\\(\\mathbf{x}\\)Using Fairfield Smith‚Äôs Variance Law, can written taking\nlogarithm base e :\\[\\log V_{x} - \\log V_{1} = \\  - b\\log x\\]consider\\[{Y = \\log}V_{x} - \\log V_{1}\\]equation \\(\\log V_{x} - \\log V_{1} = \\  - b\\log x\\) can written \nform\\(\\mathbf{Y = \\ cX}\\) \n\\(\\mathbf{c = \\  - b;\\ X =}\\mathbf{\\log}\\mathbf{x}\\)\\(\\mathbf{b}\\) estimated fitting regression \\(\\mathbf{Y}\\) \n\\(\\mathbf{X}\\).Step-6 Obtain adjusted \\(\\mathbf{b}\\) computed\n\\(\\mathbf{b}\\) value using range\n\\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\),\n\\(\\mathbf{\\ }\\mathbf{x}_{\\mathbf{1}}\\), size basic unit \n\\(\\mathbf{n}\\) whole area size. Column 2 3 table 2.1 \nrange \\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\). get\ncomputed \\(b\\) value \n\\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\). can calculate \ncorresponding adjusted \\(b\\) using table 2.1. using interpolation \nfollows.Find L1 L2 , calculated b lies ;\nL1 ‚â§ bcal ‚â§ L2, L1 L2 values \ncomputed b column table 2.1. Let y1 , y2 value\ncorresponding L1 L2 range \n\\(\\frac{\\mathbf{x}_{\\mathbf{1}}}{\\mathbf{n}}\\) table 2.1. \nusing formulaAdjusted b value =\n\\(y_{1} + (b_{\\text{cal}} - L_{1})\\frac{\\left( y_{2} - y_{1} \\right)}{\\left( L_{2} - L_{1} \\right)}\\)\nTable 16.1: Adjusted b table. Obtain value b using range values computed b value\nrelatively low value calculated adjusted Smith‚Äôs index soil\nheterogeneity (\\(\\mathbf{b}\\)) indicates relatively high degree \ncorrelation among adjacent plots study area, indicates\nchange level soil fertility tends gradual rather\npatches. Even though \\(\\mathbf{b}\\) regression coefficient\nvalue lies 0 1.","code":""},{"path":"uniformity-trials.html","id":"mcm","chapter":"16 Uniformity trials","heading":"16.6 Maximum Curvature Method","text":"method used find optimal plot size experiment. \nmethod basic units uniformity trials combined form new\nunits. new units formed combining columns, rows .\nCombination columns rows done way columns \nrows left . set units, coefficient variation\n(CV) computed. curve plotted taking plot size (terms\nbasic units) X-axis CV values Y-axis graph\nsheet. point curve takes turn, .e., point \nmaximum curvature located inspection. value corresponding \npoint maximum curvature optimum plot size.","code":""},{"path":"uniformity-trials.html","id":"uniformity-trial-explained","chapter":"16 Uniformity trials","heading":"16.7 Uniformity trial explained","text":"Let us discuss detail procedures explained using \nexample. Consider rice crop field 12m √ó 17m uniformly managed\nthroughout growing season without applying fertilizer. time\nharvest border 1m removed sides field. \nresultant effective area now 10m √ó 15m, entire field divided \nbasic units size 1m √ó 1m, yield (gms) noted \nbasic unit. 150 basic units.\nFigure 16.1: Yield observed plots 1x1m size\nNote: r=15;c=10 total number basic units n=r√óc=150","code":""},{"path":"uniformity-trials.html","id":"fertility-contour-map-1","chapter":"16 Uniformity trials","heading":"16.7.1 Fertility contour map","text":"Also known soil productivity contour map. construction fertility\ncontour map explained using example.Step 1: Calculate moving averages 3m√ó3m basic units. .e\nincluding three basic units rows 3 basic units columns.\nFigure 16.2: Calculation moving averages 3m√ó3m basic units\nStep 2: Moving averages labeled shown . \ncalculation moving averages now 8 values row 13\nvalues column. Now dimension plot figure \ncan considered 1.25m √ó 1.154m (10/8 = 1.25 15/13 = 1.514)\nFigure 16.3: Moving averages recorded 3m√ó3m basic units\nStep 3: Similar areas given colours get fertility\ncontour map.\nFigure 16.4: Colouring scheme based range moving averages. can decided experimenter\n\nFigure 16.5: Coloured plot labelled moving averages\nFinal fertility contour map obtained shown figure 2.7. Now \ncan get idea fertility gradient field plan \ncreate blocks field.\nFigure 16.6: Final fertility contour map\nFertility contour map gives vague idea fertilizer gradient.\nprocedures may give better idea.","code":""},{"path":"uniformity-trials.html","id":"serial-correlation-1","chapter":"16 Uniformity trials","heading":"16.7.2 Serial Correlation","text":"Pair wise calculation vertical values experiment\nobservations shown figure 16.1\nFigure 16.7: Pair wise calculation vertical values\nUsing\\(r_{s} = \\frac{\\sum_{= 1}^{n}{X_{}X_{+ 1} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}}{\\sum_{= 1}^{n}{X_{}}^{2} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}\\)Vertical\n\\(r_{s} = \\frac{100859156 - \\frac{\\left( 122777 \\right)^{2}}{150}}{101325715 - \\frac{\\left( 122777 \\right)^{2}}{150}}\\)=\n0.438627Pair wise calculation Horizontal values shown figure 16.1\nFigure 16.8: Pair wise calculation horizontal values\nUsing\\[r_{s} = \\frac{\\sum_{= 1}^{n}{X_{}X_{+ 1} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}}{\\sum_{= 1}^{n}{X_{}}^{2} - \\frac{\\left( \\sum_{= 1}^{n}X_{} \\right)^{2}}{n}}\\]Horizontal\n\\(r_{s} = \\frac{100946042 - \\frac{\\left( 122777 \\right)^{2}}{150}}{101325715 - \\frac{\\left( 122777 \\right)^{2}}{150}}\\)=\n0.54317Both coefficients low indicates presence fertile areas\nspots. However, horizontal serial correlation coefficient \nlittle high vertical implying fertility gradient \nhorizontal direction vertical. relative magnitude two\nserial correlations , however, used indicate \nrelative degree gradients two directions.","code":""},{"path":"uniformity-trials.html","id":"mean-square-between-strips-1","chapter":"16 Uniformity trials","heading":"16.7.3 Mean square between strips","text":"\nFigure 16.9: Mean square strips uniformity trial data\n\\(G =\\) 122777, \\(n =\\) 150\\(\\text{Sum}\\ \\text{}\\ \\text{square}\\left( \\text{vertical} \\right)\\frac{\\sum_{= 1}^{c}V_{}^{2}}{r} - \\frac{G^{2}}{n} = \\frac{12686^{2} + 12816^{2} + \\ldots 12253^{2}}{15} - \\frac{122777^{2}}{150}\\)\n= 63971.47\\(\\text{Sum}\\ \\text{}\\ \\text{square}\\left( \\text{Horizontal} \\right)\\frac{\\sum_{= 1}^{c}H_{}^{2}}{c} - \\frac{G^{2}}{n} = \\frac{8874^{2} + 8877^{2} + \\ldots 7692^{2}}{10} - \\frac{7725^{2}}{150}\\)=\n341185.7733\\(\\text{mean}\\ \\text{square}\\left( \\text{vertical} \\right) = \\frac{63971.47}{9}\\)=\n7107.941481\\(\\text{mean}\\ \\text{square}\\left( \\text{Horizontal} \\right) = \\frac{341185.773}{14}\\)=\n24370.41238Results show horizontal-strip MS almost 3 times higher\nvertical-strip MS, indicating trend soil\nfertility pronounced along length along width \nfield.","code":""},{"path":"uniformity-trials.html","id":"smiths-index-of-soil-heterogeneity","chapter":"16 Uniformity trials","heading":"16.7.4 Smith‚Äôs index of soil heterogeneity","text":"Optimal plot size can identified using method. possible\nplot sizes optimum plot size can found using method explained \nsection 16.5.1. Table shows 9 different plot sizes created\nuniformity trial data figure 16.1.recommended reader first read section 16.5.1\nread example. , illustrate variance per\nunit area plot size 25m2 (denoted V25) calculated.\nwidth length plot 5m √ó 5m. entire plot area \nfigure 16.1 divided six plots size 25m2 shown\n.\nFigure 16.10: Uniformity trail plot figure 2.1 divided six plots\nNow consider equation plot variance\n\\(V_{(x)} = \\sum_{= 1}^{w}\\frac{{T_{}}^{2}}{x} - \\frac{\\left( G \\right)^{2}}{\\text{rc}}\\).\n\\(w = \\frac{\\text{rc}}{x}\\)total number simulated plots \nsize\\(\\text{\\ x}\\) basic units. \\(r\\) number rows \\(c\\) \nnumber columns. \\(G\\) total basic units.\nFigure 16.11: Totals six plots\n$V_{(25)} = - = $218767.7133Variance per unit area given \\(V_{x} = \\frac{V_{(x)}}{rc - 1}\\)$V_{25} = ¬† = $1468.24Coefficient variation calculated \n\\(\\frac{\\text{Standard\\ deviation}}{\\text{mean}} \\times 100\\), \nexample plot size 25. Standard deviation= \\(\\sqrt{1468.24}\\) =\n38.31762. Mean entire data set = 818.5133. Therefore C.V =\n\\(\\frac{38.31762}{818.5133} = 0.047\\). Similarly, can calculated \nplot sizes. Vx C.V plot sizes \nsummarized .plot size one shape (plot size 5 \none shape), test homogeneity -plot variances\nV(x), determine significance plot orientation\n(plot-shape) effect, using F test chi-square test. \nplot size whose plot-shape effect non-significant, compute \naverage Vx, values plot shapes proceed \nestimation Smith‚Äôs index soil heterogeneity.V(x) calculated plot sizesDegrees freedom F test \\((w_{1} - 1,\\ w_{2} - 1)\\), \n\\(w_{1}\\) \\(w_{2}\\) number plots particular shapes . ,\ndegrees freedom = (30-1, 30-1)=(29,29). F calculated ratio\nV(x) plot shapes. Fcal= \\(\\frac{456895.9}{329859.5}\\)\n=1.38. Calculated value F (Fcal) compared table value F.\nTable value F (29,29) degrees freedom, Ftable= 1.860. Since\nFcal < Ftable; Calculated F non-significant, take average \nVx plot shapes proceed. example proceeded\nwithout taking average. included example F test \nbetter understanding theory written.Now Smith‚Äôs index soil heterogeneity estimated followsFit linear regression Y = cX, estimate value c; c= -bEstimated value c -0.3952. Therefore, calculated value b =\n0.3952. case \\(\\text{}\\frac{x_{1}}{n} = \\frac{1}{150} = 0.0067\\).\nNow need find adjusted value explained step-6 section\n16.5.1 using table .value \n\\(\\frac{x_{1}}{n} = \\frac{1}{150} = 0.0067\\) 0.001 0.01.\\(b_{\\text{cal}}\\)= 0.3952\\(y_{1}\\) = 0.443\\(y_{2}\\) = 0.528\\(L_{1}\\)= 0.40\\(L_{2}\\) = 0.50Adjusted b value =\n\\(0.443 + (0.3952 - 0.40)\\frac{\\left( 0.528 - 0.443 \\right)}{\\left( 0.50 - 0.40 \\right)}\\)\n= 0.43892.ComputedbIf \\(b\\) value low indicates relatively high degree correlation\namong adjacent plots study area indicating gradual change \nsoil fertility. \\(b\\) 0.43892, moderate, \nindication gradient slight indication patches.","code":""},{"path":"uniformity-trials.html","id":"maximum-curvature-method","chapter":"16 Uniformity trials","heading":"16.7.5 Maximum Curvature Method","text":"method (see section 16.6) used find optimum plot size.\ncurve plotted taking plot size (terms basic units) \nX-axis CV values Y-axis.\nFigure 16.12: Relationship CV plot size\nFigure 16.12indicates plot size increases,\ncoefficients variation decreases decrease maximum \nsquare shape plot 5m√ó5m. took small data set \nillustrative purpose, clearly seen 5m√ó5m plot lowest\nCV value also lowest variance per basic unit area.\nFigure 16.13: Relationship variance per basic unit area plot size\nFigure 16.13 shows relationship variance per basic\nunit area Vx plot size (x)¬†\n¬†\n¬†","code":""},{"path":"analysis-of-variance-anova.html","id":"analysis-of-variance-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17 Analysis of Variance (ANOVA)","text":"Consider example three chemical fertilizers , B, C tested\npotted plants. Experimenter like identify, among \nfertilizer among , B C gives highest yield. Potted plants \nmaintained way; experimental conditions \nhomogenous. Now collecting yield data plants, can\nobserve variance .e., yield values . , \nvariation caused treatment experimental error. , \ntotal observed variance = variance due treatments + variance due \nerror. , example treatment error can considered \nsource variation data. Basically, experiment perform\nsample based make generalization population.\nNow, based experiment want test whether means \ntreatment , B C significantly different considering \nobserved difference chance taking account experimental\nerror variance.Analysis variance (ANOVA) statistical procedure used analyze\ndifferences among means, observed total variance \npartitioned components attributable different sources \nvariation. logic behind simple, much variation comes\ntreatment, likely mean treatments \ndifferent. variation compared experimental error\nvariance, larger ratio treatment variance error variance, \nlikely groups different means. term \"analysis \nvariance\" originates analysis uses variances determine\nwhether means different. ANOVA works comparing variance\ntreatments (group variance) error variance (within\ngroup variance).short ANOVA statistical hypothesis test determines whether\nmeans least two populations different. ANOVA developed\nstatistician Sir Ronald . Fisher.","code":""},{"path":"analysis-of-variance-anova.html","id":"null-hypothesis-in-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.1 Null hypothesis in ANOVA","text":"Null hypothesis ANOVA population means equal, \ndenoted H0: ¬µ1 = ¬µ2 =¬µ3 = ‚Ä¶, = ¬µk . Alternate\nhypothesis atleast pair treatment equal, H1: ¬µi ‚â† ¬µj, ‚â† j; , j = 1,2, ‚Ä¶, k. ¬µ1, ¬µ2, ¬µ3, ‚Ä¶, ¬µk k population means","code":""},{"path":"analysis-of-variance-anova.html","id":"degrees-of-freedom-1","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.2 Degrees of freedom","text":"proceeding, important understand concept degrees\nfreedom. Degrees freedom can defined number \nindependent observations free vary.Consider simple example, 7 hats want wear \ndifferent hat every day week. first day, can wear \n7 hats. second day, can choose 6 remaining\nhats, day 3 can choose 5 hats, . day 6 rolls\naround, still choice 2 hats haven‚Äôt worn yet\nweek. choose hat day 6, choice\nhat wear Day 7. must wear one remaining hat.\n7-1 = 6 days ‚Äúhat‚Äù freedom‚Äîhat wore \nvary.Degrees freedom can also defined number independent\nvalues, included calculation estimate. estimate\nsingle number expresses property population \nsample. can mean, median, standard deviation, variance \ncalculated sample. independent values (\nobservations) went formula calculation. quantity \nvalues called ‚Äúdegrees freedom‚Äù.Consider three observations 6, x 9. x unknown suppose \nknow mean 6. can say x exactly equal 3, \nmean 6. two values free vary third value depends\ntwo constraint mean 6. .e.\ntwo values changed third value also change.Now consider plant height 7 plants164, 173, 158, 179, 168, 187, 167.Mean: 170.85We can find standard deviation using two formulasSD= \\(\\frac{\\sum_{}^{}\\left( x_{} - \\overline{x} \\right)^{2}}{n}\\), \nn number observationsHere SD = 9SD= \\(\\frac{\\sum_{}^{}\\left( x_{} - \\overline{x} \\right)^{2}}{n - 1}\\),\nn-1 degrees freedomHere SD = 9.72It easy notice divide degrees freedom, make\nestimate standard deviation greater diving \nsample size. need make greater? ‚Äôve already\ncalculated mean, don‚Äôt use data order \nestimate standard deviation. depend piece \ninformation, last observation contribute \nstandard deviation. , don‚Äôt delete redundant data, \nunderestimate standard deviation population sample data. using degrees freedom denominator provides unbiased estimate population standard deviation.Degrees freedom also define probability distributions \ntest statistics various hypothesis tests. example, hypothesis\ntests use t-distribution, F-distribution, chi-square\ndistribution determine statistical significance. \nprobability distributions family distributions DF\ndefine shape. Hypothesis tests use distributions make\ndecisions null hypothesis.","code":""},{"path":"analysis-of-variance-anova.html","id":"mean-squares","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.3 Mean squares","text":"sum squares sum square difference \nobservation mean. Sum squares\n=\\(\\sum_{= 1}^{n}\\left( x_{} - \\overline{x} \\right)^{2}\\). , \n=1, 2, ‚Ä¶, n, mean \\(\\overline{x} = \\frac{\\sum_{= 1}^{n}x_{}}{n}\\).\nSum squares generates measure variability. One-way ANOVA \ncalculate sum squares observation get measure total\nvariability along within group group sum squares,\ngives idea (error) group (\ntreatments) variability respectively. Mean sum squares mean\nsquares obtained dividing sum squares corresponding degrees\nfreedom. Mean square provides unbiased estimate variance. \nexample, ANOVA error mean square (MSE) given \n\\(\\frac{\\text{group sum squares}}{\\text{error degrees freedom}}\\)\nprovides unbiased estimate error variance","code":""},{"path":"analysis-of-variance-anova.html","id":"test-statistic-f","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.4 Test Statistic F","text":"function sample values known statistic. example,\nsample mean, sample variance, sum sample values statistic,\nfunctions sample values. statistic \nused test hypothesis, known test statistic. Examples\ntest statistic t, F, œá2 etc. test statistic used ANOVA\nF.F-statistic ratio two variances named Sir\nRonald . Fisher. proved null hypothesis H0: ¬µ1 =\n¬µ2 =¬µ3 = ‚Ä¶, = ¬µk true ratio mean square treatment \nmean square error follows F distribution.F distribution right-skewed distribution. general, calculated\nF value ANOVA larger F critical value, can reject\nnull hypothesis.\nFigure 17.1: Distribution F different degrees freedom\n","code":""},{"path":"analysis-of-variance-anova.html","id":"assumptions-of-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.5 Assumptions of ANOVA","text":"Observations treatment group drawn randomly normally distributed population.Observations treatment group drawn randomly normally distributed population.populations observations drawn common variance :- Homogeneity variance.populations observations drawn common variance :- Homogeneity variance.samples drawn independently .samples drawn independently .Treatment effects additive nature.Treatment effects additive nature.result assumptions. experimental errors independent identically distributed (iid) Normal distribution mean 0 variance œÉ2. word can say \\(e_{}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"types-of-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6 Types of ANOVA","text":"Based model used sources variation studied ANOVA can classified following typesOne-way ANOVATwo-way ANOVAm-way ANOVA","code":""},{"path":"analysis-of-variance-anova.html","id":"one-way-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1 One-way ANOVA","text":"simplest type analysis variance known one way analysis variance, one source variation factor interest controlled effect elementary units observed. See example section 17.6.1.6 one-way ANOVA can employed. one source variation assumed causing variance error field.","code":""},{"path":"analysis-of-variance-anova.html","id":"one-way-anova-model-oneway","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.1 One-way ANOVA model {oneway}","text":"\\[Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}}\\]\n= 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. \\(Y_{\\text{ij}}\\) observed value ith\ntreatment jth replication, \\(\\tau_{}\\) effect ith treatment. \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"one-way-classification","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.2 One-way classification","text":"\nFigure 17.2: One way classification Data\n","code":""},{"path":"analysis-of-variance-anova.html","id":"total-sum-of-squares-partitioning","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.3 Total sum of squares partitioning","text":"ANOVA consists partitioning total variation Yij values\nvariation due treatments variation caused uncontrolled\nfactors (error variation). Therefore, can write,Variance Yij =\n\\(\\frac{1}{n}\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - \\bar{Y} \\right)^{2}\\)n =\nv x r, overall mean \\(\\bar{Y} = \\frac{G}{n}\\)Total Sum Squares = \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - \\frac{G^{2}}{n}\\); \n\\(\\frac{G^{2}}{n}\\) known correction factor (C.F.).","code":""},{"path":"analysis-of-variance-anova.html","id":"proof","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.3.1 Proof","text":"\\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - \\bar{Y} \\right)^{2}\\)\n= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}}^{2} - 2Y_{\\text{ij}}\\bar{Y} + {\\bar{Y}}^{2} \\right)\\)\n= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}}^{2} - 2n{\\bar{Y}}^{2} + n{\\bar{Y}}^{2} \\right)\\)= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}}^{2} - n{\\bar{Y}}^{2} \\right)\\)= \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - n\\left( \\frac{\\sum_{= 1}^{v}\\sum_{j = 1} Y_{\\text{ij}}}{n} \\right)^{2},\\)\ndenote \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}} = G\\)\nTotal Sum Squares = \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - \\frac{G^{2}}{n}\\)Total Sum Squares partitioned Sum Squares due Treatments Sum Squares due Error.","code":""},{"path":"analysis-of-variance-anova.html","id":"proof-1","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.3.2 Proof","text":"\\[\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - \\overline{Y} \\right)^{2}} = \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} + {\\overline{Y}}_{} - \\overline{Y} \\right)^{2}}\\]\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( \\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)^{2} + \\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2} + 2\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)\\left( {\\overline{Y}}_{} - \\overline{Y} \\right) \\right)}\\]\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)^{2} +}}\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2} + 2\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)}}}}\\]Let\n\\(\\left( Y_{\\text{ij}} - {\\overline{Y}}_{} \\right)^{2} = e_{\\text{ij}}^{2}\\),\n\\(\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}e_{\\text{ij}}} = 0\\)\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{{e_{\\text{ij}}}^{2} +}}\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2} + 2\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{e_{\\text{ij}}\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)}}}}\\]\\(\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{e_{\\text{ij}}\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)}} = 0\\),\nsince sum weighted residuals zero, equation\n\\[= \\sum_{= 1}^{v}{\\sum_{j = 1}^{r}{{e_{\\text{ij}}}^{2} +}}\\sum_{= 1}^{v}{\\sum_{j = 1}^{r}\\left( {\\overline{Y}}_{} - \\overline{Y} \\right)^{2}}\\]Total Sum Square = group sum square + group sum \nsquareTotal Sum Square = Error sum square + Treatment Sum Square","code":""},{"path":"analysis-of-variance-anova.html","id":"treatment-sum-of-square","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.4 Treatment sum of square","text":"Let us denote treatment totals T1, T2,¬†‚Ä¶.,TV .e \\({T}_{}{\\ =\\ }\\sum_{j = 1}^{r} Y_{\\text{ij}}\\) \n\\(T_{1}{\\ +\\ }{T}_{2}{\\ +\\ }\\text{T}_{3}{\\ +\\ }{T}_{n}\\text{ }\\text{‚Ä¶‚Ä¶}{+\\ }{T}_{v}{\\ =\\ }\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}} = G\\)Treatment Sum Squares, TSS =\n\\(\\frac{\\sum_{= 1}^{v}\\text{T}\\text{}^{2}}{r} - \\frac{G^{2}}{n}\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"oneway","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.5 Calculations for One-way ANOVA","text":"Total Sum Square (Total SS) = \\(\\sum_{= 1}^{v}\\sum_{j = 1}^{r} Y_{\\text{ij}}^{2} - \\text{C.F.}\\)\n(square observations, sum correction factor subtracted get total sum squares)Treatment Sum Squares (TSS) = \\(\\frac{\\sum_{= 1}^{v}\\text{T}\\text{}^{2}}{r} - \\text{C.F.}\\).\n(square treatment totals, sum correction factor subtracted get treatment sum squares )Error Sum Squares (ESS)= Total SS ‚Äì TSSCorrection factor (C.F.) = \\(\\frac{G^{2}}{n}\\), \\(G\\) = Grand Total observations.Mean sum squares sum squares divided corresponding\ndegrees freedoms (d.f.).Error degrees freedom = Total degrees freedom ‚Äì treatment degrees freedom.Error d.f.= \\(vr - 1 - \\left( v - 1 \\right) = vr - 1 - v + 1 = vr - v = v(r - 1)\\)\nFigure 17.3: One-way ANOVA table\nF ~ F (v-1),¬†v(r-1)MSE estimate error variance œÉ2 \n\\(\\sqrt{\\frac{\\text{MSE}}{r}}\\) standard error treatment mean.calculated F value ANOVA larger F critical value corresponding degrees freedoms critical value table, can reject null hypothesis. see section 20","code":""},{"path":"analysis-of-variance-anova.html","id":"example-of-one-way-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.1.6 Example of One-way ANOVA","text":"data tiller count transplanting recorded 4 hill sampling units fertilizer trial involving 6 treatments. Test significant difference treatments. treatments labeled T1, T2, T3, T4, T5 T6.Data arranged shown :\nFigure 17.4: One-way classified data tiller count\nCalculation:\nequal number observations taken treatments, r1 = r2 = ‚Ä¶ = r6 = 4\nFigure 17.5: One-way ANOVA calculation\n\nFigure 17.6: One-way ANOVA table\ncan see calculated value greater table value F. null hypothesis rejected conclude least pair treatments significantly different 5% level significance.","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.2 Two-way ANOVA","text":"one way ANOVA explained previous section, treatments constitute different levels single factor controlled experiment. , however, many situations response variable interest may affected one factor. example,Milk yield cow may affected differences treatments .e., feeds fed well differences breed cows.Milk yield cow may affected differences treatments .e., feeds fed well differences breed cows.Moisture contents butter prepared churning cream may affected different levels fat churning speed etc.Moisture contents butter prepared churning cream may affected different levels fat churning speed etc.two independent factors might effect response variable interest. two factors considered source variation data addition experimental error. compare means situation use two-way ANOVA.two-way classification, data classified according two different criteria factors. procedure analysis variance somewhat different one followed earlier. One can also study interaction two factors, interaction effect included two-way ANOVA model.","code":""},{"path":"analysis-of-variance-anova.html","id":"twoway1","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.2.1 Two-way ANOVA model","text":"\\[Y_{\\text{ij}} = \\mu + \\tau_{} +  \\gamma_{j}+ e_{\\text{ij}}\\]\n= 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. \\(Y_{\\text{ij}}\\) observed value response ith level \nfactor jth level factor B, \\(\\tau_{}\\) effect ith factor,\\(\\gamma_{j}\\) effect jth factor . \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-classification","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.2.2 Two-way classification","text":"Consider experiment v levels factor 1 r levels factor 2. Observations recorded shown . considered situation one observation per cell.\nFigure 17.7: Two way classification Data\n","code":""},{"path":"analysis-of-variance-anova.html","id":"twoway","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.2.3 Calculations for Two-way ANOVA","text":"SSA = Take sum squares totals level factor divide number levels factor B ‚Äì Correction FactorSSA = Take sum squares totals level factor divide number levels factor B ‚Äì Correction FactorSSB = Take sum squares totals level factor B divide number levels factor ‚Äì Correction Factor.SSB = Take sum squares totals level factor B divide number levels factor ‚Äì Correction Factor.Total SS = Take sum squares observations ‚Äì Correction FactorTotal SS = Take sum squares observations ‚Äì Correction FactorError SS = Total SS ‚Äì SSA ‚Äì SSBError SS = Total SS ‚Äì SSA ‚Äì SSBError Degrees Freedom = Total D.F. ‚Äì Factor (D.F.) ‚Äì Factor B (D.F.)Error Degrees Freedom = Total D.F. ‚Äì Factor (D.F.) ‚Äì Factor B (D.F.)Error D.F.=\\(vr - 1 - \\left( v - 1 \\right) - \\left( r - 1 \\right) = vr - 1 - v + 1 - r + 1 = vr - v - r + 1 = (v - 1)(r - 1)\\)\nFigure 17.8: Two way ANOVA table\ntwo F values one factor one Factor B. FA ~ F (v-1),¬†(v-1)(r-1) FB ~ F (r-1),¬†(v-1)(r-1)Two-way ANOVA two null hypotheses one factor factor B. example Factor .\nH0: \\(\\mu\\)1 = \\(\\mu\\)2 =\\(\\mu\\)3 = ‚Ä¶., = \\(\\mu\\)v; \\(\\mu\\)population mean ith level factor .\nH1: atleast pair treatment means equal.\nFactor B\nH0: \\(\\beta\\)1 = \\(\\beta\\)2 =\\(\\beta\\)3 = ‚Ä¶., = \\(\\beta\\)r; \\(\\beta\\)j population mean jth level factor B.\nH1: atleast pair treatment means equal.Decisions null hypothesis made comparing corresponding F value table value explained section 20","code":""},{"path":"analysis-of-variance-anova.html","id":"example-of-two-way-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.6.2.4 Example of Two-way ANOVA","text":"Four chemicals S1, S2, S3, S4 administered 5 cow breeds (, B, C, D, E). observations taken peak lactation period. Average yield period taken. significant difference chemicals? significant difference milk yield breeds? Considering chemicals breeds independent.\nFigure 17.9: Two way classification observations chemicals breeds\n\\(\\ n = 20,\\ v = 5\\ \\ r = 4\\), factor = cow breeds; factor B =\nchemicals, Grand total (G)=874Correction Factor, \\(CF = \\frac{G^{2}}{n} = \\frac{\\left( 874 \\right)^{2}}{20} = 38193.8\\)Total Sum squares,\\(\\text{TSS} = \\text{sum squares observations} - CF = \\ \\text{38288} - \\text{CF} = \\text{94.2}\\)Sum Squares Breeds (SSA)\\(= \\sum_{=1}^{v}\\frac{T_{}^{2}}{r} - \\text{CF}\\)\n\\(= \\frac{1}{4}\\left\\lbrack \\left( \\text{179} \\right)^{2} + \\text{...} + \\left( \\text{172} \\right)^{2} \\right\\rbrack - \\text{CF} = \\text{34.7}\\)Sum Squares chemicals (SSB)\\(= \\sum_{j=1}^{r}\\frac{B_{j}^{2}}{v} - \\text{CF} = \\frac{1}{5}\\left\\lbrack \\left( \\text{217} \\right)^{2} + \\text{...} + \\left( \\text{212} \\right)^{2} \\right\\rbrack - \\text{CF} = \\text{32.2}\\)Sum squares Error\\(\\text{SSE} = \\text{TSS} - (\\text{SST} + \\text{SSB}) = \\text{27.3}\\)\nFigure 17.10: ANOVA table chemicals breeds\nchemicals breeds significance difference 5% level. F values calculated greater table values.","code":""},{"path":"analysis-of-variance-anova.html","id":"models-under-anova","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.7 Models under ANOVA","text":"Analysis variance based linear statistical model. linear model may eitherFixed effects modelFixed effects modelRandom effects modelRandom effects modelMixed effects modelMixed effects model","code":""},{"path":"analysis-of-variance-anova.html","id":"fix","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.7.1 Fixed effects model","text":"Fixed effects model statistical model model parameters\nfixed non-random quantities. using fixed effect model \nagricultural experiments, assume treatment effect unknown\nconstants.Fixed effects model can explained using example. Consider \nexperiment experimenter wants know whether three fields \nimpact yield particular strain wheat. Observations \ntaken 12 plots fields (replication). AOV (Analysis \nVariance) model \\[Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}};\\] \\[e_{ij}\\sim iid\\ N(0,\\sigma^{2})\\], = 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. example\nt =3 r =12. \\(Y_{\\text{ij}}\\) observed value ith\nfield jth plot, \\(\\tau_{}\\) effect ith treatment,\nconsidered unknown constant. \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects, independently normally distributed mean 0\nconstant variance œÉ2. model, field ‚Äúfixed effect‚Äù.\nAlong \\(\\mu\\), fixed parameters \\(\\tau_{1}\\),\\(\\tau_{2}\\) \n\\(\\tau_{3}\\) quantities interest. Using model,\nexperimenter can make decisions treatment (field) tested.","code":""},{"path":"analysis-of-variance-anova.html","id":"random-effects-model","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.7.2 Random effects model","text":"random effects model, also called variance components model, \nstatistical model model parameters random variables.\nConsider example wheat fields, random effect model\nused , fields assumed randomly sampled \npopulation fields area AOV (Analysis Variance)\nmodel \\[Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}};\\]\\[e_{ij}\\sim iid\\ N(0,\\sigma^{2}); \\tau_{}\\sim N(0,\\ \\sigma_{\\tau}^{2})\\], = 1,2, ‚Ä¶, t j =1,2, ‚Ä¶, r. \\(\\tau_{}\\) \nconsidered random variable,\n\\(\\tau_{}\\sim N(0,\\ \\sigma_{\\tau}^{2})\\); \n\\(e_{ij}\\sim N(0,\\ \\sigma^{2})\\). model, field ‚Äúrandom\neffect‚Äù. statistical model describes whole ensemble possible\nrepetitions experiment region fields \nselected. Experimenter make generalisations fields \nparticular region based experiment. One important\nconsequence random effects responses (\\(Y_{\\text{ij}}\\)'s)\nlonger independent. random \\(\\tau_{}\\)'s induce correlations\namong responses. responses jointly multivariate normal\ndistribution.","code":""},{"path":"analysis-of-variance-anova.html","id":"mixed-effects-model","chapter":"17 Analysis of Variance (ANOVA)","heading":"17.7.3 Mixed effects model","text":"mixed model, mixed-effects model mixed error-component model \nstatistical model containing fixed effects random effects.¬†\n¬†\n¬†","code":""},{"path":"multiple-comparison-test.html","id":"multiple-comparison-test","chapter":"18 Multiple comparison test","heading":"18 Multiple comparison test","text":"ANOVA omnibus test. Omnibus test, general name, refers overall global test. Omnibus test means implemented overall hypothesis tends find whether significant difference treatment means (case ANOVA) .ANOVA doesn‚Äôt tell treatment pairs different; clearly rejecting null hypothesis ANOVA, conclude least pair treatment means different, didn‚Äôt give idea treatments different. order identify treatment means significantly different one need employ multiple comparison tests post-hoc tests.Multiple comparison tests also known post-hoc tests. significant omnibus F test ANOVA procedure, advance requirement conducting post-hoc comparison, otherwise comparisons required.Post hoc tests (Multiple Comparison tests) planned tests conducted obtaining significant F test ANOVA. several methods performing post hoc tests, :-Fisher‚Äôs LSD test (Least Significant Difference test)Fisher‚Äôs LSD test (Least Significant Difference test)DMRT (Duncan‚Äôs Multiple Range Test)DMRT (Duncan‚Äôs Multiple Range Test)Tukey‚Äôs testTukey‚Äôs testBonferroni methodBonferroni methodDunnett methodDunnett methodScheffe‚Äôs testScheffe‚Äôs testNewman-Keuls methodNewman-Keuls method","code":""},{"path":"multiple-comparison-test.html","id":"why-do-we-need-a-post-hoc-test.","chapter":"18 Multiple comparison test","heading":"18.1 Why do we need a post-hoc test.","text":"Type error occurs H0 statistically rejected even though actually true, whereas type II error refers false negative. comparing treatment pairs individually post-hoc test. example, consider situation three treatments , B C compared, may form following three pairs: versus B, \nversus C, B versus C. pair comparison called ‚Äòfamily.‚Äô type error occurs family compared called ‚Äòfamily-wise error‚Äô (FWE).(Lee Lee 2018)example, one performs pairwise test two given groups B 5% Œ± level significance observed significantly different, chance correct decision made (perform type error) 95%. another pairwise tests groups B C simultaneously non-significant result, real probability correct decision made B, B C 0.95 √ó 0.95 = 0.9025, 90.25% , consequently, testing Œ± error 1 ‚àí 0.9025 = 0.0975,\n0.05. time, statistical analysis groups C also non significant result, probability non-significance three pairs (families) 0.95 √ó 0.95 √ó 0.95\n= 0.857 actual testing Œ± error 1 ‚àí 0.857 = 0.143, 14%. now chance type error increased testing several treatment pairs together. Multiple comparison tests devised way correction incorporated control inflation \\(\\alpha\\), actual level significance remain \nprescribed rate (case 0.05).Inflated \\(\\alpha = 1 - \\left( 1 - \\alpha \\right)^{N}\\), \\(N\\) =\nnumber hypotheses testedThe inflation probability type error increases increase\nnumber comparisons.","code":""},{"path":"multiple-comparison-test.html","id":"lsd","chapter":"18 Multiple comparison test","heading":"18.2 Fisher‚Äôs LSD test","text":"first pairwise comparison technique developed Fisher 1935 called least significant difference (LSD) test. technique can used ANOVA F statistic significant. Fisher's least significant difference (LSD) procedure two-step\ntesting procedure pairwise comparisons several treatment groups. first step procedure, ANOVA performed. null hypothesis can rejected pre-specified level significance, ANOVA, second step procedure, pairwise comparisons treatment means performed.Steps Fisher‚Äôs LSD testANOVA performed null hypothesis rejected proceed \nstep 2ANOVA performed null hypothesis rejected proceed \nstep 2An LSD (Least Significant Difference) value calculated. \npair treatments, difference treatment means\ngreater LSD value, conclude treatment means \nsignificantly different.LSD (Least Significant Difference) value calculated. \npair treatments, difference treatment means\ngreater LSD value, conclude treatment means \nsignificantly different.Note among agricultural researchers India use term CD (Critical Difference) instead LSD, terms used interchangeably.","code":""},{"path":"multiple-comparison-test.html","id":"lsd-least-significant-difference","chapter":"18 Multiple comparison test","heading":"18.2.1 LSD (Least Significant Difference)","text":"Consider two treatments \\(T_{}\\) \\(T_{j}\\) replications \\(r_{}\\)\n\\(r_{j}\\). LSD CD given formula given \\[\\text{LSD} = \\ t_{\\frac{\\alpha}{2},edf}.SE(d)\\]\\[SE(d) = \\ \\sqrt{\\text{MSE}\\left( \\frac{1}{r_{}} + \\frac{1}{r_{j}} \\right)}\\], \\(\\text{MSE}\\) Mean Square Error ANOVA.\n\\(t_{\\frac{\\alpha}{2},edf}\\) critical value two tailed student‚Äôs t distribution \\(\\alpha\\) level significance error degrees \nfreedom.replications \\(r_{} = r_{j} = r\\), \\[\\text{SE}\\left( d \\right) = \\ \\sqrt{\\text{MSE}\\left( \\frac{1}{r} + \\frac{1}{r} \\right)}\\  = \\ \\sqrt{\\text{MSE}\\left( \\frac{2}{r} \\right)} = \\sqrt{\\frac{2\\ MSE}{r}}\\]\\[LSD = \\ t_{\\frac{\\alpha}{2},edf}.\\sqrt{\\frac{2\\ MSE}{r}}\\]difference means two treatments \\(T_{}\\) \\(T_{j}\\) greater LSD say \\(T_{}\\) \\(T_{j}\\) significantly different specified level significance (\\(\\alpha)\\).","code":""},{"path":"multiple-comparison-test.html","id":"logic-behind-lsd-test","chapter":"18 Multiple comparison test","heading":"18.2.2 Logic behind LSD test","text":"\\(\\overline{T_{}}\\) \\(\\overline{T_{j}}\\) treatment means obtained experiment. \\(\\mu_{}\\) \\(\\mu_{j}\\) corresponding unknown population mean treatments, null hypothesis pairwise comparison isH0: \\(\\mu_{} = \\ \\mu_{j}\\).e. \\(\\mu_{} - \\ \\mu_{j\\ } = 0\\)t-test can used test hypothesis\\[t = \\frac{T_{} - T_{j}}{SE(d)}\\]\\(100\\left( 1 - \\alpha \\right)\\%\\) confidence interval mean difference \\(T_{} - T_{j}\\) given \n\\(\\left( T_{} - T_{j} \\right) \\pm t_{\\frac{\\alpha}{2},edf}SE(d)\\). \ncan written \\(\\left( T_{} - T_{j} \\right) \\pm \\text{LSD}\\). , \n\\(\\left( T_{} - T_{j} \\right) > \\text{LSD}\\) \n\\(100\\left( 1 - \\alpha \\right)\\%\\) confidence interval include 0.\ncan reject null hypothesis, population mean treatments equal \\(100\\left( 1 - \\alpha \\right)\\%\\) confidence.","code":""},{"path":"multiple-comparison-test.html","id":"advantages-and-disadvantages-of-lsd","chapter":"18 Multiple comparison test","heading":"18.2.3 Advantages and disadvantages of LSD","text":"LSD test also known protected Fisher's LSD test. Protection means perform calculations described null hypothesis rejected based ANOVA. first step sort controls false positive rate (Type error) entire family \ncomparisons (Hayter 1986).protected Fisher's LSD test first post-hoc test ever developed, longer recommended correct \\(\\alpha\\) inflation multiple comparisons effectively compared post-hoc tests. Still LSD common post-hoc\ntest used agricultural research experiments.Fisher's LSD procedure known preserve experiment wise type error rate nominal level significance, number treatments just three. (Meier 2006). , number treatments , recommended conduct DMRT Tukey‚Äôs test.Note multiple comparison tests (Bonferroni, Tukey, etc.) require ANOVA. results multiple comparisons tests valid even null hypothesis rejected based ANOVA.","code":""},{"path":"multiple-comparison-test.html","id":"duncans-multuiple-range-test-dmrt","chapter":"18 Multiple comparison test","heading":"18.3 Duncan‚Äôs Multuiple Range Test (DMRT)","text":"set treatments comparison possible pairs treatment means required can use DMRT. useful number compared large. standard error mean multiplied table values (rp) different values p (number treatment means involved) comparison made.Arrange treatment means decreasing increasing order along ranks.Arrange treatment means decreasing increasing order along ranks.Calculate standard error mean \\(SE(\\overline{Y})=\\sqrt{\\frac{\\ MSE}{r}}\\)Calculate standard error mean \\(SE(\\overline{Y})=\\sqrt{\\frac{\\ MSE}{r}}\\)statistical tables (known Studentised Ranges) note (rp) values p=2,3‚Ä¶ v treatments corresponding error degrees freedom.statistical tables (known Studentised Ranges) note (rp) values p=2,3‚Ä¶ v treatments corresponding error degrees freedom.Calculate shortest significant ranges (Rp) \\(R_p=r_p.SE(\\overline{Y})\\)Calculate shortest significant ranges (Rp) \\(R_p=r_p.SE(\\overline{Y})\\)largest mean subtract Rp (largest p). Declare means less value significantly different largest mean. remaining treatments whose values larger difference (Largest Mean- Largest Rp), compare differences appropriate Rp value.(two treatments remaining compare R2, three treatments R3 ).largest mean subtract Rp (largest p). Declare means less value significantly different largest mean. remaining treatments whose values larger difference (Largest Mean- Largest Rp), compare differences appropriate Rp value.(two treatments remaining compare R2, three treatments R3 ).second largest mean subtract second largest Rp compare treatments step v.second largest mean subtract second largest Rp compare treatments step v.Present results using either line notation (separate line homogeneous group) alphabet notation (alphabet representing homogeneous group) indicate treatments par significantly different.Present results using either line notation (separate line homogeneous group) alphabet notation (alphabet representing homogeneous group) indicate treatments par significantly different.¬†\n¬†\n¬†","code":""},{"path":"single-factor-experiments.html","id":"single-factor-experiments","chapter":"19 Single factor experiments","heading":"19 Single factor experiments","text":"Experiments single factor varies others kept constant called single-factor experiments. experiments, treatments consist solely different levels single variable factor. factors maintained uniformly experimental units.Example:experiment find best nitrogen level getting high yield, 5 different nitrogen levels tested. nitrogen levels treatments factors like irrigation, light, fertility gradient assumed homogenous experimental units (plots experimental units).experiment find best nitrogen level getting high yield, 5 different nitrogen levels tested. nitrogen levels treatments factors like irrigation, light, fertility gradient assumed homogenous experimental units (plots experimental units).experiment find best feed formulation milk yield cows 4 feed formulas. feed formula factor varies 4 levels. factors like breed cow, age etc kept constant.experiment find best feed formulation milk yield cows 4 feed formulas. feed formula factor varies 4 levels. factors like breed cow, age etc kept constant.","code":""},{"path":"single-factor-experiments.html","id":"crd","chapter":"19 Single factor experiments","heading":"19.1 Completely Randomized Design (CRD)","text":"Completely Randomized Design basic single factor design. \ndesign treatments assigned completely random \nexperimental unit chance receiving one treatment.\nCRD appropriate experiments homogeneous experimental\nunits, laboratory green house experiments, \nenvironmental effects relatively easy control. field\nexperiments, generally large variation among experimental\nplots, CRD rarely used.CRD provides layout conducting experiments, experimental\nunits homogeneous. two basic principles design,\nrandomization 15.8.1 replication15.8.2 followed \nCRD. Local control15.8.3 required experimental\nunits homogeneous. Treatments can unequal replications CRD.","code":""},{"path":"single-factor-experiments.html","id":"random1","chapter":"19 Single factor experiments","heading":"19.1.1 Randomization procedure","text":"Consider experiment involving four treatments , B, C, D CRD. treatment replicated 5 times. Randomization procedure explained .Determine total number experimental units (\\(n\\)) product number treatments (\\(t\\)) number replications (\\(r\\)); , \\(n = r\\  \\times t\\). example, \\(n = 5 \\times 4 = 20\\)Determine total number experimental units (\\(n\\)) product number treatments (\\(t\\)) number replications (\\(r\\)); , \\(n = r\\  \\times t\\). example, \\(n = 5 \\times 4 = 20\\)Assign unit number experimental units convenient manner; example, consecutively 1 n.¬†example, unit numbers 1, 2,... , 20 assigned 20 experimental units shown Figure 19.1Assign unit number experimental units convenient manner; example, consecutively 1 n.¬†example, unit numbers 1, 2,... , 20 assigned 20 experimental units shown Figure 19.1Assign treatments experimental plots using randomization schemes. explain simple randomization scheme section 19.1.1.1.Assign treatments experimental plots using randomization schemes. explain simple randomization scheme section 19.1.1.1.\nFigure 19.1: Numbered experimental units\n","code":""},{"path":"single-factor-experiments.html","id":"random","chapter":"19 Single factor experiments","heading":"19.1.1.1 Randomization by drawing lots","text":"Prepare \\(n\\) identical pieces paper divide \\(t\\) groups, group \\(r\\) pieces paper. Label piece paper group letter (number) corresponding treatment. Uniformly fold \\(n\\) labeled pieces paper, mix thoroughly, place container. example, 20 pieces paper, five treatments , B, C, D appearing .Prepare \\(n\\) identical pieces paper divide \\(t\\) groups, group \\(r\\) pieces paper. Label piece paper group letter (number) corresponding treatment. Uniformly fold \\(n\\) labeled pieces paper, mix thoroughly, place container. example, 20 pieces paper, five treatments , B, C, D appearing .Draw one piece paper time, without replacement constant shaking container draw mix content. example, label corresponding sequence piece paper drawn. sequence number can considered experimental unit number treatment assigned.Draw one piece paper time, without replacement constant shaking container draw mix content. example, label corresponding sequence piece paper drawn. sequence number can considered experimental unit number treatment assigned.\nFigure 19.2: Sequence labelled lots drawn\n","code":""},{"path":"single-factor-experiments.html","id":"layout-of-crd","chapter":"19 Single factor experiments","heading":"19.1.2 Layout of CRD","text":"allotting treatment according randomization procedure explained section 15.8.1. final layout look like figure 19.3.\nFigure 19.3: Layout Completely Randomized Design\n","code":""},{"path":"single-factor-experiments.html","id":"statistical-model-for-crd","chapter":"19 Single factor experiments","heading":"19.1.3 Statistical model for CRD","text":"Consider CRD v treatments r replications. Statistical model CRD one-way ANOVA (17.6.1.5).\\[Y_{\\text{ij}} = \\mu + \\tau_{} + e_{\\text{ij}}\\]\n= 1,2, ‚Ä¶, v j =1,2, ‚Ä¶, ri. \\(Y_{\\text{ij}}\\) observed value ith\ntreatment jth replication, \\(\\tau_{}\\) effect ith treatment. \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"single-factor-experiments.html","id":"analysis-of-crd","chapter":"19 Single factor experiments","heading":"19.1.4 Analysis of CRD","text":"Analysis CRD explained section 17.6.1.5 one-way ANOVA.Note unequal replication (.e.¬†treatment can different replication) possible CRD. calculating TSS, treatment total square divided corresponding replication sum taken. replication treatments, calculate TSS take sum squares treatment total divide replication.\nFigure 19.4: ANOVA table CRD\n","code":""},{"path":"single-factor-experiments.html","id":"postcrd","chapter":"19 Single factor experiments","heading":"19.1.4.1 Post hoc test (LSD)","text":"null hypothesis rejected ANOVA; proceed post-hoc test explained section 18.2. Fisher‚Äôs Least significant difference (LSD) test explained section 18.2 commonly employed test agricultural experiments.\nArrange treatment means descending order. Critical difference (CD) calculated treatment pair \\(T_{},\\ T_{j}\\) using following formula.\\[C.D. = t_{\\left( \\frac{\\alpha}{2},\\ \\ \\ error\\ df \\right)}\\sqrt{\\text{MSE}\\left( \\frac{1}{r_{}} + \\frac{1}{r_{j}} \\right)}\\] (CRD unequal replication)\\[C.D. = t_{\\left( \\frac{\\alpha}{2},\\ \\ \\ error\\ df \\right)}\\sqrt{2MSE/r}\\] (CRD equal replication)\\(t_{\\left( \\frac{\\alpha}{2},\\ \\ \\ error\\ df \\right)}\\) denotes critical value Student‚Äôs t two tailed test Œ± level significance error degrees freedom. \\(r_{}\\) \\(r_{j}\\) replications \\(T_{},\\ T_{j}\\) respectively. difference two treatment means greater\ncritical difference, significantly different. difference two treatment means less \\(\\text{C.D}\\). said par. (.e., statistically significant difference). treatments par given \nsymbols letters, treatment means without common letters symbols significantly different corresponding level significance (Œ±).","code":""},{"path":"single-factor-experiments.html","id":"advantages-of-a-crd","chapter":"19 Single factor experiments","heading":"19.1.5 Advantages of a CRD","text":"layout easy.layout easy.complete flexibility design .e. number \ntreatments replications treatment can tried.complete flexibility design .e. number \ntreatments replications treatment can tried.Whole experimental material can utilized design.Whole experimental material can utilized design.design yields maximum degrees freedom experimental\nerror.design yields maximum degrees freedom experimental\nerror.analysis data simplest compared design.analysis data simplest compared design.Even values missing analysis can done.Even values missing analysis can done.","code":""},{"path":"single-factor-experiments.html","id":"disadvantages-of-a-crd","chapter":"19 Single factor experiments","heading":"19.1.6 Disadvantages of a CRD","text":"suitable field experiments.suitable field experiments.Relatively low accuracy due lack local control. Completely homogeneous experimental units practically difficult many situations.Relatively low accuracy due lack local control. Completely homogeneous experimental units practically difficult many situations.","code":""},{"path":"single-factor-experiments.html","id":"replical","chapter":"19 Single factor experiments","heading":"19.1.7 Number of replications in CRD","text":"experimenter free choose number replications experiment. precision estimating experimental error variance increases number replications increases. design number replications optimum, since increasing replications increase experiment cost. , \nrule thumb says, error degrees freedom least 12 keep type II error check. One can easily find number replications required design using formula error degrees freedom design.","code":""},{"path":"single-factor-experiments.html","id":"example-10","chapter":"19 Single factor experiments","heading":"Example","text":"5 treatments; planning conduct experiment CRD. Find minimum number replications required.Formula error degrees freedom (edf) CRD \\(n - t\\); \\(n\\) total number experimental units \\(t\\) number treatments. \\(n = rt\\), \\(r\\) number replications.edf \\(\\geq\\) 12edf \\(= n - t = rt - t = t\\left( r - 1 \\right) \\geq 12\\)Since t = 5, edf \\(= 5\\left( r - 1 \\right) \\geq 12\\)\\(= r \\geq 3.4\\), Since \\(r\\) fraction \\(r =\\) 4The number replications required 4Why minimum error degrees freedom required design 12The error degrees freedom major role deciding critical value F. can see table F section 20 error degrees freedom degrees freedom denominator decreases F tends larger values, inflates type II error, .e. able reject null hypothesis considerable difference treatments (higher\nTreatment Sum Square). towards 12 degrees freedom denominator F values stabilize increase degrees freedom.","code":""},{"path":"single-factor-experiments.html","id":"example-11","chapter":"19 Single factor experiments","heading":"19.1.8 Example","text":"order find yielding abilities five varieties sesame experiment conducted green house using CRD four pots per variety. results given following table .\nFigure 19.5: ANOVA table CRD\n","code":""},{"path":"single-factor-experiments.html","id":"solution","chapter":"19 Single factor experiments","heading":"19.1.8.1 Solution","text":"Correction Factor:\\(= \\frac{G^{2}}{n} = \\frac{\\left( 397 \\right)^{2}}{20}\\)=7880.45Total Sum squares:\\(= \\left( 25^{2} + 21^{2} + .... + 11^{2} \\right) - CF\\)\\(= 8307 - CF = 426.55\\)Treatment Sum Squares:\\(\\frac{1}{4}(85^{2} + 102^{2} + ... + 53^{2}) - CF\\)\\(= 8211.75 - CF = 331.30\\)Error Sum Square:\\(426.55 - 331.30 = 95.25\\)\nFigure 19.6: ANOVA table CRD\nSince F value calculated greater table value F, can reject null hypothesis conclude significant difference atleast pair treatments. order find, treatments significantly different, perform Fisher‚Äôs LSD test.\\[\\text{CD} = t_{\\frac{\\alpha}{2}}\\sqrt{\\frac{\\text{2xMSE}}{r}} = \\text{2.131}\\sqrt{\\frac{2\\left( \\text{6.350} \\right)}{4}} = (\\text{2.131})(\\text{1.7819}) = \\text{3.797}\\text{2}\\]\\(t_{0.025\\text{ }15 \\text{ edf}} = 2.131\\) (Just look critical value \nt two tailed test Œ±= 0.05 statistical table. (see\nsection 20. shown \\(t_{\\frac{\\alpha}{2}}\\) , since\ntwo tailed test one side t-distribution curve \nprobability \\(\\frac{\\alpha}{2}\\), combined probability \nŒ±.)Treatment means arranged descending order. treatment pairs\nwhose difference means greater C.D. labelled \ndifferent letters.table can see difference mean V2 V1\n4 greater C.D. value. V2 V1 significantly\ndifferent; different letters assigned . Instead \nletters one can also use symbols shown . treatments \nletters/symbols significantly different., example can conclude V2 best variety ,\nhighest mean significantly different \nothers.Note: cases, one alphabets can appear \ntreatment mean, see hypothetical example .\nFigure 19.7: hypothetical example treatment grouping\nlettering like appears meaning treatments common alphabets significantly different. table V2\nV1 significantly different common\nletter . time can see V5 V4 significantly\ndifferent don‚Äôt common letters.","code":""},{"path":"single-factor-experiments.html","id":"randomized-complete-block-designs-rcbd","chapter":"19 Single factor experiments","heading":"19.2 Randomized Complete Block Designs (RCBD)","text":"field experiments, plots treatments applied may uniform may difference plot--plot fertility. cases CRD \nrecommended. one source constitutes difference plots can recommend randomized block design.randomized complete block (RCB) design one widely used experimental designs agricultural research. design especially suited field experiments number treatments large experimental area predictable fertility gradient.experimental material heterogeneous, experimental material grouped homogeneous sub-groups called blocks. block consists entire set treatments block equivalent replication. example, field known fertility gradient one direction, plots perpendicular gradient considered block. RCBD (Randomized Complete\nBlock Design) blocks equal size contains treatments. Since block contains treatments, term ‚Äòcomplete block design‚Äô used. See figure 15.8 15.9 see blocks formed field.RCBD number replications, \\(r\\) = number blocks\nNumber plots within block = number treatments, \\(v\\)","code":""},{"path":"single-factor-experiments.html","id":"blocking-technique","chapter":"19 Single factor experiments","heading":"19.2.1 Blocking technique","text":"Blocking local control, one basic principles design effectively implemented RCBD. primary purpose blocking reduce experimental error eliminating contribution known sources variation among experimental units. achieved grouping homogeneous units together, variability blocks minimized, blocks maximized.Two important factors considered blockingThe selection source variability used basis blocking, field experiments usually fertility gradient productivity gradient used identified source variability.selection source variability used basis blocking, field experiments usually fertility gradient productivity gradient used identified source variability.Proper selection block shape orientation. example, gradient unidirectional use long narrow blocks. Use square blocks strong fertility gradient directions. experimenter use common sense identify shape orientation block examining field conditions, suc plots block homogeneous.Proper selection block shape orientation. example, gradient unidirectional use long narrow blocks. Use square blocks strong fertility gradient directions. experimenter use common sense identify shape orientation block examining field conditions, suc plots block homogeneous.","code":""},{"path":"single-factor-experiments.html","id":"rcbdrandom","chapter":"19 Single factor experiments","heading":"19.2.2 Randomization in RCBD","text":"Consider experiment involving four treatments , B, C, D RCBD. treatment replicated 5 times. Randomization procedure explained . randomization process RCB design applied separately independently blocks.STEP 1. Divide experimental area \\(r\\) equal blocks, \\(r\\) number replications. example \\(r =\\) 5.\nFigure 19.8: Division experimental field five blocks, consisting four plots, RCBD four treatments five replications. Blocking done blocks rectangular perpendicular direction unidirectional fertility gradient\nSTEP 2. Subdivide block \\(v\\) experimental plots, \\(v\\) number treatments. Number \\(v\\) plots consecutively 1 \\(v\\), assign \\(v\\) treatments random \\(v\\) plots following randomization scheme CRD described Section 19.1.1.1. example 5 blocks divided 4 plots. Plots numbered 1 4 block. Now block, paper lots 1 4 prepared. Treatment names written order plot number lot recorded order shown .STEP 3. treatments allotted randomly selected plots. Step 2 repeated blocks.\nFigure 19.9: Allotment treatments blocks. RCBD block treatments treatment appear block exactly .\n","code":""},{"path":"single-factor-experiments.html","id":"layout-of-rcbd","chapter":"19 Single factor experiments","heading":"19.2.3 Layout of RCBD","text":"RCBD block treatments treatment appear block exactly . allotting treatment according randomization procedure explained section 19.2.2. final layout look like figure 19.10.Note:\nRBD every treatment number replications. (treatments equally replicated say, \\(r\\) times). need total \\(v\\) \\(\\times\\) \\(r\\) plots conducting experiment. \\(r\\) blocks containing \\(v\\) plots. design, number blocks = number replication treatments = \\(r\\)\nnumber plots within block = number treatments = \\(v\\)\nFigure 19.10: Layout RCBD four treatments five replications\n","code":""},{"path":"single-factor-experiments.html","id":"statistical-model-for-rcbd","chapter":"19 Single factor experiments","heading":"19.2.4 Statistical Model for RCBD","text":"Consider RCBD ¬†\\(v\\) treatments ¬†\\(r\\)¬†replications (Blocks). Statistical model RCBD two-way ANOVA¬†described section 17.6.2.1.\\[Y_{\\text{ij}} = \\mu + \\tau_{} +  \\gamma_{j}+ e_{\\text{ij}}\\]\n, = 1,2, ‚Ä¶, v j =1,2, ‚Ä¶, r. \\(Y_{\\text{ij}}\\) observed value response ith treatment jth block, \\(\\tau_{}\\) effect ith factor,\\(\\gamma_{j}\\) effect jth block . \\(\\mu\\) general\neffect common treatments. \\(e_{\\text{ij}}\\) error\neffects.\\(e_{ij}\\) \\(\\sim iid\\ N(0,\\sigma^{2})\\)","code":""},{"path":"single-factor-experiments.html","id":"analysis-of-rcbd","chapter":"19 Single factor experiments","heading":"19.2.5 Analysis of RCBD","text":"Analysis RCBD explained section 17.6.2.3 two-way ANOVA.Observation \\(v\\) treatments \\(r\\) blocks arranged shown .\nFigure 19.11: Two-way arrangement observations RCBD\n\nFigure 19.12: ANOVA table RCBD\nF1 ~ F (\\(v\\)‚Äì1),¬†(\\(v\\)‚Äì1)(\\(r\\)‚Äì1) F2 ~ F (\\(r\\)‚Äì1),(\\(v\\)‚Äì1)(\\(r\\)‚Äì1) two F values, one treatment one blocks respectively. Decisions null hypothesis made comparing corresponding F value table value explained section 20.F2 significant?\nF2 significant, indicates significant difference blocks, means blocking effective. Blocking considered effective reducing experimental error F2 significant.","code":""},{"path":"single-factor-experiments.html","id":"number-of-replications-in-rcbd","chapter":"19 Single factor experiments","heading":"19.2.6 Number of replications in RCBD","text":"explained section 19.1.7 number replications RCBD found using formula error degrees freedom (edf).","code":""},{"path":"single-factor-experiments.html","id":"example-12","chapter":"19 Single factor experiments","heading":"Example","text":"4 treatments; planning conduct experiment RCBD. Find minimum number replications required.Formula error degrees freedom (edf) RCBD \\((v - 1) (r-1)\\); \\(v\\) number treatments \\(r\\) number blocks.edf \\(\\geq\\) 12edf \\(=(v - 1) (r-1) \\geq 12\\)Since \\(v\\) = 4, edf \\(= 3\\left( r - 1 \\right) \\geq 12\\)\\(\\implies r \\geq 5\\), minimum replication required 4The number replications required 4.","code":""},{"path":"single-factor-experiments.html","id":"post-hoc-test-in-rcbd","chapter":"19 Single factor experiments","heading":"19.2.7 Post-hoc test in RCBD","text":"null hypothesis treatments rejected ANOVA; proceed post-hoc test explained section 18.2. Arrange treatment means descending order. Critical difference (CD) LSD (Least Significant Difference) calculated treatment pair \\(T_{},\\ T_{j}\\) using following formula.\\[CD = \\ t_{\\frac{\\alpha}{2},edf}.\\sqrt{\\frac{2\\ MSE}{r}}\\]\n, \\(\\text{MSE}\\) Mean Square Error ANOVA.\n\\(t_{\\frac{\\alpha}{2},edf}\\) critical value two tailed student‚Äôs t distribution \\(\\alpha\\) level significance error degrees \nfreedom. \\(r\\) number blocks/replication.\nProcedure grouping using symbols CRD explained section 19.1.4.1.","code":""},{"path":"single-factor-experiments.html","id":"advantages-of-rcbd","chapter":"19 Single factor experiments","heading":"19.2.8 Advantages of RCBD","text":"Accuracy: RCB design shown efficient accurate C.R.D. types experimental work.Accuracy: RCB design shown efficient accurate C.R.D. types experimental work.Flexibility: RCBD restrictions placed number treatments \nnumber replicates.Flexibility: RCBD restrictions placed number treatments \nnumber replicates.Even values missing, still analysis can done using missing plot\ntechnique.Even values missing, still analysis can done using missing plot\ntechnique.Ease analysis: Statistical analysis simple, rapid straight forward.Ease analysis: Statistical analysis simple, rapid straight forward.","code":""},{"path":"single-factor-experiments.html","id":"disadvantages-of-rcbd","chapter":"19 Single factor experiments","heading":"19.2.9 Disadvantages of RCBD","text":"RCBD may give misleading results blocks homogeneous.RCBD may give misleading results blocks homogeneous.RCBD suitable large number treatments case block size increase may possible keep large blocks\nhomogeneous.RCBD suitable large number treatments case block size increase may possible keep large blocks\nhomogeneous.data two plots missing statistical analysis becomes\ntedious complicated.data two plots missing statistical analysis becomes\ntedious complicated.","code":""},{"path":"single-factor-experiments.html","id":"latin-square-design-lsd","chapter":"19 Single factor experiments","heading":"19.3 Latin Square Design (LSD)","text":"design, variations field two different directions (variations\ndue two different factors) controlled simultaneously affecting \ntreatment comparisons. layout RBD controls one source external\nvariation affecting treatment comparisons construction homogeneous\nblocks. Sometimes blocks laid field environmental conditions\nwithin block need . example, soil variability North South\ndirection environmental gradient East West Direction conditions\nwithin block vary. situations variability can controlled \nLatin Square design (LSD). blocks LSD called rows columns; \nrepresent two external factors. entire experimental area can divided small\nplots arranged form square. number plots row \ncolumn equal, equal number treatments compared.","code":""},{"path":"single-factor-experiments.html","id":"layout-and-randaomization","chapter":"19 Single factor experiments","heading":"19.3.1 Layout and Randaomization","text":"Latin square \\(v\\) treatments (usually called side \\(v\\)) arrangement \n\\(v\\) treatments \\(v\\) rows \\(v\\) columns every treatment replicated \nrow column exactly .Latin squares various sides available Statistical tables. \\(v\\)\ntreatments compared, chose Latin square side \\(v\\) statistical tables. \nFisher Yates (1948) tables Statisticians Biometricians squares\npresented given standard order alphabets first row. Choose one \nsquares given \\(v\\) carryout randomization following way.Let \\(v\\) = 6. Latin Square 6 treatments denoted , B, C, D, E F\ncan denoted keeping alphabetical order first row order symmetry\nremaining rows can written \nFigure 19.13: Layout LSD six treatments six replications\nsquare represents Latin square \\(v\\) = 6 treatments 6 replications.\nBlocks represented two directions rows columns number also\n= \\(v\\)Randomization\n\\(v\\) treatments take square order \\(v\\) random\nstatistical tables.Number rows square 1 \\(v\\). Keeping first row fixed, \nremaining rows arranged randomly using random numbers develop another\nsquare.\nsquare can randomised many ways like\nFigure 19.14: Randomization LSD six treatments six replications\nrandomisation rows columns randomised get another randomized square.\nFigure 19.15: Randomization LSD\nLastly , \\(v\\) treatments can assigned randomly \\(v\\) letters (, B, ‚Ä¶).","code":""},{"path":"single-factor-experiments.html","id":"analysis","chapter":"19 Single factor experiments","heading":"19.3.2 Analysis","text":"data collected experiments LSD classified according \nlevels three factors. called three way classified design. two types \nblockings (Rows columns) treatments make three factors.\nlinear model analysis :\n\\[Y_{\\text{ijk}} = \\mu + \\tau_{} + R_{j}+ C_{k}+e_{\\text{ijk}}\\]\n,j, k = 1,2,3,‚Ä¶ \\(v\\).e., response ( yield (\\(Y_{ijk}\\))) plot = general mean effect (Œº) + Effect due \\(^{th}\\) treatment (\\(\\tau_{}\\)) + Effect \\(j^{th}\\) Row ( \\(R_{j}\\) ) + Effect due \\(k^{th}\\) Column ( \\(C_{k}\\)) + Random error terms associated plot ( \\(e_{ijk}\\) ).\neffect general mean subtracted sum squares ( \nsubtracting correction factor \\(C.F.  = \\frac{(\\sum{y_{ijk}})^2} {v^2}\\)). total sum squares split four components () variation treatments treatment Sum squares (ii) variation rows (iii) variation columns (iv) Variation within factors: .e., Error Sum squares.\nFigure 19.16: ANOVA LSD\ncomputed F values compared F table values (v-1) (v-1)(v-2) degrees freedom. treatments turns significant means compared pair wise using\n\\[C.D. = t_{\\left( \\frac{\\alpha}{2},\\ \\ \\ error\\ df \\right)}\\sqrt{2MSE/v}\\]Advantages\nLSD efficient trends fertility two perpendicular\ndirections two factors contributing experimental error.Disadvantages\nRBD restriction like number replications = Number\ntreatments. error degrees freedom design \\((v-1)(v-2)\\). number\ntreatments like \\(v\\) = 2 , 3 4 value 0 , 2 6. ( ie., small- hence carryout experiment values \\(v\\)).Similarly \\(v>10\\) total number plots required \\(v^2\\) > 100. \ndifficulty getting large number homogeneous plots. LSD \ncommonly used 5 10 treatments .","code":""},{"path":"single-factor-experiments.html","id":"example-13","chapter":"19 Single factor experiments","heading":"19.3.3 Example","text":"Consider 5x5 LSD experimental setup followsSolution\\(H_{01}:Œº_{11}=Œº_{12}=Œº_{13}=Œº_{14}=Œº_{15}\\)\\(H_{11}\\): Atleast two treatment means different\\(H_{02}:Œº_{21}=Œº_{22}=Œº_{23}=Œº_{24}=Œº_{25}\\)\\(H_{12}\\): least two row means different\\(H_{03}:Œº_{31}=Œº_{32}=Œº_{34}=Œº_{35}\\)\\(H_{12}\\): Atleast two column means differentGrand Total (G)= 829Correction Factor(C.F)\n\\(= \\frac{G^{2}}{n} = \\frac{\\left( 829 \\right)^{2}}{25}\\)= 27489.64Total Sum Squares(TSS)= \\((28^2+36^2+18^2+...+31^2)-27489.64\\)\\(=29243-27489.64 = 1753.36\\)Row Sum Squares(RSS)=\\(\\frac{1}{5}(159^{2} + 163^{2} + 197^{2}+157^{2}+153^{2}) - CF\\)\\(=27743.4-27489.64 = 253.76\\)Column Sum Squares(CSS)=\\(\\frac{1}{5}(177^{2} + 165^{2} + 138^{2}+162^{2}+187^{2}) - CF\\)\n\\(= 27762.2-27489.64 = 272.56\\)Treatment Sum Squares(SSt)=\\(\\frac{1}{5}(188^{2} + 171^{2} + 142^{2}+210^{2}+118^{2}) - CF\\)\n\\(= 28554.6 - 27489.64 = 1064.96\\)Error Sum Squares \\(= 1753.36 - 253.76- 272.56-1064.96 = 162.08\\)\nFigure 19.17: ANOVA\nSince F cal greater F table value, null hypotheses rejected.\\(C.D. = t_{\\left( \\frac{\\alpha}{2},\\ \\ \\ error\\ df \\right)}\\sqrt{2MSE/v}\\)\\(C.D. = 2.18\\sqrt{2x13.51/5}\\)\\(C.D=5.06\\)Mean treatment D maximum statistically par . Mean E \nminimum statistically par C.","code":""},{"path":"single-factor-experiments.html","id":"missing-plot-analysis","chapter":"19 Single factor experiments","heading":"19.4 MISSING PLOT ANALYSIS","text":"agricultural field experiments experimenter often encountered \nsituation observations particular treatment/ plot may lost may affected external factors possible analyse observations including normal values. observation treatment may get lost various reasons like, attack cattle birds, manure may dumped side, disease infestation . data recorded plots affected omitted analysis carried ‚Äì called missing plot analysis.Analysis data may done different methods. () \ncommonly used method currently adopted known ‚Äòmethod analysis non\northogonal data‚Äô highly computer based (ii) Method substitution Yates based minimization error sum squares (iii) Analysis data missing values technique Analysis Covariance Bartlett‚Äôs.Covariance Method Bartlett‚Äôs\nAssume imaginary covariate X taking values zero every plot except \nmissing plot take value 1 (‚Äì1) . Now value main variate, Y = 0 missing plot, actual values remaining plots. data analysed per ANCOVA technique respective design used.\nmissing plot analysis degrees freedom error total \nbased existing number observations . method also degrees freedom adjusted error sum squares less one (one missing value).Method Substitution\\(()\\) method calculate aid formula estimate missing value. formula vary design design actually supply exact missing value. procedure permits researcher complete analysis without resorting complex procedures.\\((ii)\\) Insert estimated value missing position workout estimates treatment means mean error sum squares.\\((iii)\\) additional adjustments treatment sum squares needed. pair comparison also changes made.\\((iv)\\) iterative procedure adopted one observation missing.","code":""},{"path":"single-factor-experiments.html","id":"randomised-block-design-with-a-single-missing-value","chapter":"19 Single factor experiments","heading":"19.4.1 Randomised Block Design with a single missing value","text":"\\(()\\) missing value RBD estimated \n\\[x=\\frac{rB+vT-G}{(r-1)(v-1)}\\]\n,\nx = estimate missing data\nv = number treatments\nr = number replications\nB = Total observed values replication contain missing data\nT = Total observed values treatment contain missing data\nG = Grand total existing observations.\\((ii)\\) estimate missing value placed position analysis carried based procedure RBD. Subtract one degree freedom total error degrees freedom.method provides proper estimate error variance per plot \ninflation treatment sum squares; (treatment sum squares positively biased). treatments turn significant can ignore bias results accepted. treatments turns just significant may due bias. case actual treatment sum squares obtained suitable formula.\\((iii)\\) Estimate Bias case RBD\n\\[Bias=\\frac{(B+vT-G)^2}{v(v-1)(r-1)^2}\\]\nbias subtracted treatment sum squares. Now test actual\ntreatment mean square Mean Square error: make conclusion significance treatments.\\((iv)\\) Pair comparison\\(()\\) comparing two treatment means one contain missing observation\n\\[C.D. = t_{\\alpha}\\sqrt{MSE(\\frac{2}{r}+\\frac{v}{r(r-1)(v-1)})}\\]\\((b)\\) comparing pairs treatment means: (none \ncontain missing observation)\\[C.D. = t_{\\alpha}\\sqrt{2MSE/r}\\]\n, \\(t_{\\alpha}\\) denote t value (r-1)(v-1)-1 degrees freedom.\\((v)\\) one missing value estimates missing\nvalues obtained iteration procedure.","code":""},{"path":"single-factor-experiments.html","id":"latin-square-design-with-a-single-missing-value","chapter":"19 Single factor experiments","heading":"19.4.2 Latin Square Design with a single missing value","text":"procedure RBD used also.\\(()\\) missing value LSD obtained \n\\[x=\\frac{v(R+C+T)-2G}{(v-1)(v-2)}\\]\n,\nx = estimate missing data\nv = number treatments/rows/blocks/columns\nR = Total observed values row contain missing data\nC = Total observed values column contain missing data\nT = Total observed values treatment contain missing data\nG = Grand total existing observations.\\((ii)\\) Carry analysis similar method substitution. estimate Bias case \n\\[Bias=\\frac{[(v-1)T+R+C-G]^2}{[(v-1)(v-2)]^2}\\]\\((iii)\\) pair wise comparison involving treatment missing observation- (comparing two treatment means one contain missing observation )\\[C.D. = t_{\\alpha}\\sqrt{MSE(\\frac{2}{v}+\\frac{1}{(v-1)(v-2)})}\\]¬†\n¬†\n¬†","code":""},{"path":"ftable.html","id":"ftable","chapter":"20 How to use F table","heading":"20 How to use F table","text":"table gives model critical values table F (also known statistical table F) \\(\\alpha\\) = 0.05 level significance. Critical values F can generated now days using softwares (Microsoft Excel, R)Obtain F-ratio ANOVA using formula.(x,y) degrees freedom associated . example, ANOVA treatment degrees freedom (x) error degrees freedom (y)Go along x columns, y rows. point intersection critical F-ratio.obtained value F equal larger critical F-value, can reject null hypothesis stating population means equal 100(1-\\(\\alpha\\))%, 100(1-0.05) % = 95% confidence.example: obtain F ratio 3.26 (2, 24) degrees freedom. Go along 2 columns 24 rows. critical value F 3.40. obtained F-ratio less , conclude enough evidence reject null hypothesis.\nFigure 20.1: model critical value table F alpha=0.05\nClick download full statistical table","code":""},{"path":"ftable.html","id":"generate-critical-value-of-f-using-excel","chapter":"20 How to use F table","heading":"20.1 Generate critical value of F using excel","text":"find F critical value Excel, can use F.INV.RT() function, uses following syntax:= F.INV.RT(probability, deg_freedom1, deg_freedom2).example F(2, 24) \\(\\alpha\\)=0.05, Excel function \n> = F.INV.RT(0.05, 2, 24).¬†\n¬†\n¬†","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
